{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p> All-in-one AI framework </p> <p> </p> <p>txtai is an all-in-one AI framework for semantic search, LLM orchestration and language model workflows.</p> <p> </p> <p>The key component of txtai is an embeddings database, which is a union of vector indexes (sparse and dense), graph networks and relational databases.</p> <p>This foundation enables vector search and/or serves as a powerful knowledge source for large language model (LLM) applications.</p> <p>Build autonomous agents, retrieval augmented generation (RAG) processes, multi-model workflows and more.</p> <p>Summary of txtai features:</p> <ul> <li>\ud83d\udd0e Vector search with SQL, object storage, topic modeling, graph analysis and multimodal indexing</li> <li>\ud83d\udcc4 Create embeddings for text, documents, audio, images and video</li> <li>\ud83d\udca1 Pipelines powered by language models that run LLM prompts, question-answering, labeling, transcription, translation, summarization and more</li> <li>\u21aa\ufe0f\ufe0f Workflows to join pipelines together and aggregate business logic. txtai processes can be simple microservices or multi-model workflows.</li> <li>\ud83e\udd16 Agents that intelligently connect embeddings, pipelines, workflows and other agents together to autonomously solve complex problems</li> <li>\u2699\ufe0f Web and Model Context Protocol (MCP) APIs. Bindings available for JavaScript, Java, Rust and Go.</li> <li>\ud83d\udd0b Batteries included with defaults to get up and running fast</li> <li>\u2601\ufe0f Run local or scale out with container orchestration</li> </ul> <p>txtai is built with Python 3.10+, Hugging Face Transformers, Sentence Transformers and FastAPI. txtai is open-source under an Apache 2.0 license.</p> <p>Note</p> <p>NeuML is the company behind txtai and we provide AI consulting services around our stack. Schedule a meeting or send a message to learn more.</p> <p>We're also building an easy and secure way to run hosted txtai applications with txtai.cloud.</p>"},{"location":"cloud/","title":"Cloud","text":"<p>Scalable cloud-native applications can be built with txtai. The following cloud runtimes are supported.</p> <ul> <li>Container Orchestration Systems (i.e. Kubernetes)</li> <li>Docker Engine</li> <li>Serverless Compute</li> <li>txtai.cloud (planned for future)</li> </ul> <p>Images for txtai are available on Docker Hub for CPU and GPU installs. The CPU install is recommended when GPUs aren't available given the image is significantly smaller.</p> <p>The base txtai images have no models installed and models will be downloaded each time the container starts. Caching the models is recommended as that will significantly reduce container start times. This can be done a couple different ways.</p> <ul> <li>Create a container with the models cached</li> <li>Set the transformers cache environment variable and mount that volume when starting the image     <pre><code>docker run -v &lt;local dir&gt;:/models -e TRANSFORMERS_CACHE=/models --rm -it &lt;docker image&gt;\n</code></pre></li> </ul>"},{"location":"cloud/#build-txtai-images","title":"Build txtai images","text":"<p>The txtai images found on Docker Hub are configured to support most situations. This image can be locally built with different options as desired.</p> <p>Examples build commands below.</p> <pre><code># Get Dockerfile\nwget https://raw.githubusercontent.com/neuml/txtai/master/docker/base/Dockerfile\n\n# Build Ubuntu 22.04 image running Python 3.10\ndocker build -t txtai --build-arg BASE_IMAGE=ubuntu:22.04 --build-arg PYTHON_VERSION=3.10 .\n\n# Build image with GPU support\ndocker build -t txtai --build-arg GPU=1 .\n\n# Build minimal image with the base txtai components\ndocker build -t txtai --build-arg COMPONENTS= .\n</code></pre>"},{"location":"cloud/#container-image-model-caching","title":"Container image model caching","text":"<p>As mentioned previously, model caching is recommended to reduce container start times. The following commands demonstrate this. In all cases, it is assumed a config.yml file is present in the local directory with the desired configuration set.</p>"},{"location":"cloud/#api","title":"API","text":"<p>This section builds an image that caches models and starts an API service. The config.yml file should be configured with the desired components to expose via the API.</p> <p>The following is a sample config.yml file that creates an Embeddings API service.</p> <pre><code># config.yml\nwritable: true\n\nembeddings:\n  path: sentence-transformers/nli-mpnet-base-v2\n  content: true\n</code></pre> <p>The next section builds the image and starts an instance.</p> <pre><code># Get Dockerfile\nwget https://raw.githubusercontent.com/neuml/txtai/master/docker/api/Dockerfile\n\n# CPU build\ndocker build -t txtai-api .\n\n# GPU build\ndocker build -t txtai-api --build-arg BASE_IMAGE=neuml/txtai-gpu .\n\n# Run\ndocker run -p 8000:8000 --rm -it txtai-api\n</code></pre>"},{"location":"cloud/#service","title":"Service","text":"<p>This section builds a scheduled workflow service. More on scheduled workflows can be found here.</p> <pre><code># Get Dockerfile\nwget https://raw.githubusercontent.com/neuml/txtai/master/docker/service/Dockerfile\n\n# CPU build\ndocker build -t txtai-service .\n\n# GPU build\ndocker build -t txtai-service --build-arg BASE_IMAGE=neuml/txtai-gpu .\n\n# Run\ndocker run --rm -it txtai-service\n</code></pre>"},{"location":"cloud/#workflow","title":"Workflow","text":"<p>This section builds a single run workflow. Example workflows can be found here.</p> <pre><code># Get Dockerfile\nwget https://raw.githubusercontent.com/neuml/txtai/master/docker/workflow/Dockerfile\n\n# CPU build\ndocker build -t txtai-workflow . \n\n# GPU build\ndocker build -t txtai-workflow --build-arg BASE_IMAGE=neuml/txtai-gpu .\n\n# Run\ndocker run --rm -it txtai-workflow &lt;workflow name&gt; &lt;workflow parameters&gt;\n</code></pre>"},{"location":"cloud/#serverless-compute","title":"Serverless Compute","text":"<p>One of the most powerful features of txtai is building YAML-configured applications with the \"build once, run anywhere\" approach. API instances and workflows can run locally, on a server, on a cluster or serverless.</p> <p>Serverless instances of txtai are supported on frameworks such as AWS Lambda, Google Cloud Functions, Azure Cloud Functions and Kubernetes with Knative.</p>"},{"location":"cloud/#aws-lambda","title":"AWS Lambda","text":"<p>The following steps show a basic example of how to build a serverless API instance with AWS SAM.</p> <ul> <li>Create config.yml and template.yml</li> </ul> <pre><code># config.yml\nwritable: true\n\nembeddings:\n  path: sentence-transformers/nli-mpnet-base-v2\n  content: true\n</code></pre> <pre><code># template.yml\nResources:\n  txtai:\n    Type: AWS::Serverless::Function\n    Properties:\n      PackageType: Image\n      MemorySize: 3000\n      Timeout: 20\n      Events:\n        Api:\n          Type: Api\n          Properties:\n            Path: \"/{proxy+}\"\n            Method: ANY\n    Metadata:\n      Dockerfile: Dockerfile\n      DockerContext: ./\n      DockerTag: api\n</code></pre> <ul> <li> <p>Install AWS SAM</p> </li> <li> <p>Run following</p> </li> </ul> <pre><code># Get Dockerfile and application\nwget https://raw.githubusercontent.com/neuml/txtai/master/docker/aws/api.py\nwget https://raw.githubusercontent.com/neuml/txtai/master/docker/aws/Dockerfile\n\n# Build the docker image\nsam build\n\n# Start API gateway and Lambda instance locally\nsam local start-api -p 8000 --warm-containers LAZY\n\n# Verify instance running (should return 0)\ncurl http://localhost:8080/count\n</code></pre> <p>If successful, a local API instance is now running in a \"serverless\" fashion. This configuration can be deployed to AWS using SAM. See this link for more information.</p>"},{"location":"cloud/#kubernetes-with-knative","title":"Kubernetes with Knative","text":"<p>txtai scales with container orchestration systems. This can be self-hosted or with a cloud provider such as Amazon Elastic Kubernetes Service, Google Kubernetes Engine and Azure Kubernetes Service. There are also other smaller providers with a managed Kubernetes offering.</p> <p>A full example covering how to build a serverless txtai application on Kubernetes with Knative can be found here.</p>"},{"location":"cloud/#txtaicloud","title":"txtai.cloud","text":"<p>txtai.cloud is a planned effort that will offer an easy and secure way to run hosted txtai applications.</p>"},{"location":"examples/","title":"Examples","text":"<p>See below for a comprehensive series of example notebooks and applications covering txtai.</p>"},{"location":"examples/#semantic-search","title":"Semantic Search","text":"<p>Build semantic/similarity/vector/neural search applications.</p> Notebook Description Introducing txtai \u25b6\ufe0f Overview of the functionality provided by txtai Build an Embeddings index with Hugging Face Datasets Index and search Hugging Face Datasets Build an Embeddings index from a data source Index and search a data source with word embeddings Add semantic search to Elasticsearch Add semantic search to existing search systems Similarity search with images Embed images and text into the same space for search Custom Embeddings SQL functions Add user-defined functions to Embeddings SQL Model explainability Explainability for semantic search Query translation Domain-specific natural language queries with query translation Build a QA database Question matching with semantic search Semantic Graphs Explore topics, data connectivity and run network analysis Topic Modeling with BM25 Topic modeling backed by a BM25 index"},{"location":"examples/#llm","title":"LLM","text":"<p>Autonomous agents, retrieval augmented generation (RAG), chat with your data, pipelines and workflows that interface with large language models (LLMs).</p> Notebook Description Prompt-driven search with LLMs Embeddings-guided and Prompt-driven search with Large Language Models (LLMs) Prompt templates and task chains Build model prompts and connect tasks together with workflows Build RAG pipelines with txtai \u25b6\ufe0f Guide on retrieval augmented generation including how to create citations Integrate LLM frameworks Integrate llama.cpp, LiteLLM and custom generation frameworks Generate knowledge with Semantic Graphs and RAG Knowledge exploration and discovery with Semantic Graphs and RAG Build knowledge graphs with LLMs Build knowledge graphs with LLM-driven entity extraction Advanced RAG with graph path traversal Graph path traversal to collect complex sets of data for advanced RAG Advanced RAG with guided generation Retrieval Augmented and Guided Generation RAG with llama.cpp and external API services RAG with additional vector and LLM frameworks How RAG with txtai works Create RAG processes, API services and Docker instances Speech to Speech RAG \u25b6\ufe0f Full cycle speech to speech workflow with RAG Analyzing Hugging Face Posts with Graphs and Agents Explore a rich dataset with Graph Analysis and Agents Granting autonomy to agents Agents that iteratively solve problems as they see fit Getting started with LLM APIs Generate embeddings and run LLMs with OpenAI, Claude, Gemini, Bedrock and more Analyzing LinkedIn Company Posts with Graphs and Agents Exploring how to improve social media engagement with AI Parsing the stars with txtai Explore an astronomical knowledge graph of known stars, planets, galaxies Chunking your data for RAG Extract, chunk and index content for effective retrieval Medical RAG Research with txtai Analyze PubMed article metadata with RAG GraphRAG with Wikipedia and GPT OSS Deep graph search powered RAG RAG is more than Vector Search Context retrieval via Web, SQL and other sources OpenCode as a txtai LLM Integrate OpenCode with the txtai ecosystem Agentic College Search Identify a list of strong engineering colleges TxtAI got skills Integrate skill.md files with your agent"},{"location":"examples/#pipelines","title":"Pipelines","text":"<p>Transform data with language model backed pipelines.</p> Notebook Description Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Extractive QA to build structured data Build structured datasets using extractive question-answering Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling Building abstractive text summaries Run abstractive text summarization Extract text from documents Extract text from PDF, Office, HTML and more Text to speech generation Generate speech from text Transcribe audio to text Convert audio files to text Translate text between languages Streamline machine translation and language detection Generate image captions and detect objects Captions and object detection for images Near duplicate image detection Identify duplicate and near-duplicate images"},{"location":"examples/#workflows","title":"Workflows","text":"<p>Efficiently process data at scale.</p> Notebook Description Run pipeline workflows \u25b6\ufe0f Simple yet powerful constructs to efficiently process data Transform tabular data with composable workflows Transform, index and search tabular data Tensor workflows Performant processing of large tensor arrays Entity extraction workflows Identify entity/label combinations Workflow Scheduling Schedule workflows with cron expressions Push notifications with workflows Generate and push notifications with workflows Pictures are a worth a thousand words Generate webpage summary images with DALL-E mini Run txtai with native code Execute workflows in native code with the Python C API Generative Audio Storytelling with generative audio workflows"},{"location":"examples/#model-training","title":"Model Training","text":"<p>Train, distill, fine-tune and export models.</p> Notebook Description Train a text labeler Build text sequence classification models Train without labels Use zero-shot classifiers to train new models Train a QA model Build and fine-tune question-answering models Train a language model from scratch Build new language models Distilling Knowledge into Tiny LLMs \u25b6\ufe0f Finetune tiny LLMs to enable inference using less resources Export and run models with ONNX Export models with ONNX, run natively in JavaScript, Java and Rust Export and run other machine learning models Export and run models from scikit-learn, PyTorch and more"},{"location":"examples/#api","title":"API","text":"<p>Run distributed txtai, integrate with the API and cloud endpoints.</p> Notebook Description API Gallery Using txtai in JavaScript, Java, Rust and Go Distributed embeddings cluster Distribute an embeddings index across multiple data nodes Embeddings in the Cloud Load and use an embeddings index from the Hugging Face Hub Custom API Endpoints Extend the API with custom endpoints API Authorization and Authentication Add authorization, authentication and middleware dependencies to the API OpenAI Compatible API Connect to txtai with a standard OpenAI client library"},{"location":"examples/#architecture","title":"Architecture","text":"<p>Project architecture, data formats, external integrations, scale to production, benchmarks, and performance.</p> Notebook Description Anatomy of a txtai index Deep dive into the file formats behind a txtai embeddings index Embeddings components Composable search with vector, SQL and scoring components Customize your own embeddings database Ways to combine vector indexes with relational databases Building an efficient sparse keyword index in Python Fast and accurate sparse keyword indexing Benefits of hybrid search Improve accuracy with a combination of semantic and keyword search External database integration Store metadata in PostgreSQL, MariaDB, MySQL and more All about vector quantization Benchmarking scalar and product quantization methods External vectorization Vectorization with precomputed embeddings datasets and APIs Integrate txtai with Postgres Persist content, vectors and graph data in Postgres Embeddings index format for open data access Platform and programming language independent data storage with txtai Accessing Low Level Vector APIs Build a vector database using txtai's low-level APIs"},{"location":"examples/#releases","title":"Releases","text":"<p>New functionality added in major releases.</p> Notebook Description What's new in txtai 4.0 Content storage, SQL, object storage, reindex and compressed indexes What's new in txtai 6.0 Sparse, hybrid and subindexes for embeddings, LLM improvements What's new in txtai 7.0 Semantic graph 2.0, LoRA/QLoRA training and binary API support What's new in txtai 8.0 Agents with txtai What's new in txtai 9.0 Learned sparse vectors, late interaction models and rerankers"},{"location":"examples/#applications","title":"Applications","text":"<p>Series of example applications with txtai. Links to hosted versions on Hugging Face Spaces are also provided, when available.</p> Application Description Basic similarity search Basic similarity search example. Data from the original txtai demo. \ud83e\udd17 Baseball stats Match historical baseball player stats using vector search. \ud83e\udd17 Benchmarks Calculate performance metrics for the BEIR datasets. Local run only Book search Book similarity search application. Index book descriptions and query using natural language statements. Local run only Image search Image similarity search application. Index a directory of images and run searches to identify images similar to the input query. \ud83e\udd17 Retrieval Augmented Generation RAG with txtai embeddings databases. Ask questions and get answers from LLMs bound by a context. Local run only Summarize an article Summarize an article. Workflow that extracts text from a webpage and builds a summary. \ud83e\udd17 Wiki search Wikipedia search application. Queries Wikipedia API and summarizes the top result. \ud83e\udd17 Workflow builder Build and execute txtai workflows. Connect summarization, text extraction, transcription, translation and similarity search pipelines together to run unified workflows. \ud83e\udd17"},{"location":"faq/","title":"FAQ","text":"<p>Below is a list of frequently asked questions and common issues encountered.</p>"},{"location":"faq/#questions","title":"Questions","text":"<p>Question</p> <p>What models are recommended?</p> <p>Answer</p> <p>See the model guide.</p> <p>Question</p> <p>What is the best way to track the progress of an <code>embeddings.index</code> call?</p> <p>Answer</p> <p>Wrap the list or generator passed to the index call with tqdm. See #478 for more.</p> <p>Question</p> <p>What is the best way to analyze and debug a txtai process?</p> <p>Answer</p> <p>See the observability section for more on how this can be enabled in txtai processes.</p> <p>txtai also has a console application. This article has more details.</p> <p>Question</p> <p>How can models be externally loaded and passed to embeddings and pipelines?</p> <p>Answer</p> <p>Embeddings example.</p> <pre><code>from transformers import AutoModel, AutoTokenizer\nfrom txtai import Embeddings\n\n# Load model externally\nmodel = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Pass to embeddings instance\nembeddings = Embeddings(path=model, tokenizer=tokenizer)\n</code></pre> <p>LLM pipeline example.</p> <pre><code>import torch\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom txtai import LLM\n\n# Load Qwen3 0.6B\npath = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(\n  path,\n  dtype=torch.bfloat16,\n)\ntokenizer = AutoTokenizer.from_pretrained(path)\n\nllm = LLM((model, tokenizer))\n</code></pre>"},{"location":"faq/#common-issues","title":"Common issues","text":"<p>Issue</p> <p>Embeddings query errors like this:</p> <pre><code>SQLError: no such function: json_extract\n</code></pre> <p>Solution</p> <p>Upgrade Python version as it doesn't have SQLite support for <code>json_extract</code></p> <p>Issue</p> <p>Segmentation faults and similar errors on macOS</p> <p>Solution</p> <p>Set the following environment parameters.</p> <ul> <li>Disable OpenMP multithreading via <code>export OMP_NUM_THREADS=1</code></li> <li>Workaround <code>OMP: Error #15</code> errors via <code>export KMP_DUPLICATE_LIB_OK=TRUE</code></li> <li>Disable PyTorch MPS device via <code>export PYTORCH_MPS_DISABLE=1</code></li> <li>Disable llama.cpp metal via <code>export LLAMA_NO_METAL=1</code></li> </ul> <p>For more details, refer to this issue on GitHub.</p> <p>Issue</p> <p>Error running SQLite ANN on macOS</p> <pre><code>AttributeError: 'sqlite3.Connection' object has no attribute 'enable_load_extension'\n</code></pre> <p>Solution</p> <p>See this note for options on how to fix this.</p> <p>Issue</p> <p><code>ContextualVersionConflict</code> and/or package METADATA exception while running one of the examples notebooks on Google Colab</p> <p>Solution</p> <p>Restart the kernel. See issue #409 for more on this issue. </p> <p>Issue</p> <p>Error installing optional/extra dependencies such as <code>pipeline</code></p> <p>Solution</p> <p>The default MacOS shell (zsh) and Windows PowerShell require escaping square brackets</p> <pre><code>pip install 'txtai[pipeline]'\n</code></pre>"},{"location":"further/","title":"Further reading","text":"<ul> <li>Introducing txtai, the all-in-one AI framework</li> <li>Tutorial series on Hashnode | dev.to</li> <li>What's new in txtai 9.0 | 8.0 | 7.0 | 6.0 | 5.0 | 4.0</li> <li>Getting started with semantic search | workflows | rag</li> <li>Running txtai at scale</li> <li>Vector search &amp; RAG Landscape: A review with txtai</li> </ul>"},{"location":"install/","title":"Installation","text":"<p>The easiest way to install is via pip and PyPI</p> <pre><code>pip install txtai\n</code></pre> <p>Python 3.10+ is supported. Using a Python virtual environment is recommended.</p>"},{"location":"install/#optional-dependencies","title":"Optional dependencies","text":"<p>txtai has the following optional dependencies that can be installed as extras. The patterns below are supported in setup.py install_requires sections.</p> <p>Note: Extras are provided for convenience. Alternatively, individual packages can be installed to limit dependencies.</p>"},{"location":"install/#all","title":"All","text":"<p>Install all dependencies.</p> <pre><code>pip install txtai[all]\n</code></pre>"},{"location":"install/#ann","title":"ANN","text":"<p>Additional ANN backends.</p> <pre><code>pip install txtai[ann]\n</code></pre>"},{"location":"install/#api","title":"API","text":"<p>Serve txtai via a web API.</p> <pre><code>pip install txtai[api]\n</code></pre>"},{"location":"install/#cloud","title":"Cloud","text":"<p>Interface with cloud compute.</p> <pre><code>pip install txtai[cloud]\n</code></pre>"},{"location":"install/#console","title":"Console","text":"<p>Command line index query console.</p> <pre><code>pip install txtai[console]\n</code></pre>"},{"location":"install/#database","title":"Database","text":"<p>Additional content storage options.</p> <pre><code>pip install txtai[database]\n</code></pre>"},{"location":"install/#graph","title":"Graph","text":"<p>Topic modeling, data connectivity and network analysis.</p> <pre><code>pip install txtai[graph]\n</code></pre>"},{"location":"install/#model","title":"Model","text":"<p>Additional non-standard models.</p> <pre><code>pip install txtai[model]\n</code></pre>"},{"location":"install/#pipeline","title":"Pipeline","text":"<p>All pipelines - default install comes with most common pipelines.</p> <pre><code>pip install txtai[pipeline]\n</code></pre> <p>More granular extras are available for pipeline categories: <code>pipeline-audio</code>, <code>pipeline-data</code>, <code>pipeline-image</code>, <code>pipeline-llm</code>, <code>pipeline-text</code>, and <code>pipeline-train</code>.</p>"},{"location":"install/#scoring","title":"Scoring","text":"<p>Additional scoring methods.</p> <pre><code>pip install txtai[scoring]\n</code></pre>"},{"location":"install/#vectors","title":"Vectors","text":"<p>Additional vector methods.</p> <pre><code>pip install txtai[vectors]\n</code></pre>"},{"location":"install/#workflow","title":"Workflow","text":"<p>All workflow tasks - default install comes with most common workflow tasks.</p> <pre><code>pip install txtai[workflow]\n</code></pre>"},{"location":"install/#combining-dependencies","title":"Combining dependencies","text":"<p>Multiple dependencies can be specified at the same time.</p> <pre><code>pip install txtai[pipeline,workflow]\n</code></pre>"},{"location":"install/#environment-specific-prerequisites","title":"Environment specific prerequisites","text":"<p>Additional environment specific prerequisites are below.</p>"},{"location":"install/#linux","title":"Linux","text":"<p>The AudioStream and Microphone pipelines require the PortAudio system library. The Transcription pipeline requires the SoundFile system library.</p>"},{"location":"install/#macos","title":"macOS","text":"<p>Older versions of Faiss have a runtime dependency on <code>libomp</code> for macOS. Run <code>brew install libomp</code> in this case.</p> <p>The AudioStream and Microphone pipelines require the PortAudio system library. Run <code>brew install portaudio</code>.</p>"},{"location":"install/#windows","title":"Windows","text":"<p>Optional dependencies require C++ Build Tools</p> <p>The txtai build workflow occasionally has work arounds for other known but temporary dependency issues. The FAQ also has a list of common problems, including common installation issues.</p>"},{"location":"install/#cpu-only","title":"CPU-only","text":"<p>The default install adds PyTorch with GPU support. There are a number of dependencies that come with that. When running in a CPU-only environment or using Embeddings/LLM models without PyTorch (i.e. llama.cpp or API services), the CPU-only PyTorch package can be installed with txtai as follows.</p> <pre><code>pip install txtai torch==[version]+cpu \\\n-f https://download.pytorch.org/whl/torch\n</code></pre> <p>Where <code>[version]</code> is the version of PyTorch (such as 2.4.1). The txtai-cpu image on Docker Hub uses this method to reduce the image size.</p>"},{"location":"install/#install-from-source","title":"Install from source","text":"<p>txtai can also be installed directly from GitHub to access the latest, unreleased features.</p> <pre><code>pip install git+https://github.com/neuml/txtai\n</code></pre> <p>Extras can be installed from GitHub by adding <code>#egg=txtai[&lt;name-of-extra&gt;]</code> to the end of the above URL.</p>"},{"location":"install/#conda","title":"Conda","text":"<p>A community-supported txtai package is available via conda-forge.</p> <pre><code>conda install -c conda-forge txtai\n</code></pre>"},{"location":"install/#run-with-containers","title":"Run with containers","text":"<p>Docker images are available for txtai. See this section for more information on container-based installs.</p>"},{"location":"models/","title":"Model guide","text":"<p>See the table below for the current recommended models. These models all allow commercial use and offer a blend of speed and performance.</p> Component Model(s) Embeddings all-MiniLM-L6-v2 Image Captions BLIP Labels - Zero Shot BART-Large-MNLI Labels - Fixed Fine-tune with training pipeline Large Language Model (LLM) gpt-oss-20b Summarization DistilBART Text-to-Speech ESPnet JETS Transcription Whisper Translation OPUS Model Series <p>Models can be loaded as either a path from the Hugging Face Hub or a local directory. Model paths are optional, defaults are loaded when not specified. For tasks with no recommended model, txtai uses the default models as shown in the Hugging Face Tasks guide.</p> <p>See the following links to learn more.</p> <ul> <li>Hugging Face Tasks</li> <li>Hugging Face Model Hub</li> <li>MTEB Leaderboard</li> <li>LMSYS LLM Leaderboard</li> <li>Open LLM Leaderboard</li> </ul>"},{"location":"observability/","title":"Observability","text":"<p>Observability enables tracking the inner workings of a system without having to change the system. This makes it much easier to debug and evaluate overall performance.</p> <p><code>txtai</code> has an integration with MLflow and it's tracing module to provide insights into each of the components in <code>txtai</code>.</p>"},{"location":"observability/#examples","title":"Examples","text":"<p>The following shows a number of examples on how to introduce observability into a <code>txtai</code> process.</p>"},{"location":"observability/#initialization","title":"Initialization","text":"<p>Run the following sections first to initialize tracing.</p> <pre><code># Install MLflow plugin for txtai\npip install mlflow-txtai\n\n# Start a local MLflow service\nmlflow server --host 127.0.0.1 --port 8000\n</code></pre> <pre><code>import mlflow\n\nmlflow.set_tracking_uri(uri=\"http://localhost:8000\")\nmlflow.set_experiment(\"txtai\")\n\n# Enable txtai automatic tracing\nmlflow.txtai.autolog()\n</code></pre>"},{"location":"observability/#textractor","title":"Textractor","text":"<p>The first example traces a Textractor pipeline.</p> <pre><code>from txtai.pipeline import Textractor\n\nwith mlflow.start_run():\n    textractor = Textractor()\n    textractor(\"https://github.com/neuml/txtai\")\n</code></pre> <p></p>"},{"location":"observability/#embeddings","title":"Embeddings","text":"<p>Next, we'll trace an Embeddings query.</p> <pre><code>from txtai import Embeddings\n\nwith mlflow.start_run():\n    wiki = Embeddings()\n    wiki.load(provider=\"huggingface-hub\", container=\"neuml/txtai-wikipedia-slim\")\n\n    embeddings = Embeddings(content=True, graph=True)\n    embeddings.index(wiki.search(\"SELECT id, text FROM txtai LIMIT 25\"))\n\n    embeddings.search(\"MATCH (A)-[]-&gt;(B) RETURN A\")\n</code></pre> <p> </p>"},{"location":"observability/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<p>The next example traces a RAG pipeline.</p> <pre><code>from txtai import Embeddings, RAG\n\nwith mlflow.start_run():\n    wiki = Embeddings()\n    wiki.load(provider=\"huggingface-hub\", container=\"neuml/txtai-wikipedia-slim\")\n\n    # Define prompt template\n    template = \"\"\"\n    Answer the following question using only the context below. Only include information\n    specifically discussed.\n\n    question: {question}\n    context: {context} \"\"\"\n\n    # Create RAG pipeline\n    rag = RAG(\n        wiki,\n        \"openai/gpt-oss-20b\",\n        system=\"You are a friendly assistant. You answer questions from users.\",\n        template=template,\n        context=10\n    )\n\n    rag(\"Tell me about the Roman Empire\", maxlength=2048)\n</code></pre> <p></p>"},{"location":"observability/#workflow","title":"Workflow","text":"<p>This example runs a workflow. This workflow runs an embeddings query and then translates each result to French. </p> <pre><code>from txtai import Embeddings, Workflow\nfrom txtai.pipeline import Translation\nfrom txtai.workflow import Task\n\nwith mlflow.start_run():\n    wiki = Embeddings()\n    wiki.load(provider=\"huggingface-hub\", container=\"neuml/txtai-wikipedia-slim\")\n\n    # Translation instance\n    translate = Translation()\n\n    workflow = Workflow([\n        Task(lambda x: [y[0][\"text\"] for y in wiki.batchsearch(x, 1)]),\n        Task(lambda x: translate(x, \"fr\"))\n    ])\n\n    print(list(workflow([\"Roman Empire\", \"Greek Empire\", \"Industrial Revolution\"])))\n</code></pre> <p></p>"},{"location":"observability/#agent","title":"Agent","text":"<p>The last example runs a txtai agent designed to research questions on astronomy.</p> <pre><code>from txtai import Agent, Embeddings\n\ndef search(query):\n    \"\"\"\n    Searches a database of astronomy data.\n\n    Make sure to call this tool only with a string input, never use JSON.    \n\n    Args:\n        query: concepts to search for using similarity search\n\n    Returns:\n        list of search results with for each match\n    \"\"\"\n\n    return embeddings.search(\n        \"SELECT id, text, distance FROM txtai WHERE similar(:query)\",\n        10, parameters={\"query\": query}\n    )\n\nembeddings = Embeddings()\nembeddings.load(provider=\"huggingface-hub\", container=\"neuml/txtai-astronomy\")\n\nagent = Agent(\n    tools=[search],\n    llm=\"Qwen/Qwen3-4B-Instruct-2507\",\n    max_steps=10,\n)\n\nresearcher = \"\"\"\n{command}\n\nDo the following.\n - Search for results related to the topic.\n - Analyze the results\n - Continue querying until conclusive answers are found\n - Write a Markdown report\n\"\"\"\n\nwith mlflow.start_run():\n    agent(researcher.format(command=\"\"\"\n    Write a detailed list with explanations of 10 candidate stars that could potentially be habitable to life.\n    \"\"\"), maxlength=16000)\n</code></pre> <p></p>"},{"location":"observability/#read-more","title":"Read more","text":"<p>Check out the mlflow-txtai project to see more examples.</p>"},{"location":"poweredby/","title":"Powered by txtai","text":"<p>The following applications are powered by txtai. </p> <p></p> Application Description rag Retrieval Augmented Generation (RAG) application ncoder Open-Source AI coding agent paperai AI for medical and scientific papers annotateai Automatically annotate papers with LLMs <p>In addition to this list, there are also many other open-source projects, published research and closed proprietary/commercial projects that have built on txtai in production.</p>"},{"location":"usecases/","title":"Use Cases","text":"<p>The following sections introduce common txtai use cases. A comprehensive set of over 70 example notebooks and applications are also available.</p>"},{"location":"usecases/#semantic-search","title":"Semantic Search","text":"<p>Build semantic/similarity/vector/neural search applications.</p> <p></p> <p>Traditional search systems use keywords to find data. Semantic search has an understanding of natural language and identifies results that have the same meaning, not necessarily the same keywords.</p> <p> </p> <p>Get started with the following examples.</p> Notebook Description Introducing txtai \u25b6\ufe0f Overview of the functionality provided by txtai Similarity search with images Embed images and text into the same space for search Build a QA database Question matching with semantic search Semantic Graphs Explore topics, data connectivity and run network analysis"},{"location":"usecases/#llm-orchestration","title":"LLM Orchestration","text":"<p>Autonomous agents, retrieval augmented generation (RAG), chat with your data, pipelines and workflows that interface with large language models (LLMs).</p> <p></p> <p>See below to learn more.</p> Notebook Description Prompt templates and task chains Build model prompts and connect tasks together with workflows Integrate LLM frameworks Integrate llama.cpp, LiteLLM and custom generation frameworks Build knowledge graphs with LLMs Build knowledge graphs with LLM-driven entity extraction"},{"location":"usecases/#agents","title":"Agents","text":"<p>Agents connect embeddings, pipelines, workflows and other agents together to autonomously solve complex problems.</p> <p></p> <p>txtai agents are built on top of the smolagents framework. This supports all LLMs txtai supports (Hugging Face, llama.cpp, OpenAI / Claude / AWS Bedrock via LiteLLM). Agent prompting with <code>agents.md</code> and <code>skill.md</code> are also supported.</p> <p>Check out this Agent Quickstart Example. Additional examples are listed below.</p> Notebook Description Analyzing Hugging Face Posts with Graphs and Agents Explore a rich dataset with Graph Analysis and Agents Granting autonomy to agents Agents that iteratively solve problems as they see fit Analyzing LinkedIn Company Posts with Graphs and Agents Exploring how to improve social media engagement with AI"},{"location":"usecases/#retrieval-augmented-generation","title":"Retrieval augmented generation","text":"<p>Retrieval augmented generation (RAG) reduces the risk of LLM hallucinations by constraining the output with a knowledge base as context. RAG is commonly used to \"chat with your data\".</p> <p> </p> <p>Check out this RAG Quickstart Example. Additional examples are listed below.</p> Notebook Description Build RAG pipelines with txtai \u25b6\ufe0f Guide on retrieval augmented generation including how to create citations RAG is more than Vector Search Context retrieval via Web, SQL and other sources GraphRAG with Wikipedia and GPT OSS Deep graph search powered RAG Speech to Speech RAG \u25b6\ufe0f Full cycle speech to speech workflow with RAG"},{"location":"usecases/#language-model-workflows","title":"Language Model Workflows","text":"<p>Language model workflows, also known as semantic workflows, connect language models together to build intelligent applications.</p> <p> </p> <p>While LLMs are powerful, there are plenty of smaller, more specialized models that work better and faster for specific tasks. This includes models for extractive question-answering, automatic summarization, text-to-speech, transcription and translation.</p> <p>Check out this Workflow Quickstart Example. Additional examples are listed below.</p> Notebook Description Run pipeline workflows \u25b6\ufe0f Simple yet powerful constructs to efficiently process data Building abstractive text summaries Run abstractive text summarization Transcribe audio to text Convert audio files to text Translate text between languages Streamline machine translation and language detection"},{"location":"why/","title":"Why txtai?","text":"<p>New vector databases, LLM frameworks and everything in between are sprouting up daily. Why build with txtai?</p> <ul> <li>Up and running in minutes with pip or Docker <pre><code># Get started in a couple lines\nimport txtai\n\nembeddings = txtai.Embeddings()\nembeddings.index([\"Correct\", \"Not what we hoped\"])\nembeddings.search(\"positive\", 1)\n#[(0, 0.29862046241760254)]\n</code></pre></li> <li>Built-in API makes it easy to develop applications using your programming language of choice <pre><code># app.yml\nembeddings:\n    path: sentence-transformers/all-MiniLM-L6-v2\n</code></pre> <pre><code>CONFIG=app.yml uvicorn \"txtai.api:app\"\ncurl -X GET \"http://localhost:8000/search?query=positive\"\n</code></pre></li> <li>Run local - no need to ship data off to disparate remote services</li> <li>Work with micromodels all the way up to large language models (LLMs)</li> <li>Low footprint - install additional dependencies and scale up when needed</li> <li>Learn by example - notebooks cover all available functionality</li> </ul>"},{"location":"agent/","title":"Agent","text":"<p>An agent automatically creates workflows to answer multi-faceted user requests. Agents iteratively prompt and/or interface with tools to step through a process and ultimately come to an answer for a request.</p> <p>Agent prompting with <code>agents.md</code> and <code>skill.md</code> are also supported. Read the configuration for more on how to setup those up.</p> <p>Agents excel at complex tasks where multiple tools and/or methods are required. They incorporate a level of randomness similar to different people working on the same task. When the request is simple and/or there is a rule-based process, other methods such as RAG and Workflows should be explored.</p> <p>The following code snippet defines a basic agent.</p> <pre><code>from datetime import datetime\n\nfrom txtai import Agent\n\nwikipedia = {\n    \"name\": \"wikipedia\",\n    \"description\": \"Searches a Wikipedia database\",\n    \"provider\": \"huggingface-hub\",\n    \"container\": \"neuml/txtai-wikipedia\"\n}\n\narxiv = {\n    \"name\": \"arxiv\",\n    \"description\": \"Searches a database of scientific papers\",\n    \"provider\": \"huggingface-hub\",\n    \"container\": \"neuml/txtai-arxiv\"\n}\n\ndef today() -&gt; str:\n    \"\"\"\n    Gets the current date and time\n\n    Returns:\n        current date and time\n    \"\"\"\n\n    return datetime.today().isoformat()\n\nagent = Agent(\n    model=\"Qwen/Qwen3-4B-Instruct-2507\",\n    tools=[today, wikipedia, arxiv, \"websearch\"],\n    max_steps=10,\n)\n</code></pre> <p>The agent above has access to two embeddings databases (Wikipedia and ArXiv) and the web. Given the user's input request, the agent decides the best tool to solve the task.</p>"},{"location":"agent/#example","title":"Example","text":"<p>The first example will solve a problem with multiple data points. See below.</p> <pre><code>agent(\"Which city has the highest population, Boston or New York?\")\n</code></pre> <p>This requires looking up the population of each city before knowing how to answer the question. Multiple search requests are run to generate a final answer.</p>"},{"location":"agent/#agentic-rag","title":"Agentic RAG","text":"<p>Standard retrieval augmented generation (RAG) runs a single vector search to obtain a context and builds a prompt with the context + input question. Agentic RAG is a more complex process that goes through multiple iterations. It can also utilize multiple databases to come to a final conclusion.</p> <p>The example below aggregates information from multiple sources and builds a report on a topic.</p> <pre><code>researcher = \"\"\"\nYou're an expert researcher looking to write a paper on {topic}.\nSearch for websites, scientific papers and Wikipedia related to the topic.\nWrite a report with summaries and references (with hyperlinks).\nWrite the text as Markdown.\n\"\"\"\n\nagent(researcher.format(topic=\"alien life\"))\n</code></pre>"},{"location":"agent/#agent-teams","title":"Agent Teams","text":"<p>Agents can also be tools. This enables the concept of building \"Agent Teams\" to solve problems. The previous example can be rewritten as a list of agents.</p> <pre><code>from txtai import Agent, LLM\n\nllm = LLM(\"Qwen/Qwen3-4B-Instruct-2507\")\n\nwebsearcher = Agent(\n    model=llm,\n    tools=[\"websearch\"],\n)\n\nwikiman = Agent(\n    model=llm,\n    tools=[{\n        \"name\": \"wikipedia\",\n        \"description\": \"Searches a Wikipedia database\",\n        \"provider\": \"huggingface-hub\",\n        \"container\": \"neuml/txtai-wikipedia\"\n    }],\n)\n\nresearcher = Agent(\n    model=llm,\n    tools=[{\n        \"name\": \"arxiv\",\n        \"description\": \"Searches a database of scientific papers\",\n        \"provider\": \"huggingface-hub\",\n        \"container\": \"neuml/txtai-arxiv\"\n    }],\n)\n\nagent = Agent(\n    model=llm,\n    tools=[{\n        \"name\": \"websearcher\",\n        \"description\": \"I run web searches, there is no answer a web search can't solve!\",\n        \"target\": websearcher\n    }, {\n        \"name\": \"wikiman\",\n        \"description\": \"Wikipedia has all the answers, I search Wikipedia and answer questions\",\n        \"target\": wikiman\n    }, {\n        \"name\": \"researcher\",\n        \"description\": \"I'm a science guy. I search arXiv to get all my answers.\",\n        \"target\": researcher\n    }],\n    max_steps=10\n)\n</code></pre> <p>This provides another level of intelligence to the process. Instead of just a single tool execution, each agent-tool combination has it's own reasoning engine.</p> <pre><code>agent(\"\"\"\nResearch fundamental concepts about Signal Processing and build a comprehensive report.\nWrite the output in Markdown.\n\"\"\")\n</code></pre>"},{"location":"agent/#more-examples","title":"More examples","text":"<p>Check out this Agent Quickstart Example. Additional examples are listed below.</p> Notebook Description What's new in txtai 8.0 Agents with txtai Analyzing Hugging Face Posts with Graphs and Agents Explore a rich dataset with Graph Analysis and Agents Granting autonomy to agents Agents that iteratively solve problems as they see fit Analyzing LinkedIn Company Posts with Graphs and Agents Exploring how to improve social media engagement with AI Parsing the stars with txtai Explore an astronomical knowledge graph of known stars, planets, galaxies Agentic College Search Identify list of strong engineering colleges TxtAI got skills Integrate skill.md files with your agent"},{"location":"agent/configuration/","title":"Configuration","text":"<p>An agent takes two main arguments, an LLM and a list of tools.</p> <p>The txtai agent framework is built with smolagents. Additional options can be passed in the <code>Agent</code> constructor.</p> <pre><code>from datetime import datetime\n\nfrom txtai import Agent\n\nwikipedia = {\n    \"name\": \"wikipedia\",\n    \"description\": \"Searches a Wikipedia database\",\n    \"provider\": \"huggingface-hub\",\n    \"container\": \"neuml/txtai-wikipedia\"\n}\n\narxiv = {\n    \"name\": \"arxiv\",\n    \"description\": \"Searches a database of scientific papers\",\n    \"provider\": \"huggingface-hub\",\n    \"container\": \"neuml/txtai-arxiv\"\n}\n\ndef today() -&gt; str:\n    \"\"\"\n    Gets the current date and time\n\n    Returns:\n        current date and time\n    \"\"\"\n\n    return datetime.today().isoformat()\n\nagent = Agent(\n    model=\"Qwen/Qwen3-4B-Instruct-2507\",\n    tools=[today, wikipedia, arxiv, \"websearch\"],\n)\n</code></pre>"},{"location":"agent/configuration/#model","title":"model","text":"<pre><code>model: string|llm instance\n</code></pre> <p>LLM model path or LLM pipeline instance. The <code>llm</code> parameter is also supported for backwards compatibility.</p> <p>See the LLM pipeline for more information.</p>"},{"location":"agent/configuration/#tools","title":"tools","text":"<pre><code>tools: list\n</code></pre> <p>List of tools to supply to the agent. Supports the following configurations.</p>"},{"location":"agent/configuration/#function","title":"function","text":"<p>A function tool takes the following dictionary fields.</p> Field Description name name of the tool description tool description target target method / callable <p>A function or callable method can also be directly supplied in the <code>tools</code> list. In this case, the fields are inferred from the method documentation.</p>"},{"location":"agent/configuration/#embeddings","title":"embeddings","text":"<p>Embeddings indexes have built-in support. Provide the following dictionary configuration to add an embeddings index as a tool.</p> Field Description name embeddings index name description embeddings index description **kwargs Parameters to pass to embeddings.load"},{"location":"agent/configuration/#tool","title":"tool","text":"<p>The following shortcut strings load tools directly. Passing a Tool instance is also supported.</p> Tool Description http.* HTTP Path to a Model Context Protocol (MCP) server python Runs a Python action *.md Loads a <code>skill.md</code> file websearch Runs a websearch using the built-in websearch tool webview Extracts content from a web page"},{"location":"agent/configuration/#instructions","title":"instructions","text":"<pre><code>instructions: string|path\n</code></pre> <p>Supports loading an <code>agents.md</code> file. Can be provided directly as a string or as a path to a file.</p> <p>Read more about agents.md here</p>"},{"location":"agent/configuration/#template","title":"template","text":"<pre><code>template: string\n</code></pre> <p>Customize the prompt template used by this agent. Supports Jinja templates. Uses a default template when this parameter is not provided. Must include <code>{{ text }}</code> and <code>{{ memory }}</code> placeholders.</p>"},{"location":"agent/configuration/#memory","title":"memory","text":"<pre><code>memory: int\n</code></pre> <p>Keeps a rolling window of <code>memory</code> inputs and outputs. These are added to future prompts and serve as \"agent memory\".</p> <p>Supports storing memory by <code>session</code> to enable multiple conversation threads. Defaults to shared memory when not set. See the method documentation for more information.</p>"},{"location":"agent/configuration/#method","title":"method","text":"<pre><code>method: code|tool\n</code></pre> <p>Sets the agent method. Supports either a <code>code</code> or <code>tool</code> (default) calling agent. A code agent generates Python code and executes that. A tool calling agent generates JSON blocks and calls the agents within those blocks.</p> <p>Additional options can be directly passed. See CodeAgent or ToolCallingAgent for a list of parameters.</p> <p>Read more here.</p>"},{"location":"agent/methods/","title":"Methods","text":""},{"location":"agent/methods/#txtai.agent.base.Agent.__init__","title":"<code>__init__(template=None, memory=None, **kwargs)</code>","text":"<p>Creates a new Agent.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <p>optional prompt jinja template, must include {{ text }} and {{ memory }} placeholders</p> <code>None</code> <code>memory</code> <p>number of prior outputs to keep as \"memory\", defaults to None for no memory</p> <code>None</code> <code>kwargs</code> <p>arguments to pass to the underlying Agent backend and LLM pipeline instance</p> <code>{}</code> Source code in <code>txtai/agent/base.py</code> <pre><code>def __init__(self, template=None, memory=None, **kwargs):\n    \"\"\"\n    Creates a new Agent.\n\n    Args:\n        template: optional prompt jinja template, must include {{ text }} and {{ memory }} placeholders\n        memory: number of prior outputs to keep as \"memory\", defaults to None for no memory\n        kwargs: arguments to pass to the underlying Agent backend and LLM pipeline instance\n    \"\"\"\n\n    # Ensure backwards compatibility\n    if \"max_iterations\" in kwargs:\n        kwargs[\"max_steps\"] = kwargs.pop(\"max_iterations\")\n\n    # Custom instructions\n    if \"instructions\" in kwargs:\n        kwargs[\"instructions\"] = self.instructions(kwargs)\n\n    # Create agent process runner\n    self.process = ProcessFactory.create(kwargs)\n\n    # Tools dictionary\n    self.tools = self.process.tools\n\n    # Agent memory\n    self.memory = {}\n    self.window = memory\n    self.template = template\n</code></pre>"},{"location":"agent/methods/#txtai.agent.base.Agent.__call__","title":"<code>__call__(text, maxlength=8192, stream=False, session=None, reset=False, **kwargs)</code>","text":"<p>Runs an agent loop.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>instructions to run</p> required <code>maxlength</code> <p>maximum sequence length</p> <code>8192</code> <code>stream</code> <p>stream response if True, defaults to False</p> <code>False</code> <code>session</code> <p>session id for stored memory, defaults to None which shares all memory</p> <code>None</code> <code>reset</code> <p>clears previously stored memory if True, defaults to False</p> <code>False</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <p>result</p> Source code in <code>txtai/agent/base.py</code> <pre><code>def __call__(self, text, maxlength=8192, stream=False, session=None, reset=False, **kwargs):\n    \"\"\"\n    Runs an agent loop.\n\n    Args:\n        text: instructions to run\n        maxlength: maximum sequence length\n        stream: stream response if True, defaults to False\n        session: session id for stored memory, defaults to None which shares all memory\n        reset: clears previously stored memory if True, defaults to False\n        kwargs: additional keyword arguments\n\n    Returns:\n        result\n    \"\"\"\n\n    # Process parameters\n    self.process.model.parameters(maxlength)\n\n    # Create memory, if necessary\n    if self.window and (session not in self.memory or reset):\n        self.memory[session] = deque(maxlen=self.window)\n\n    # Run agent loop\n    output = self.process.run(self.prompt(text, session), stream=stream, **kwargs)\n\n    # Add output to memory, if necessary\n    if session in self.memory:\n        self.memory[session].append((text, output))\n\n    return output\n</code></pre>"},{"location":"api/","title":"API","text":"<p>txtai has a full-featured API, backed by FastAPI, that can optionally be enabled for any txtai process. All functionality found in txtai can be accessed via the API.</p> <p>The following is an example configuration and startup script for the API.</p> <p>Note: This configuration file enables all functionality. For memory-bound systems, splitting pipelines into multiple instances is a best practice.</p> <pre><code># Index file path\npath: /tmp/index\n\n# Allow indexing of documents\nwritable: True\n\n# Enbeddings index\nembeddings:\n  path: sentence-transformers/nli-mpnet-base-v2\n\n# Extractive QA\nextractor:\n  path: distilbert-base-cased-distilled-squad\n\n# Zero-shot labeling\nlabels:\n\n# Similarity\nsimilarity:\n\n# Text segmentation\nsegmentation:\n    sentences: true\n\n# Text summarization\nsummary:\n\n# Text extraction\ntextractor:\n    paragraphs: true\n    minlength: 100\n    join: true\n\n# Transcribe audio to text\ntranscription:\n\n# Translate text between languages\ntranslation:\n\n# Workflow definitions\nworkflow:\n    sumfrench:\n        tasks:\n            - action: textractor\n              task: url\n            - action: summary\n            - action: translation\n              args: [\"fr\"]\n    sumspanish:\n        tasks:\n            - action: textractor\n              task: url\n            - action: summary\n            - action: translation\n              args: [\"es\"]\n</code></pre> <p>Assuming this YAML content is stored in a file named config.yml, the following command starts the API process.</p> <pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\"\n</code></pre> <p>Uvicorn is a full-featured production-ready server. See the Uvicorn deployment guide for more on configuration options.</p>"},{"location":"api/#connect-to-api","title":"Connect to API","text":"<p>The default port for the API is 8000. See the uvicorn link above to change this.</p> <p>txtai has a number of language bindings which abstract the API (see links below). Alternatively, code can be written to connect directly to the API. Documentation for a live running instance can be found at the <code>/docs</code> url (i.e. http://localhost:8000/docs). The following example runs a workflow using cURL.</p> <pre><code>curl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"sumfrench\", \"elements\": [\"https://github.com/neuml/txtai\"]}'\n</code></pre>"},{"location":"api/#local-instance","title":"Local instance","text":"<p>A local instance can be instantiated. In this case, a txtai application runs internally, without any network connections, providing the same consolidated functionality. This enables running txtai in Python with configuration.</p> <p>The configuration above can be run in Python with:</p> <pre><code>from txtai import Application\n\n# Load and run workflow\napp = Application(config.yml)\napp.workflow(\"sumfrench\", [\"https://github.com/neuml/txtai\"])\n</code></pre> <p>See this link for a full list of methods.</p>"},{"location":"api/#run-with-containers","title":"Run with containers","text":"<p>The API can be containerized and run. This will bring up an API instance without having to install Python, txtai or any dependencies on your machine!</p> <p>See this section for more information.</p>"},{"location":"api/#supported-language-bindings","title":"Supported language bindings","text":"<p>The following programming languages have bindings with the txtai API:</p> <ul> <li>Python</li> <li>JavaScript</li> <li>Java</li> <li>Rust</li> <li>Go</li> </ul> <p>The API also supports hosting OpenAI-compatible and Model Context Protocol (MCP) endpoints.</p> <p>See the links below for detailed examples covering the API.</p> Notebook Description API Gallery Using txtai in JavaScript, Java, Rust and Go Distributed embeddings cluster Distribute an embeddings index across multiple data nodes Embeddings in the Cloud Load and use an embeddings index from the Hugging Face Hub Custom API Endpoints Extend the API with custom endpoints API Authorization and Authentication Add authorization, authentication and middleware dependencies to the API OpenAI Compatible API Connect to txtai with a standard OpenAI client library"},{"location":"api/cluster/","title":"Distributed embeddings clusters","text":"<p>The API supports combining multiple API instances into a single logical embeddings index. An example configuration is shown below.</p> <pre><code>cluster:\n    shards:\n        - http://127.0.0.1:8002\n        - http://127.0.0.1:8003\n</code></pre> <p>This configuration aggregates the API instances above as index shards. Data is evenly split among each of the shards at index time. Queries are run in parallel against each shard and the results are joined together. This method allows horizontal scaling and supports very large index clusters.</p> <p>This method is only recommended for data sets in the 1 billion+ records. The ANN libraries can easily support smaller data sizes and this method is not worth the additional complexity. At this time, new shards can not be added after building the initial index.</p> <p>See the link below for a detailed example covering distributed embeddings clusters.</p> Notebook Description Distributed embeddings cluster Distribute an embeddings index across multiple data nodes"},{"location":"api/configuration/","title":"Configuration","text":"<p>Configuration is set through YAML. In most cases, YAML keys map to fields names in Python. The example in the previous section gave a full-featured example covering a wide array of configuration options.</p> <p>Each section below describes the available configuration settings.</p>"},{"location":"api/configuration/#embeddings","title":"Embeddings","text":"<p>The configuration parser expects a top level <code>embeddings</code> key to be present in the YAML. All embeddings configuration is supported.</p> <p>The following example defines an embeddings index.</p> <pre><code>path: index path\nwritable: true\n\nembeddings:\n  path: vector model\n  content: true\n</code></pre> <p>Three top level settings are available to control where indexes are saved and if an index is a read-only index.</p>"},{"location":"api/configuration/#path","title":"path","text":"<pre><code>path: string\n</code></pre> <p>Path to save and load the embeddings index. Each API instance can only access a single index at a time.</p>"},{"location":"api/configuration/#writable","title":"writable","text":"<pre><code>writable: boolean\n</code></pre> <p>Determines if the input embeddings index is writable (true) or read-only (false). This allows serving a read-only index.</p>"},{"location":"api/configuration/#cloud","title":"cloud","text":"<p>Cloud storage settings can be set under a <code>cloud</code> top level configuration group.</p>"},{"location":"api/configuration/#agent","title":"Agent","text":"<p>Agents are defined under a top level <code>agent</code> key. Each key under the <code>agent</code> key is the name of the agent. Constructor parameters can be passed under this key.</p> <p>The following example defines an agent.</p> <pre><code>agent:\n    researcher:\n        tools:\n            - websearch\n\nllm:\n    path: Qwen/Qwen3-4B-Instruct-2507\n</code></pre>"},{"location":"api/configuration/#pipeline","title":"Pipeline","text":"<p>Pipelines are loaded as top level configuration parameters. Pipeline names are automatically detected in the YAML configuration and created upon startup. All pipelines are supported.</p> <p>The following example defines a series of pipelines. Note that entries below are the lower-case names of the pipeline class.</p> <pre><code>caption:\n\nextractor:\n  path: model path\n\nlabels:\n\nsummary:\n\ntabular:\n\ntranslation:\n</code></pre> <p>Under each pipeline name, configuration settings for the pipeline can be set.</p>"},{"location":"api/configuration/#workflow","title":"Workflow","text":"<p>Workflows are defined under a top level <code>workflow</code> key. Each key under the <code>workflow</code> key is the name of the workflow. Under that is a <code>tasks</code> key with each task definition.</p> <p>The following example defines a workflow.</p> <pre><code>workflow:\n  sumtranslate:\n    tasks:\n        - action: summary\n        - action: translation\n</code></pre>"},{"location":"api/configuration/#schedule","title":"schedule","text":"<p>Schedules a workflow using a cron expression.</p> <pre><code>workflow:\n  index:\n    schedule:\n      cron: 0/10 * * * * *\n      elements: [\"api params\"] \n    tasks:\n      - task: service\n        url: api url\n      - action: index\n</code></pre>"},{"location":"api/configuration/#tasks","title":"tasks","text":"<pre><code>tasks: list\n</code></pre> <p>Expects a list of workflow tasks. Each element defines a single workflow task. All task configuration is supported.</p> <p>A shorthand syntax for creating tasks is supported. This syntax will automatically map task strings to an <code>action:value</code> pair.</p> <p>Example below.</p> <pre><code>workflow:\n  index:\n    tasks:\n      - action1\n      - action2\n</code></pre> <p>Each task element supports the following additional arguments.</p>"},{"location":"api/configuration/#action","title":"action","text":"<pre><code>action: string|list\n</code></pre> <p>Both single and multi-action tasks are supported.</p> <p>The action parameter works slightly different when passed via configuration. The parameter(s) needs to be converted into callable method(s). If action is a pipeline that has been defined in the current configuration, it will use that pipeline as the action.</p> <p>There are three special action names <code>index</code>, <code>upsert</code> and <code>search</code>. If <code>index</code> or <code>upsert</code> are used as the action, the task will collect workflow data elements and load them into defined the embeddings index. If <code>search</code> is used, the task will execute embeddings queries for each input data element.</p> <p>Otherwise, the action must be a path to a callable object or function. The configuration parser will resolve the function name and use that as the task action.</p>"},{"location":"api/configuration/#task","title":"task","text":"<pre><code>task: string\n</code></pre> <p>Optionally sets the type of task to create. For example, this could be a <code>file</code> task or a <code>retrieve</code> task. If this is not specified, a generic task is created. The list of workflow tasks can be found here.</p>"},{"location":"api/configuration/#args","title":"args","text":"<pre><code>args: list\n</code></pre> <p>Optional list of static arguments to pass to the workflow task. These are combined with workflow data to pass to each <code>__call__</code>.</p>"},{"location":"api/customization/","title":"Customization","text":"<p>The txtai API has a number of features out of the box that are designed to help get started quickly. API services can also be augmented with custom code and functionality. The two main ways to do this are with extensions and dependencies.</p> <p>Extensions add a custom endpoint. Dependencies add middleware that executes with each request. See the sections below for more.</p>"},{"location":"api/customization/#extensions","title":"Extensions","text":"<p>While the API is extremely flexible and complex logic can be executed through YAML-driven workflows, some may prefer to create an endpoint in Python. API extensions define custom Python endpoints that interact with txtai applications. </p> <p>See the link below for a detailed example.</p> Notebook Description Custom API Endpoints Extend the API with custom endpoints"},{"location":"api/customization/#dependencies","title":"Dependencies","text":"<p>txtai has a default API token authorization method that works well in many cases. Dependencies can also add custom logic with each request. This could be an additional authorization step and/or an authentication method. </p> <p>See the link below for a detailed example.</p> Notebook Description API Authorization and Authentication Add authorization, authentication and middleware dependencies to the API"},{"location":"api/mcp/","title":"Model Context Protocol","text":"<p>The Model Context Protocol (MCP) is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools.</p> <p>The API can be configured to handle MCP requests. All enabled endpoints set in the API configuration are automatically added as MCP tools.</p> <pre><code>mcp: True\n</code></pre> <p>Once this configuration option is added, a new route is added to the application <code>/mcp</code>. </p> <p>The Model Context Protocol Inspector tool is a quick way to explore how the MCP tools are exported through this interface.</p> <p>Run the following and go to the local URL specified.</p> <pre><code>npx @modelcontextprotocol/inspector node build/index.js\n</code></pre> <p>Enter <code>http://localhost:8000/mcp</code> to see the full list of tools available.</p>"},{"location":"api/methods/","title":"Methods","text":""},{"location":"api/methods/#txtai.api.API","title":"<code>API</code>","text":"<p>               Bases: <code>Application</code></p> <p>Base API template. The API is an extended txtai application, adding the ability to cluster API instances together.</p> <p>Downstream applications can extend this base template to add/modify functionality.</p> Source code in <code>txtai/api/base.py</code> <pre><code>class API(Application):\n    \"\"\"\n    Base API template. The API is an extended txtai application, adding the ability to cluster API instances together.\n\n    Downstream applications can extend this base template to add/modify functionality.\n    \"\"\"\n\n    def __init__(self, config, loaddata=True):\n        super().__init__(config, loaddata)\n\n        # Embeddings cluster\n        self.cluster = None\n        if self.config.get(\"cluster\"):\n            self.cluster = Cluster(self.config[\"cluster\"])\n\n    # pylint: disable=W0221\n    def search(self, query, limit=None, weights=None, index=None, parameters=None, graph=False, request=None):\n        # When search is invoked via the API, limit is set from the request\n        # When search is invoked directly, limit is set using the method parameter\n        limit = self.limit(request.query_params.get(\"limit\") if request and hasattr(request, \"query_params\") else limit)\n        weights = self.weights(request.query_params.get(\"weights\") if request and hasattr(request, \"query_params\") else weights)\n        index = request.query_params.get(\"index\") if request and hasattr(request, \"query_params\") else index\n        parameters = request.query_params.get(\"parameters\") if request and hasattr(request, \"query_params\") else parameters\n        graph = request.query_params.get(\"graph\") if request and hasattr(request, \"query_params\") else graph\n\n        # Decode parameters\n        parameters = json.loads(parameters) if parameters and isinstance(parameters, str) else parameters\n\n        if self.cluster:\n            return self.cluster.search(query, limit, weights, index, parameters, graph)\n\n        return super().search(query, limit, weights, index, parameters, graph)\n\n    def batchsearch(self, queries, limit=None, weights=None, index=None, parameters=None, graph=False):\n        if self.cluster:\n            return self.cluster.batchsearch(queries, self.limit(limit), weights, index, parameters, graph)\n\n        return super().batchsearch(queries, limit, weights, index, parameters, graph)\n\n    def add(self, documents):\n        \"\"\"\n        Adds a batch of documents for indexing.\n\n        Downstream applications can override this method to also store full documents in an external system.\n\n        Args:\n            documents: list of {id: value, text: value}\n\n        Returns:\n            unmodified input documents\n        \"\"\"\n\n        if self.cluster:\n            self.cluster.add(documents)\n        else:\n            super().add(documents)\n\n        return documents\n\n    def index(self):\n        \"\"\"\n        Builds an embeddings index for previously batched documents.\n        \"\"\"\n\n        if self.cluster:\n            self.cluster.index()\n        else:\n            super().index()\n\n    def upsert(self):\n        \"\"\"\n        Runs an embeddings upsert operation for previously batched documents.\n        \"\"\"\n\n        if self.cluster:\n            self.cluster.upsert()\n        else:\n            super().upsert()\n\n    def delete(self, ids):\n        \"\"\"\n        Deletes from an embeddings index. Returns list of ids deleted.\n\n        Args:\n            ids: list of ids to delete\n\n        Returns:\n            ids deleted\n        \"\"\"\n\n        if self.cluster:\n            return self.cluster.delete(ids)\n\n        return super().delete(ids)\n\n    def reindex(self, config, function=None):\n        \"\"\"\n        Recreates this embeddings index using config. This method only works if document content storage is enabled.\n\n        Args:\n            config: new config\n            function: optional function to prepare content for indexing\n        \"\"\"\n\n        if self.cluster:\n            self.cluster.reindex(config, function)\n        else:\n            super().reindex(config, function)\n\n    def count(self):\n        \"\"\"\n        Total number of elements in this embeddings index.\n\n        Returns:\n            number of elements in embeddings index\n        \"\"\"\n\n        if self.cluster:\n            return self.cluster.count()\n\n        return super().count()\n\n    def limit(self, limit):\n        \"\"\"\n        Parses the number of results to return from the request. Allows range of 1-250, with a default of 10.\n\n        Args:\n            limit: limit parameter\n\n        Returns:\n            bounded limit\n        \"\"\"\n\n        # Return between 1 and 250 results, defaults to 10\n        return max(1, min(250, int(limit) if limit else 10))\n\n    def weights(self, weights):\n        \"\"\"\n        Parses the weights parameter from the request.\n\n        Args:\n            weights: weights parameter\n\n        Returns:\n            weights\n        \"\"\"\n\n        return float(weights) if weights else weights\n</code></pre>"},{"location":"api/methods/#txtai.api.API.add","title":"<code>add(documents)</code>","text":"<p>Adds a batch of documents for indexing.</p> <p>Downstream applications can override this method to also store full documents in an external system.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <p>list of {id: value, text: value}</p> required <p>Returns:</p> Type Description <p>unmodified input documents</p> Source code in <code>txtai/api/base.py</code> <pre><code>def add(self, documents):\n    \"\"\"\n    Adds a batch of documents for indexing.\n\n    Downstream applications can override this method to also store full documents in an external system.\n\n    Args:\n        documents: list of {id: value, text: value}\n\n    Returns:\n        unmodified input documents\n    \"\"\"\n\n    if self.cluster:\n        self.cluster.add(documents)\n    else:\n        super().add(documents)\n\n    return documents\n</code></pre>"},{"location":"api/methods/#txtai.api.API.addobject","title":"<code>addobject(data, uid, field)</code>","text":"<p>Helper method that builds a batch of object documents.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>object content</p> required <code>uid</code> <p>optional list of corresponding uids</p> required <code>field</code> <p>optional field to set</p> required <p>Returns:</p> Type Description <p>documents</p> Source code in <code>txtai/app/base.py</code> <pre><code>def addobject(self, data, uid, field):\n    \"\"\"\n    Helper method that builds a batch of object documents.\n\n    Args:\n        data: object content\n        uid: optional list of corresponding uids\n        field: optional field to set\n\n    Returns:\n        documents\n    \"\"\"\n\n    # Raise error if index is not writable\n    if not self.config.get(\"writable\"):\n        raise ReadOnlyError(\"Attempting to add documents to a read-only index (writable != True)\")\n\n    documents = []\n    for x, content in enumerate(data):\n        if field:\n            row = {\"id\": uid[x], field: content} if uid else {field: content}\n        elif uid:\n            row = (uid[x], content)\n        else:\n            row = content\n\n        documents.append(row)\n\n    return self.add(documents)\n</code></pre>"},{"location":"api/methods/#txtai.api.API.agent","title":"<code>agent(name, *args, **kwargs)</code>","text":"<p>Executes an agent.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>agent name</p> required <code>args</code> <p>agent positional arguments</p> <code>()</code> <code>kwargs</code> <p>agent keyword arguments</p> <code>{}</code> Source code in <code>txtai/app/base.py</code> <pre><code>def agent(self, name, *args, **kwargs):\n    \"\"\"\n    Executes an agent.\n\n    Args:\n        name: agent name\n        args: agent positional arguments\n        kwargs: agent keyword arguments\n    \"\"\"\n\n    if name in self.agents:\n        return self.agents[name](*args, **kwargs)\n\n    return None\n</code></pre>"},{"location":"api/methods/#txtai.api.API.batchexplain","title":"<code>batchexplain(queries, texts=None, limit=10)</code>","text":"<p>Explains the importance of each input token in text for a list of queries.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>queries text</p> required <code>texts</code> <p>optional list of text, otherwise runs search queries</p> <code>None</code> <code>limit</code> <p>optional limit if texts is None</p> <code>10</code> <p>Returns:</p> Type Description <p>list of dict per input text per query where a higher token scores represents higher importance relative to the query</p> Source code in <code>txtai/app/base.py</code> <pre><code>def batchexplain(self, queries, texts=None, limit=10):\n    \"\"\"\n    Explains the importance of each input token in text for a list of queries.\n\n    Args:\n        query: queries text\n        texts: optional list of text, otherwise runs search queries\n        limit: optional limit if texts is None\n\n    Returns:\n        list of dict per input text per query where a higher token scores represents higher importance relative to the query\n    \"\"\"\n\n    if self.embeddings:\n        with self.lock:\n            return self.embeddings.batchexplain(queries, texts, limit)\n\n    return None\n</code></pre>"},{"location":"api/methods/#txtai.api.API.batchsimilarity","title":"<code>batchsimilarity(queries, texts)</code>","text":"<p>Computes the similarity between list of queries and list of text. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the index in texts.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <p>queries text</p> required <code>texts</code> <p>list of text</p> required <p>Returns:</p> Type Description <p>list of {id: value, score: value} per query</p> Source code in <code>txtai/app/base.py</code> <pre><code>def batchsimilarity(self, queries, texts):\n    \"\"\"\n    Computes the similarity between list of queries and list of text. Returns a list\n    of {id: value, score: value} sorted by highest score per query, where id is the\n    index in texts.\n\n    Args:\n        queries: queries text\n        texts: list of text\n\n    Returns:\n        list of {id: value, score: value} per query\n    \"\"\"\n\n    # Use similarity instance if available otherwise fall back to embeddings model\n    if \"similarity\" in self.pipelines:\n        return [[{\"id\": uid, \"score\": float(score)} for uid, score in r] for r in self.pipelines[\"similarity\"](queries, texts)]\n    if self.embeddings:\n        return [[{\"id\": uid, \"score\": float(score)} for uid, score in r] for r in self.embeddings.batchsimilarity(queries, texts)]\n\n    return None\n</code></pre>"},{"location":"api/methods/#txtai.api.API.batchtransform","title":"<code>batchtransform(texts, category=None, index=None)</code>","text":"<p>Transforms list of text into embeddings arrays.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <p>list of text</p> required <code>category</code> <p>category for instruction-based embeddings</p> <code>None</code> <code>index</code> <p>index name, if applicable</p> <code>None</code> <p>Returns:</p> Type Description <p>embeddings arrays</p> Source code in <code>txtai/app/base.py</code> <pre><code>def batchtransform(self, texts, category=None, index=None):\n    \"\"\"\n    Transforms list of text into embeddings arrays.\n\n    Args:\n        texts: list of text\n        category: category for instruction-based embeddings\n        index: index name, if applicable\n\n    Returns:\n        embeddings arrays\n    \"\"\"\n\n    if self.embeddings:\n        return [[float(x) for x in result] for result in self.embeddings.batchtransform(texts, category, index)]\n\n    return None\n</code></pre>"},{"location":"api/methods/#txtai.api.API.count","title":"<code>count()</code>","text":"<p>Total number of elements in this embeddings index.</p> <p>Returns:</p> Type Description <p>number of elements in embeddings index</p> Source code in <code>txtai/api/base.py</code> <pre><code>def count(self):\n    \"\"\"\n    Total number of elements in this embeddings index.\n\n    Returns:\n        number of elements in embeddings index\n    \"\"\"\n\n    if self.cluster:\n        return self.cluster.count()\n\n    return super().count()\n</code></pre>"},{"location":"api/methods/#txtai.api.API.createagents","title":"<code>createagents()</code>","text":"<p>Create agents.</p> Source code in <code>txtai/app/base.py</code> <pre><code>def createagents(self):\n    \"\"\"\n    Create agents.\n    \"\"\"\n\n    # Agent definitions\n    self.agents = {}\n\n    # Create agents\n    if \"agent\" in self.config:\n        for agent, config in self.config[\"agent\"].items():\n            # Create copy of config\n            config = config.copy()\n\n            # Resolve LLM\n            config[\"llm\"] = self.function(\"llm\")\n\n            # Resolve tools\n            for tool in config.get(\"tools\", []):\n                if isinstance(tool, dict) and \"target\" in tool:\n                    tool[\"target\"] = self.function(tool[\"target\"])\n\n            # Create agent\n            self.agents[agent] = Agent(**config)\n</code></pre>"},{"location":"api/methods/#txtai.api.API.createpipelines","title":"<code>createpipelines()</code>","text":"<p>Create pipelines.</p> Source code in <code>txtai/app/base.py</code> <pre><code>def createpipelines(self):\n    \"\"\"\n    Create pipelines.\n    \"\"\"\n\n    # Pipeline definitions\n    self.pipelines = {}\n\n    # Default pipelines\n    pipelines = list(PipelineFactory.list().keys())\n\n    # Add custom pipelines\n    for key in self.config:\n        if \".\" in key:\n            pipelines.append(key)\n\n    # Move dependent pipelines to end of list\n    dependent = [\"similarity\", \"extractor\", \"rag\", \"reranker\"]\n    pipelines = sorted(pipelines, key=lambda x: dependent.index(x) + 1 if x in dependent else 0)\n\n    # Create pipelines\n    for pipeline in pipelines:\n        if pipeline in self.config:\n            config = self.config[pipeline] if self.config[pipeline] else {}\n\n            # Add application reference, if requested\n            if \"application\" in config:\n                config[\"application\"] = self\n\n            # Custom pipeline parameters\n            if pipeline in [\"extractor\", \"rag\"]:\n                if \"similarity\" not in config:\n                    # Add placeholder, will be set to embeddings index once initialized\n                    config[\"similarity\"] = None\n\n                # Resolve reference pipelines\n                if config.get(\"similarity\") in self.pipelines:\n                    config[\"similarity\"] = self.pipelines[config[\"similarity\"]]\n\n                if config.get(\"path\") in self.pipelines:\n                    config[\"path\"] = self.pipelines[config[\"path\"]]\n\n            elif pipeline == \"similarity\" and \"path\" not in config and \"labels\" in self.pipelines:\n                config[\"model\"] = self.pipelines[\"labels\"]\n\n            elif pipeline == \"reranker\":\n                config[\"embeddings\"] = None\n                config[\"similarity\"] = self.pipelines[\"similarity\"]\n\n            self.pipelines[pipeline] = PipelineFactory.create(config, pipeline)\n</code></pre>"},{"location":"api/methods/#txtai.api.API.delete","title":"<code>delete(ids)</code>","text":"<p>Deletes from an embeddings index. Returns list of ids deleted.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <p>list of ids to delete</p> required <p>Returns:</p> Type Description <p>ids deleted</p> Source code in <code>txtai/api/base.py</code> <pre><code>def delete(self, ids):\n    \"\"\"\n    Deletes from an embeddings index. Returns list of ids deleted.\n\n    Args:\n        ids: list of ids to delete\n\n    Returns:\n        ids deleted\n    \"\"\"\n\n    if self.cluster:\n        return self.cluster.delete(ids)\n\n    return super().delete(ids)\n</code></pre>"},{"location":"api/methods/#txtai.api.API.explain","title":"<code>explain(query, texts=None, limit=10)</code>","text":"<p>Explains the importance of each input token in text for a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>query text</p> required <code>texts</code> <p>optional list of text, otherwise runs search query</p> <code>None</code> <code>limit</code> <p>optional limit if texts is None</p> <code>10</code> <p>Returns:</p> Type Description <p>list of dict per input text where a higher token scores represents higher importance relative to the query</p> Source code in <code>txtai/app/base.py</code> <pre><code>def explain(self, query, texts=None, limit=10):\n    \"\"\"\n    Explains the importance of each input token in text for a query.\n\n    Args:\n        query: query text\n        texts: optional list of text, otherwise runs search query\n        limit: optional limit if texts is None\n\n    Returns:\n        list of dict per input text where a higher token scores represents higher importance relative to the query\n    \"\"\"\n\n    if self.embeddings:\n        with self.lock:\n            return self.embeddings.explain(query, texts, limit)\n\n    return None\n</code></pre>"},{"location":"api/methods/#txtai.api.API.extract","title":"<code>extract(queue, texts=None)</code>","text":"<p>Extracts answers to input questions.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <p>list of {name: value, query: value, question: value, snippet: value}</p> required <code>texts</code> <p>optional list of text</p> <code>None</code> <p>Returns:</p> Type Description <p>list of {name: value, answer: value}</p> Source code in <code>txtai/app/base.py</code> <pre><code>def extract(self, queue, texts=None):\n    \"\"\"\n    Extracts answers to input questions.\n\n    Args:\n        queue: list of {name: value, query: value, question: value, snippet: value}\n        texts: optional list of text\n\n    Returns:\n        list of {name: value, answer: value}\n    \"\"\"\n\n    if self.embeddings and \"extractor\" in self.pipelines:\n        # Get extractor instance\n        extractor = self.pipelines[\"extractor\"]\n\n        # Run extractor and return results as dicts\n        return extractor(queue, texts)\n\n    return None\n</code></pre>"},{"location":"api/methods/#txtai.api.API.index","title":"<code>index()</code>","text":"<p>Builds an embeddings index for previously batched documents.</p> Source code in <code>txtai/api/base.py</code> <pre><code>def index(self):\n    \"\"\"\n    Builds an embeddings index for previously batched documents.\n    \"\"\"\n\n    if self.cluster:\n        self.cluster.index()\n    else:\n        super().index()\n</code></pre>"},{"location":"api/methods/#txtai.api.API.label","title":"<code>label(text, labels)</code>","text":"<p>Applies a zero shot classifier to text using a list of labels. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in labels.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <code>labels</code> <p>list of labels</p> required <p>Returns:</p> Type Description <p>list of {id: value, score: value} per text element</p> Source code in <code>txtai/app/base.py</code> <pre><code>def label(self, text, labels):\n    \"\"\"\n    Applies a zero shot classifier to text using a list of labels. Returns a list of\n    {id: value, score: value} sorted by highest score, where id is the index in labels.\n\n    Args:\n        text: text|list\n        labels: list of labels\n\n    Returns:\n        list of {id: value, score: value} per text element\n    \"\"\"\n\n    if \"labels\" in self.pipelines:\n        # Text is a string\n        if isinstance(text, str):\n            return [{\"id\": uid, \"score\": float(score)} for uid, score in self.pipelines[\"labels\"](text, labels)]\n\n        # Text is a list\n        return [[{\"id\": uid, \"score\": float(score)} for uid, score in result] for result in self.pipelines[\"labels\"](text, labels)]\n\n    return None\n</code></pre>"},{"location":"api/methods/#txtai.api.API.pipeline","title":"<code>pipeline(name, *args, **kwargs)</code>","text":"<p>Generic pipeline execution method.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>pipeline name</p> required <code>args</code> <p>pipeline positional arguments</p> <code>()</code> <code>kwargs</code> <p>pipeline keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <p>pipeline results</p> Source code in <code>txtai/app/base.py</code> <pre><code>def pipeline(self, name, *args, **kwargs):\n    \"\"\"\n    Generic pipeline execution method.\n\n    Args:\n        name: pipeline name\n        args: pipeline positional arguments\n        kwargs: pipeline keyword arguments\n\n    Returns:\n        pipeline results\n    \"\"\"\n\n    # Backwards compatible with previous pipeline function arguments\n    args = args[0] if args and len(args) == 1 and isinstance(args[0], tuple) else args\n\n    if name in self.pipelines:\n        return self.pipelines[name](*args, **kwargs)\n\n    return None\n</code></pre>"},{"location":"api/methods/#txtai.api.API.reindex","title":"<code>reindex(config, function=None)</code>","text":"<p>Recreates this embeddings index using config. This method only works if document content storage is enabled.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>new config</p> required <code>function</code> <p>optional function to prepare content for indexing</p> <code>None</code> Source code in <code>txtai/api/base.py</code> <pre><code>def reindex(self, config, function=None):\n    \"\"\"\n    Recreates this embeddings index using config. This method only works if document content storage is enabled.\n\n    Args:\n        config: new config\n        function: optional function to prepare content for indexing\n    \"\"\"\n\n    if self.cluster:\n        self.cluster.reindex(config, function)\n    else:\n        super().reindex(config, function)\n</code></pre>"},{"location":"api/methods/#txtai.api.API.similarity","title":"<code>similarity(query, texts)</code>","text":"<p>Computes the similarity between query and list of text. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in texts.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>query text</p> required <code>texts</code> <p>list of text</p> required <p>Returns:</p> Type Description <p>list of {id: value, score: value}</p> Source code in <code>txtai/app/base.py</code> <pre><code>def similarity(self, query, texts):\n    \"\"\"\n    Computes the similarity between query and list of text. Returns a list of\n    {id: value, score: value} sorted by highest score, where id is the index\n    in texts.\n\n    Args:\n        query: query text\n        texts: list of text\n\n    Returns:\n        list of {id: value, score: value}\n    \"\"\"\n\n    # Use similarity instance if available otherwise fall back to embeddings model\n    if \"similarity\" in self.pipelines:\n        return [{\"id\": uid, \"score\": float(score)} for uid, score in self.pipelines[\"similarity\"](query, texts)]\n    if self.embeddings:\n        return [{\"id\": uid, \"score\": float(score)} for uid, score in self.embeddings.similarity(query, texts)]\n\n    return None\n</code></pre>"},{"location":"api/methods/#txtai.api.API.transform","title":"<code>transform(text, category=None, index=None)</code>","text":"<p>Transforms text into embeddings arrays.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>input text</p> required <code>category</code> <p>category for instruction-based embeddings</p> <code>None</code> <code>index</code> <p>index name, if applicable</p> <code>None</code> <p>Returns:</p> Type Description <p>embeddings array</p> Source code in <code>txtai/app/base.py</code> <pre><code>def transform(self, text, category=None, index=None):\n    \"\"\"\n    Transforms text into embeddings arrays.\n\n    Args:\n        text: input text\n        category: category for instruction-based embeddings\n        index: index name, if applicable\n\n    Returns:\n        embeddings array\n    \"\"\"\n\n    if self.embeddings:\n        return [float(x) for x in self.embeddings.transform(text, category, index)]\n\n    return None\n</code></pre>"},{"location":"api/methods/#txtai.api.API.upsert","title":"<code>upsert()</code>","text":"<p>Runs an embeddings upsert operation for previously batched documents.</p> Source code in <code>txtai/api/base.py</code> <pre><code>def upsert(self):\n    \"\"\"\n    Runs an embeddings upsert operation for previously batched documents.\n    \"\"\"\n\n    if self.cluster:\n        self.cluster.upsert()\n    else:\n        super().upsert()\n</code></pre>"},{"location":"api/methods/#txtai.api.API.wait","title":"<code>wait()</code>","text":"<p>Closes threadpool and waits for completion.</p> Source code in <code>txtai/app/base.py</code> <pre><code>def wait(self):\n    \"\"\"\n    Closes threadpool and waits for completion.\n    \"\"\"\n\n    if self.pool:\n        self.pool.close()\n        self.pool.join()\n        self.pool = None\n</code></pre>"},{"location":"api/methods/#txtai.api.API.workflow","title":"<code>workflow(name, elements)</code>","text":"<p>Executes a workflow.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>workflow name</p> required <code>elements</code> <p>elements to process</p> required <p>Returns:</p> Type Description <p>processed elements</p> Source code in <code>txtai/app/base.py</code> <pre><code>def workflow(self, name, elements):\n    \"\"\"\n    Executes a workflow.\n\n    Args:\n        name: workflow name\n        elements: elements to process\n\n    Returns:\n        processed elements\n    \"\"\"\n\n    if hasattr(elements, \"__len__\") and hasattr(elements, \"__getitem__\"):\n        # Convert to tuples and return as a list since input is sized\n        elements = [tuple(element) if isinstance(element, list) else element for element in elements]\n    else:\n        # Convert to tuples and return as a generator since input is not sized\n        elements = (tuple(element) if isinstance(element, list) else element for element in elements)\n\n    # Execute workflow\n    return self.workflows[name](elements)\n</code></pre>"},{"location":"api/openai/","title":"OpenAI-compatible API","text":"<p>The API can be configured to serve an OpenAI-compatible API as shown below.</p> <pre><code>openai: True\n</code></pre> <p>See the link below for a detailed example.</p> Notebook Description OpenAI Compatible API Connect to txtai with a standard OpenAI client library"},{"location":"api/security/","title":"Security","text":"<p>The default implementation of an API service runs via HTTP and is fully open. If the service is being run as a prototype on an internal network, that may be fine. In most scenarios, the connection should at least be encrypted. Authorization is another built-in feature that requires a valid API token with each request. See below for more.</p>"},{"location":"api/security/#https","title":"HTTPS","text":"<p>The default API service command starts a Uvicorn server as a HTTP service on port 8000. To run a HTTPS service, consider the following options.</p> <ul> <li> <p>TLS Proxy Server. Recommended choice. With this configuration, the txtai API service runs as a HTTP service only accessible on the localhost/local network. The proxy server handles all encryption and redirects requests to local services. See this example configuration for more.</p> </li> <li> <p>Uvicorn SSL Certificate. Another option is setting the SSL certificate on the Uvicorn service. This works in simple situations but gets complex when hosting multiple txtai or other related services.</p> </li> </ul>"},{"location":"api/security/#authorization","title":"Authorization","text":"<p>Authorization requires a valid API token with each API request. This token is sent as a HTTP <code>Authorization</code> header. </p> <p>Server <pre><code>CONFIG=config.yml TOKEN=&lt;sha256 encoded token&gt; uvicorn \"txtai.api:app\"\n</code></pre></p> <p>Client <pre><code>curl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer &lt;token&gt;\" \\ \n  -d '{\"name\":\"sumfrench\", \"elements\": [\"https://github.com/neuml/txtai\"]}'\n</code></pre></p> <p>It's important to note that HTTPS must be enabled using one of the methods mentioned above. Otherwise, tokens will be exchanged as clear text. </p> <p>Authentication and Authorization can be fully customized. See the dependencies section for more.</p>"},{"location":"embeddings/","title":"Embeddings","text":"<p>Embeddings databases are the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords.</p> <p>The following code snippet shows how to build and search an embeddings index.</p> <pre><code>from txtai import Embeddings\n\n# Create embeddings model, backed by sentence-transformers &amp; transformers\nembeddings = Embeddings(path=\"sentence-transformers/nli-mpnet-base-v2\")\n\ndata = [\n  \"US tops 5 million confirmed virus cases\",\n  \"Canada's last fully intact ice shelf has suddenly collapsed, \" +\n  \"forming a Manhattan-sized iceberg\",\n  \"Beijing mobilises invasion craft along coast as Taiwan tensions escalate\",\n  \"The National Park Service warns against sacrificing slower friends \" +\n  \"in a bear attack\",\n  \"Maine man wins $1M from $25 lottery ticket\",\n  \"Make huge profits without work, earn up to $100,000 a day\"\n]\n\n# Index the list of text\nembeddings.index(data)\n\nprint(f\"{'Query':20} Best Match\")\nprint(\"-\" * 50)\n\n# Run an embeddings search for each query\nfor query in (\"feel good story\", \"climate change\", \"public health story\", \"war\",\n              \"wildlife\", \"asia\", \"lucky\", \"dishonest junk\"):\n    # Extract uid of first result\n    # search result format: (uid, score)\n    uid = embeddings.search(query, 1)[0][0]\n\n    # Print text\n    print(f\"{query:20} {data[uid]}\")\n</code></pre>"},{"location":"embeddings/#build","title":"Build","text":"<p>An embeddings instance is configuration-driven based on what is passed in the constructor. Vectors are stored with the option to also store content. Content storage enables additional filtering and data retrieval options.</p> <p>The example above sets a specific embeddings vector model via the path parameter. An embeddings instance with no configuration can also be created.</p> <pre><code>embeddings = Embeddings()\n</code></pre> <p>In this case, when loading and searching for data, the default transformers vector model is used to vectorize data. See the model guide for current model recommentations.</p>"},{"location":"embeddings/#index","title":"Index","text":"<p>After creating a new embeddings instance, the next step is adding data to it.</p> <pre><code>embeddings.index(rows)\n</code></pre> <p>The index method takes an iterable and supports the following formats for each element.</p> <ul> <li><code>(id, data, tags)</code> - default processing format</li> </ul> Element Description id unique record id data input data to index, can be text, a dictionary or object tags optional tags string, used to mark/label data as it's indexed <ul> <li><code>(id, data)</code></li> </ul> <p>Same as above but without tags.</p> <ul> <li><code>data</code></li> </ul> <p>Single element to index. In this case, unique id's will automatically be generated. Note that for generated id's, upsert and delete calls require a separate search to get the target ids.</p> <p>When the data field is a dictionary, text is passed via the <code>text</code> key, binary objects via the <code>object</code> key. Note that content must be enabled to store metadata and objects to store binary object data. The <code>id</code> and <code>tags</code> keys will be extracted, if provided.</p> <p>The input iterable can be a list or generator. Generators help with indexing very large datasets as only portions of the data is in memory at any given time.</p> <p>More information on indexing can be found in the index guide.</p>"},{"location":"embeddings/#search","title":"Search","text":"<p>Once data is indexed, it is ready for search.</p> <pre><code>embeddings.search(query, limit)\n</code></pre> <p>The search method takes two parameters, the query and query limit. The results format is different based on whether content is stored or not.</p> <ul> <li>List of <code>(id, score)</code> when content is not stored</li> <li>List of <code>{**query columns}</code> when content is stored</li> </ul> <p>Both natural language and SQL queries are supported. More information can be found in the query guide.</p>"},{"location":"embeddings/#resource-management","title":"Resource management","text":"<p>Embeddings databases are context managers. The following blocks automatically close and free resources upon completion.</p> <pre><code># Create a new Embeddings database, index data and save\nwith Embeddings() as embeddings:\n  embeddings.index(rows)\n  embeddings.save(path)\n\n# Search a saved Embeddings database\nwith Embeddings().load(path) as embeddings:\n  embeddings.search(query)\n</code></pre> <p>While calling <code>close</code> isn't always necessary (resources will be garbage collected), it's best to free shared resources like database connections as soon as they aren't needed.</p>"},{"location":"embeddings/#more-examples","title":"More examples","text":"<p>See this link for a full list of embeddings examples.</p>"},{"location":"embeddings/format/","title":"Index format","text":"<p>This section documents the txtai index format. Each component is designed to ensure open access to the underlying data in a programmatic and platform independent way</p> <p>If an underlying library has an index format, that is used. Otherwise, txtai persists content with MessagePack serialization.</p> <p>To learn more about how these components work together, read the Index Guide and Query Guide.</p>"},{"location":"embeddings/format/#ann","title":"ANN","text":"<p>Approximate Nearest Neighbor (ANN) index configuration for storing vector embeddings.</p> Component Storage Format Faiss Local file format provided by library Hnswlib Local file format provided by library Annoy Local file format provided by library NumPy Local NumPy array files via np.save / np.load Postgres via pgvector Vector tables in a Postgres database"},{"location":"embeddings/format/#core","title":"Core","text":"<p>Core embeddings index files.</p> Component Storage Format Configuration Embeddings index configuration stored as JSON Index Ids Embeddings index ids serialized with MessagePack. Only enabled when when content storage (database) is disabled."},{"location":"embeddings/format/#database","title":"Database","text":"<p>Databases store metadata, text and binary content.</p> Component Storage Format SQLite Local database files with SQLite DuckDB Local database files with DuckDB Postgres Postgres relational database via SQLAlchemy. Supports additional databases via this library."},{"location":"embeddings/format/#graph","title":"Graph","text":"<p>Graph nodes and edges for an embeddings index</p> Component Storage Format NetworkX Nodes and edges exported to local file serialized with MessagePack Postgres Nodes and edges stored in a Postgres database. Supports additional databases."},{"location":"embeddings/format/#scoring","title":"Scoring","text":"<p>Sparse/keyword indexing</p> Component Storage Format Local index Metadata serialized with MessagePack. Terms stored in SQLite. Postgres Text indexed with Postgres Full Text Search (FTS)"},{"location":"embeddings/indexing/","title":"Index guide","text":"<p>This section gives an in-depth overview on how to index data with txtai. We'll cover vectorization, indexing/updating/deleting data and the various components of an embeddings database.</p>"},{"location":"embeddings/indexing/#vectorization","title":"Vectorization","text":"<p>The most compute intensive step in building an index is vectorization. The path parameter sets the path to the vector model. There is logic to automatically detect the vector model method but it can also be set directly.</p> <p>The batch and encodebatch parameters control the vectorization process. Larger values for <code>batch</code> will pass larger batches to the vectorization method. Larger values for <code>encodebatch</code> will pass larger batches for each vector encode call. In the case of GPU vector models, larger values will consume more GPU memory.</p> <p>Data is buffered to temporary storage during indexing as embeddings vectors can be quite large (for example 768 dimensions of float32 is 768 * 4 = 3072 bytes per vector). Once vectorization is complete, a mmapped array is created with all vectors for Approximate Nearest Neighbor (ANN) indexing.</p> <p>The terms <code>ANN</code> and <code>dense vector index</code> are used interchangeably throughout txtai's documentation.</p>"},{"location":"embeddings/indexing/#setting-a-backend","title":"Setting a backend","text":"<p>As mentioned above, computed vectors are stored in an ANN. There are various index backends that can be configured. Faiss is the default backend.</p>"},{"location":"embeddings/indexing/#content-storage","title":"Content storage","text":"<p>Embeddings indexes can optionally store content. When this is enabled, the input content is saved in a database alongside the computed vectors. This enables filtering on additional fields and content retrieval.</p> <p>The columns used for text, object and JSON data storage are set via column configuration.</p>"},{"location":"embeddings/indexing/#index-vs-upsert","title":"Index vs Upsert","text":"<p>Data is loaded into an index with either an index or upsert call.</p> <pre><code>embeddings.index([(uid, text, None) for uid, text in enumerate(data)])\nembeddings.upsert([(uid, text, None) for uid, text in enumerate(data)])\n</code></pre> <p>The <code>index</code> call will build a brand new index replacing an existing one. <code>upsert</code> will insert or update records. <code>upsert</code> ops do not require a full index rebuild.</p>"},{"location":"embeddings/indexing/#save","title":"Save","text":"<p>Indexes can be stored in a directory using the save method.</p> <pre><code>embeddings.save(\"/path/to/save\")\n</code></pre> <p>Compressed indexes are also supported.</p> <pre><code>embeddings.save(\"/path/to/save/index.tar.gz\")\n</code></pre> <p>In addition to saving indexes locally, they can also be persisted to cloud storage.</p> <pre><code>embeddings.save(\"/path/to/save/index.tar.gz\", cloud={...})\n</code></pre> <p>This is especially useful when running in a serverless context or otherwise running on temporary compute. Cloud storage is only supported with compressed indexes.</p> <p>Embeddings indexes can be restored using the load method.</p> <pre><code>embeddings.load(\"/path/to/load\")\n</code></pre>"},{"location":"embeddings/indexing/#delete","title":"Delete","text":"<p>Content can be removed from the index with the delete method. This method takes a list of ids to delete.</p> <pre><code>embeddings.delete(ids)\n</code></pre>"},{"location":"embeddings/indexing/#reindex","title":"Reindex","text":"<p>When content storage is enabled, reindex can be called to rebuild the index with new settings. For example, the backend can be switched from faiss to hnsw or the vector model can be updated. This prevents having to go back to the original raw data. </p> <pre><code>embeddings.reindex(path=\"sentence-transformers/all-MiniLM-L6-v2\", backend=\"hnsw\")\n</code></pre>"},{"location":"embeddings/indexing/#graph","title":"Graph","text":"<p>Enabling a graph network adds a semantic graph at index time as data is being vectorized. Vector embeddings are used to automatically create relationships in the graph. Relationships can also be manually specified at index time.</p> <pre><code># Manual relationships by id\nembeddings.index([{\"id\": \"0\", \"text\": \"...\", \"relationships\": [\"2\"]}])\n\n# Manual relationships with additional edge attributes\nembeddings.index([\"id\": \"0\", \"text\": \"...\", \"relationships\": [\n    {\"id\": \"2\", \"type\": \"MEMBER_OF\"}\n]])\n</code></pre> <p>Additionally, graphs can be used for topic modeling. Dimensionality reduction with UMAP combined with HDBSCAN is a popular topic modeling method found in a number of libraries. txtai takes a different approach using community detection algorithms to build topic clusters.</p> <p>This approach has the advantage of only having to vectorize data once. It also has the advantage of better topic precision given there isn't a dimensionality reduction operation (UMAP). Semantic graph examples are shown below.</p> <p>Get a mapping of discovered topics to associated ids.</p> <pre><code>embeddings.graph.topics\n</code></pre> <p>Show the most central nodes in the index.</p> <pre><code>embeddings.graph.centrality()\n</code></pre> <p>Graphs are persisted alongside an embeddings index. Each save and load will also save and load the graph.</p>"},{"location":"embeddings/indexing/#sparse-vectors","title":"Sparse vectors","text":"<p>Scoring instances can create a standalone sparse keyword indexes (BM25, TF-IDF) and sparse vector indexes (SPLADE). This enables hybrid search when there is an available dense vector index.</p> <p>The terms <code>sparse vector index</code>, <code>keyword index</code>, <code>terms index</code> and <code>scoring index</code> are used interchangeably throughout txtai's documentation.</p> <p>See this link to learn more.</p>"},{"location":"embeddings/indexing/#subindexes","title":"Subindexes","text":"<p>An embeddings instance can optionally have associated subindexes, which are also embeddings databases. This enables indexing additional fields, vector models and much more.</p>"},{"location":"embeddings/indexing/#word-vectors","title":"Word vectors","text":"<p>When using word vector backed models with scoring set, a separate call is required before calling <code>index</code> as follows:</p> <pre><code>embeddings.score(rows)\nembeddings.index(rows)\n</code></pre> <p>Both calls are required to support generator-backed iteration of data with word vectors models.</p>"},{"location":"embeddings/methods/","title":"Methods","text":""},{"location":"embeddings/methods/#txtai.embeddings.Embeddings","title":"<code>Embeddings</code>","text":"<p>Embeddings databases are the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords.</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>class Embeddings:\n    \"\"\"\n    Embeddings databases are the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts\n    will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results\n    that have the same meaning, not necessarily the same keywords.\n    \"\"\"\n\n    # pylint: disable=W0231\n    def __init__(self, config=None, models=None, **kwargs):\n        \"\"\"\n        Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized.\n\n        Args:\n            config: embeddings configuration\n            models: models cache, used for model sharing between embeddings\n            kwargs: additional configuration as keyword args\n        \"\"\"\n\n        # Index configuration\n        self.config = None\n\n        # Dimensionality reduction - word vectors only\n        self.reducer = None\n\n        # Dense vector model - transforms data into similarity vectors\n        self.model = None\n\n        # Approximate nearest neighbor index\n        self.ann = None\n\n        # Index ids when content is disabled\n        self.ids = None\n\n        # Document database\n        self.database = None\n\n        # Resolvable functions\n        self.functions = None\n\n        # Graph network\n        self.graph = None\n\n        # Sparse vectors\n        self.scoring = None\n\n        # Query model\n        self.query = None\n\n        # Index archive\n        self.archive = None\n\n        # Subindexes for this embeddings instance\n        self.indexes = None\n\n        # Models cache\n        self.models = models\n\n        # Merge configuration into single dictionary\n        config = {**config, **kwargs} if config and kwargs else kwargs if kwargs else config\n\n        # Set initial configuration\n        self.configure(config)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n    def score(self, documents):\n        \"\"\"\n        Builds a term weighting scoring index. Only used by word vectors models.\n\n        Args:\n            documents: iterable of (id, data, tags), (id, data) or data\n        \"\"\"\n\n        # Build scoring index for word vectors term weighting\n        if self.isweighted():\n            self.scoring.index(Stream(self)(documents))\n\n    def index(self, documents, reindex=False, checkpoint=None):\n        \"\"\"\n        Builds an embeddings index. This method overwrites an existing index.\n\n        Args:\n            documents: iterable of (id, data, tags), (id, data) or data\n            reindex: if this is a reindex operation in which case database creation is skipped, defaults to False\n            checkpoint: optional checkpoint directory, enables indexing restart\n        \"\"\"\n\n        # Initialize index\n        self.initindex(reindex)\n\n        # Create transform and stream\n        transform = Transform(self, Action.REINDEX if reindex else Action.INDEX, checkpoint)\n        stream = Stream(self, Action.REINDEX if reindex else Action.INDEX)\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".npy\") as buffer:\n            # Load documents into database and transform to vectors\n            ids, dimensions, embeddings = transform(stream(documents), buffer)\n            if embeddings is not None:\n                # Build LSA model (if enabled). Remove principal components from embeddings.\n                if self.config.get(\"pca\"):\n                    self.reducer = Reducer(embeddings, self.config[\"pca\"])\n                    self.reducer(embeddings)\n\n                # Save index dimensions\n                self.config[\"dimensions\"] = dimensions\n\n                # Create approximate nearest neighbor index\n                self.ann = self.createann()\n\n                # Add embeddings to the index\n                self.ann.index(embeddings)\n\n            # Save indexids-ids mapping for indexes with no database, except when this is a reindex\n            if ids and not reindex and not self.database:\n                self.ids = self.createids(ids)\n\n        # Index scoring, if necessary\n        # This must occur before graph index in order to be available to the graph\n        if self.issparse():\n            self.scoring.index()\n\n        # Index subindexes, if necessary\n        if self.indexes:\n            self.indexes.index()\n\n        # Index graph, if necessary\n        if self.graph:\n            self.graph.index(Search(self, indexonly=True), Ids(self), self.batchsimilarity)\n\n    def upsert(self, documents, checkpoint=None):\n        \"\"\"\n        Runs an embeddings upsert operation. If the index exists, new data is\n        appended to the index, existing data is updated. If the index doesn't exist,\n        this method runs a standard index operation.\n\n        Args:\n            documents: iterable of (id, data, tags), (id, data) or data\n            checkpoint: optional checkpoint directory, enables indexing restart\n        \"\"\"\n\n        # Run standard insert if index doesn't exist or it has no records\n        if not self.count():\n            self.index(documents, checkpoint=checkpoint)\n            return\n\n        # Create transform and stream\n        transform = Transform(self, Action.UPSERT, checkpoint=checkpoint)\n        stream = Stream(self, Action.UPSERT)\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".npy\") as buffer:\n            # Load documents into database and transform to vectors\n            ids, _, embeddings = transform(stream(documents), buffer)\n            if embeddings is not None:\n                # Remove principal components from embeddings, if necessary\n                if self.reducer:\n                    self.reducer(embeddings)\n\n                # Append embeddings to the index\n                self.ann.append(embeddings)\n\n            # Save indexids-ids mapping for indexes with no database\n            if ids and not self.database:\n                self.ids = self.createids(self.ids + ids)\n\n        # Scoring upsert, if necessary\n        # This must occur before graph upsert in order to be available to the graph\n        if self.issparse():\n            self.scoring.upsert()\n\n        # Subindexes upsert, if necessary\n        if self.indexes:\n            self.indexes.upsert()\n\n        # Graph upsert, if necessary\n        if self.graph:\n            self.graph.upsert(Search(self, indexonly=True), Ids(self), self.batchsimilarity)\n\n    def delete(self, ids):\n        \"\"\"\n        Deletes from an embeddings index. Returns list of ids deleted.\n\n        Args:\n            ids: list of ids to delete\n\n        Returns:\n            list of ids deleted\n        \"\"\"\n\n        # List of internal indices for each candidate id to delete\n        indices = []\n\n        # List of deleted ids\n        deletes = []\n\n        if self.database:\n            # Retrieve indexid-id mappings from database\n            ids = self.database.ids(ids)\n\n            # Parse out indices and ids to delete\n            indices = [i for i, _ in ids]\n            deletes = sorted(set(uid for _, uid in ids))\n\n            # Delete ids from database\n            self.database.delete(deletes)\n        elif self.ann or self.scoring:\n            # Find existing ids\n            for uid in ids:\n                indices.extend([index for index, value in enumerate(self.ids) if uid == value])\n\n            # Clear embeddings ids\n            for index in indices:\n                deletes.append(self.ids[index])\n                self.ids[index] = None\n\n        # Delete indices for all indexes and data stores\n        if indices:\n            # Delete ids from ann\n            if self.isdense():\n                self.ann.delete(indices)\n\n            # Delete ids from scoring\n            if self.issparse():\n                self.scoring.delete(indices)\n\n            # Delete ids from subindexes\n            if self.indexes:\n                self.indexes.delete(indices)\n\n            # Delete ids from graph\n            if self.graph:\n                self.graph.delete(indices)\n\n        return deletes\n\n    def reindex(self, config=None, function=None, **kwargs):\n        \"\"\"\n        Recreates embeddings index using config. This method only works if document content storage is enabled.\n\n        Args:\n            config: new config\n            function: optional function to prepare content for indexing\n            kwargs: additional configuration as keyword args\n        \"\"\"\n\n        if self.database:\n            # Merge configuration into single dictionary\n            config = {**config, **kwargs} if config and kwargs else config if config else kwargs\n\n            # Keep content and objects parameters to ensure database is preserved\n            config[\"content\"] = self.config[\"content\"]\n            if \"objects\" in self.config:\n                config[\"objects\"] = self.config[\"objects\"]\n\n            # Reset configuration\n            self.configure(config)\n\n            # Reset function references\n            if self.functions:\n                self.functions.reset()\n\n            # Reindex\n            if function:\n                self.index(function(self.database.reindex(self.config)), True)\n            else:\n                self.index(self.database.reindex(self.config), True)\n\n    def transform(self, document, category=None, index=None):\n        \"\"\"\n        Transforms document into an embeddings vector.\n\n        Args:\n            documents: iterable of (id, data, tags), (id, data) or data\n            category: category for instruction-based embeddings\n            index: index name, if applicable\n\n        Returns:\n            embeddings vector\n        \"\"\"\n\n        return self.batchtransform([document], category, index)[0]\n\n    def batchtransform(self, documents, category=None, index=None):\n        \"\"\"\n        Transforms documents into embeddings vectors.\n\n        Args:\n            documents: iterable of (id, data, tags), (id, data) or data\n            category: category for instruction-based embeddings\n            index: index name, if applicable\n\n        Returns:\n            embeddings vectors\n        \"\"\"\n\n        # Initialize default parameters, if necessary\n        self.defaults()\n\n        # Get vector model\n        model = self.findmodel(index)\n\n        # Convert documents into embeddings\n        embeddings = model.batchtransform(Stream(self)(documents), category)\n\n        # Reduce the dimensionality of the embeddings. Scale the embeddings using this\n        # model to reduce the noise of common but less relevant terms.\n        if self.reducer:\n            self.reducer(embeddings)\n\n        return embeddings\n\n    def count(self):\n        \"\"\"\n        Total number of elements in this embeddings index.\n\n        Returns:\n            number of elements in this embeddings index\n        \"\"\"\n\n        if self.ann:\n            return self.ann.count()\n        if self.scoring:\n            return self.scoring.count()\n        if self.database:\n            return self.database.count()\n        if self.ids:\n            return len([uid for uid in self.ids if uid is not None])\n\n        # Default to 0 when no suitable method found\n        return 0\n\n    def search(self, query, limit=None, weights=None, index=None, parameters=None, graph=False):\n        \"\"\"\n        Finds documents most similar to the input query. This method runs an index search, index + database search\n        or a graph search, depending on the embeddings configuration and query.\n\n        Args:\n            query: input query\n            limit: maximum results\n            weights: hybrid score weights, if applicable\n            index: index name, if applicable\n            parameters: dict of named parameters to bind to placeholders\n            graph: return graph results if True\n\n        Returns:\n            list of (id, score) for index search\n            list of dict for an index + database search\n            graph when graph is set to True\n        \"\"\"\n\n        results = self.batchsearch([query], limit, weights, index, [parameters], graph)\n        return results[0] if results else results\n\n    def batchsearch(self, queries, limit=None, weights=None, index=None, parameters=None, graph=False):\n        \"\"\"\n        Finds documents most similar to the input query. This method runs an index search, index + database search\n        or a graph search, depending on the embeddings configuration and query.\n\n        Args:\n            queries: input queries\n            limit: maximum results\n            weights: hybrid score weights, if applicable\n            index: index name, if applicable\n            parameters: list of dicts of named parameters to bind to placeholders\n            graph: return graph results if True\n\n        Returns:\n            list of (id, score) per query for index search\n            list of dict per query for an index + database search\n            list of graph per query when graph is set to True\n        \"\"\"\n\n        # Determine if graphs should be returned\n        graph = graph if self.graph else False\n\n        # Execute search\n        results = Search(self, indexids=graph)(queries, limit, weights, index, parameters)\n\n        # Create subgraphs using results, if necessary\n        return [self.graph.filter(x) if isinstance(x, list) else x for x in results] if graph else results\n\n    def similarity(self, query, data):\n        \"\"\"\n        Computes the similarity between query and list of data. Returns a list of\n        (id, score) sorted by highest score, where id is the index in data.\n\n        Args:\n            query: input query\n            data: list of data\n\n        Returns:\n            list of (id, score)\n        \"\"\"\n\n        return self.batchsimilarity([query], data)[0]\n\n    def batchsimilarity(self, queries, data):\n        \"\"\"\n        Computes the similarity between list of queries and list of data. Returns a list\n        of (id, score) sorted by highest score per query, where id is the index in data.\n\n        Args:\n            queries: input queries\n            data: list of data\n\n        Returns:\n            list of (id, score) per query\n        \"\"\"\n\n        # Convert queries to embedding vectors\n        queries = self.batchtransform(((None, query, None) for query in queries), \"query\")\n        data = self.batchtransform(((None, row, None) for row in data), \"data\")\n\n        # Get vector model\n        model = self.findmodel()\n\n        # Dot product on normalized vectors is equal to cosine similarity\n        scores = model.dot(queries, data)\n\n        # Add index and sort desc based on score\n        return [sorted(enumerate(score), key=lambda x: x[1], reverse=True) for score in scores]\n\n    def explain(self, query, texts=None, limit=None):\n        \"\"\"\n        Explains the importance of each input token in text for a query. This method requires either content to be enabled\n        or texts to be provided.\n\n        Args:\n            query: input query\n            texts: optional list of (text|list of tokens), otherwise runs search query\n            limit: optional limit if texts is None\n\n        Returns:\n            list of dict per input text where a higher token scores represents higher importance relative to the query\n        \"\"\"\n\n        results = self.batchexplain([query], texts, limit)\n        return results[0] if results else results\n\n    def batchexplain(self, queries, texts=None, limit=None):\n        \"\"\"\n        Explains the importance of each input token in text for a list of queries. This method requires either content to be enabled\n        or texts to be provided.\n\n        Args:\n            queries: input queries\n            texts: optional list of (text|list of tokens), otherwise runs search queries\n            limit: optional limit if texts is None\n\n        Returns:\n            list of dict per input text per query where a higher token scores represents higher importance relative to the query\n        \"\"\"\n\n        return Explain(self)(queries, texts, limit)\n\n    def terms(self, query):\n        \"\"\"\n        Extracts keyword terms from a query.\n\n        Args:\n            query: input query\n\n        Returns:\n            query reduced down to keyword terms\n        \"\"\"\n\n        return self.batchterms([query])[0]\n\n    def batchterms(self, queries):\n        \"\"\"\n        Extracts keyword terms from a list of queries.\n\n        Args:\n            queries: list of queries\n\n        Returns:\n            list of queries reduced down to keyword term strings\n        \"\"\"\n\n        return Terms(self)(queries)\n\n    def exists(self, path=None, cloud=None, **kwargs):\n        \"\"\"\n        Checks if an index exists at path.\n\n        Args:\n            path: input path\n            cloud: cloud storage configuration\n            kwargs: additional configuration as keyword args\n\n        Returns:\n            True if index exists, False otherwise\n        \"\"\"\n\n        # Check if this exists in a cloud instance\n        cloud = self.createcloud(cloud=cloud, **kwargs)\n        if cloud:\n            return cloud.exists(path)\n\n        # Check if this is an archive file and exists\n        path, apath = self.checkarchive(path)\n        if apath:\n            return os.path.exists(apath)\n\n        # Return true if path has a config.json or config file with an offset set\n        return path and (os.path.exists(f\"{path}/config.json\") or os.path.exists(f\"{path}/config\")) and \"offset\" in Configuration().load(path)\n\n    def load(self, path=None, cloud=None, config=None, **kwargs):\n        \"\"\"\n        Loads an existing index from path.\n\n        Args:\n            path: input path\n            cloud: cloud storage configuration\n            config: configuration overrides\n            kwargs: additional configuration as keyword args\n\n        Returns:\n            Embeddings\n        \"\"\"\n\n        # Load from cloud, if configured\n        cloud = self.createcloud(cloud=cloud, **kwargs)\n        if cloud:\n            path = cloud.load(path)\n\n        # Check if this is an archive file and extract\n        path, apath = self.checkarchive(path)\n        if apath:\n            self.archive.load(apath)\n\n        # Load index configuration\n        self.config = Configuration().load(path)\n\n        # Apply config overrides\n        self.config = {**self.config, **config} if config else self.config\n\n        # Approximate nearest neighbor index - stores dense vectors\n        self.ann = self.createann()\n        if self.ann:\n            self.ann.load(f\"{path}/embeddings\")\n\n        # Dimensionality reduction model - word vectors only\n        if self.config.get(\"pca\"):\n            self.reducer = Reducer()\n            self.reducer.load(f\"{path}/lsa\")\n\n        # Index ids when content is disabled\n        self.ids = self.createids()\n        if self.ids:\n            self.ids.load(f\"{path}/ids\")\n\n        # Document database - stores document content\n        self.database = self.createdatabase()\n        if self.database:\n            self.database.load(f\"{path}/documents\")\n\n        # Sparse vectors - stores term sparse arrays\n        self.scoring = self.createscoring()\n        if self.scoring:\n            self.scoring.load(f\"{path}/scoring\")\n\n        # Subindexes\n        self.indexes = self.createindexes()\n        if self.indexes:\n            self.indexes.load(f\"{path}/indexes\")\n\n        # Graph network - stores relationships\n        self.graph = self.creategraph()\n        if self.graph:\n            self.graph.load(f\"{path}/graph\")\n\n        # Dense vectors - transforms data to embeddings vectors\n        self.model = self.loadvectors()\n\n        # Query model\n        self.query = self.loadquery()\n\n        return self\n\n    def save(self, path, cloud=None, **kwargs):\n        \"\"\"\n        Saves an index in a directory at path unless path ends with tar.gz, tar.bz2, tar.xz or zip.\n        In those cases, the index is stored as a compressed file.\n\n        Args:\n            path: output path\n            cloud: cloud storage configuration\n            kwargs: additional configuration as keyword args\n        \"\"\"\n\n        if self.config:\n            # Check if this is an archive file\n            path, apath = self.checkarchive(path)\n\n            # Create output directory, if necessary\n            os.makedirs(path, exist_ok=True)\n\n            # Save index configuration\n            Configuration().save(self.config, path)\n\n            # Save approximate nearest neighbor index\n            if self.ann:\n                self.ann.save(f\"{path}/embeddings\")\n\n            # Save dimensionality reduction model (word vectors only)\n            if self.reducer:\n                self.reducer.save(f\"{path}/lsa\")\n\n            # Save index ids\n            if self.ids:\n                self.ids.save(f\"{path}/ids\")\n\n            # Save document database\n            if self.database:\n                self.database.save(f\"{path}/documents\")\n\n            # Save scoring index\n            if self.scoring:\n                self.scoring.save(f\"{path}/scoring\")\n\n            # Save subindexes\n            if self.indexes:\n                self.indexes.save(f\"{path}/indexes\")\n\n            # Save graph\n            if self.graph:\n                self.graph.save(f\"{path}/graph\")\n\n            # If this is an archive, save it\n            if apath:\n                self.archive.save(apath)\n\n            # Save to cloud, if configured\n            cloud = self.createcloud(cloud=cloud, **kwargs)\n            if cloud:\n                cloud.save(apath if apath else path)\n\n    def close(self):\n        \"\"\"\n        Closes this embeddings index and frees all resources.\n        \"\"\"\n\n        self.config, self.archive = None, None\n        self.reducer, self.query = None, None\n        self.ids = None\n\n        # Close ANN\n        if self.ann:\n            self.ann.close()\n            self.ann = None\n\n        # Close database\n        if self.database:\n            self.database.close()\n            self.database, self.functions = None, None\n\n        # Close scoring\n        if self.scoring:\n            self.scoring.close()\n            self.scoring = None\n\n        # Close graph\n        if self.graph:\n            self.graph.close()\n            self.graph = None\n\n        # Close indexes\n        if self.indexes:\n            self.indexes.close()\n            self.indexes = None\n\n        # Close vectors model\n        if self.model:\n            self.model.close()\n            self.model = None\n\n        self.models = None\n\n    def info(self):\n        \"\"\"\n        Prints the current embeddings index configuration.\n        \"\"\"\n\n        if self.config:\n            # Print configuration\n            print(json.dumps(self.config, sort_keys=True, default=str, indent=2))\n\n    def issparse(self):\n        \"\"\"\n        Checks if this instance has an associated sparse keyword or sparse vectors scoring index.\n\n        Returns:\n            True if scoring has an associated sparse keyword/vector index, False otherwise\n        \"\"\"\n\n        return self.scoring and self.scoring.issparse()\n\n    def isdense(self):\n        \"\"\"\n        Checks if this instance has an associated ANN instance.\n\n        Returns:\n            True if this instance has an associated ANN, False otherwise\n        \"\"\"\n\n        return self.ann is not None\n\n    def isweighted(self):\n        \"\"\"\n        Checks if this instance has an associated scoring instance with term weighting enabled.\n\n        Returns:\n            True if term weighting is enabled, False otherwise\n        \"\"\"\n\n        return self.scoring and self.scoring.isweighted()\n\n    def findmodel(self, index=None):\n        \"\"\"\n        Finds the primary vector model used by this instance.\n\n        Returns:\n            Vectors\n        \"\"\"\n\n        return (\n            self.indexes.findmodel(index)\n            if index and self.indexes\n            else (\n                self.model\n                if self.model\n                else self.scoring.findmodel() if self.scoring and self.scoring.findmodel() else self.indexes.findmodel() if self.indexes else None\n            )\n        )\n\n    def configure(self, config):\n        \"\"\"\n        Sets the configuration for this embeddings index and loads config-driven models.\n\n        Args:\n            config: embeddings configuration\n        \"\"\"\n\n        # Configuration\n        self.config = config\n\n        # Dimensionality reduction model\n        self.reducer = None\n\n        # Create scoring instance for word vectors term weighting\n        scoring = self.config.get(\"scoring\") if self.config else None\n        self.scoring = self.createscoring() if scoring and not self.hassparse() else None\n\n        # Dense vectors - transforms data to embeddings vectors\n        self.model = self.loadvectors() if self.config else None\n\n        # Query model\n        self.query = self.loadquery() if self.config else None\n\n    def initindex(self, reindex):\n        \"\"\"\n        Initialize new index.\n\n        Args:\n            reindex: if this is a reindex operation in which case database creation is skipped, defaults to False\n        \"\"\"\n\n        # Initialize default parameters, if necessary\n        self.defaults()\n\n        # Initialize index ids, only created when content is disabled\n        self.ids = None\n\n        # Create document database, if necessary\n        if not reindex:\n            self.database = self.createdatabase()\n\n            # Reset archive since this is a new index\n            self.archive = None\n\n        # Close existing ANN, if necessary\n        if self.ann:\n            self.ann.close()\n\n        # Initialize ANN, will be created after index transformations complete\n        self.ann = None\n\n        # Create scoring only if the scoring config is for a sparse index\n        if self.hassparse():\n            self.scoring = self.createscoring()\n\n        # Create subindexes, if necessary\n        self.indexes = self.createindexes()\n\n        # Create graph, if necessary\n        self.graph = self.creategraph()\n\n    def defaults(self):\n        \"\"\"\n        Apply default parameters to current configuration.\n\n        Returns:\n            configuration with default parameters set\n        \"\"\"\n\n        self.config = self.config if self.config else {}\n\n        # Expand sparse index shortcuts\n        if not self.config.get(\"scoring\") and any(self.config.get(key) for key in [\"keyword\", \"sparse\", \"hybrid\"]):\n            self.defaultsparse()\n\n        # Expand graph shortcuts\n        if self.config.get(\"graph\") is True:\n            self.config[\"graph\"] = {}\n\n        # Check if default model should be loaded\n        if not self.model and (self.defaultallowed() or self.config.get(\"dense\")):\n            self.config[\"path\"] = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n            # Load dense vectors model\n            self.model = self.loadvectors()\n\n    def defaultsparse(self):\n        \"\"\"\n        Logic to derive default sparse index configuration.\n        \"\"\"\n\n        # Check for keyword and hybrid parameters\n        method = None\n        for x in [\"keyword\", \"hybrid\"]:\n            value = self.config.get(x)\n            if value:\n                method = value if isinstance(value, str) else \"bm25\"\n\n                # Enable dense index when hybrid enabled\n                if x == \"hybrid\":\n                    self.config[\"dense\"] = True\n\n        sparse = self.config.get(\"sparse\", {})\n        if sparse or method == \"sparse\":\n            # Sparse vector configuration\n            sparse = {\"path\": self.config.get(\"sparse\")} if isinstance(sparse, str) else {} if isinstance(sparse, bool) else sparse\n            sparse[\"path\"] = sparse.get(\"path\", \"opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini\")\n\n            # Merge in sparse parameters\n            self.config[\"scoring\"] = {**{\"method\": \"sparse\"}, **sparse}\n\n        elif method:\n            # Sparse keyword configuration\n            self.config[\"scoring\"] = {\"method\": method, \"terms\": True, \"normalize\": True}\n\n    def defaultallowed(self):\n        \"\"\"\n        Tests if this embeddings instance can use a default model if not otherwise provided.\n\n        Returns:\n            True if a default model is allowed, False otherwise\n        \"\"\"\n\n        params = [(\"keyword\", False), (\"sparse\", False), (\"defaults\", True)]\n        return all(self.config.get(key, default) == default for key, default in params)\n\n    def loadvectors(self):\n        \"\"\"\n        Loads a vector model set in config.\n\n        Returns:\n            vector model\n        \"\"\"\n\n        # Create model cache if subindexes are enabled\n        if \"indexes\" in self.config and self.models is None:\n            self.models = {}\n\n        # Support path via dense parameter\n        dense = self.config.get(\"dense\")\n        if not self.config.get(\"path\") and dense and isinstance(dense, str):\n            self.config[\"path\"] = dense\n\n        # Load vector model\n        return VectorsFactory.create(self.config, self.scoring, self.models)\n\n    def loadquery(self):\n        \"\"\"\n        Loads a query model set in config.\n\n        Returns:\n            query model\n        \"\"\"\n\n        if \"query\" in self.config:\n            return Query(**self.config[\"query\"])\n\n        return None\n\n    def checkarchive(self, path):\n        \"\"\"\n        Checks if path is an archive file.\n\n        Args:\n            path: path to check\n\n        Returns:\n            (working directory, current path) if this is an archive, original path otherwise\n        \"\"\"\n\n        # Create archive instance, if necessary\n        self.archive = ArchiveFactory.create()\n\n        # Check if path is an archive file\n        if self.archive.isarchive(path):\n            # Return temporary archive working directory and original path\n            return self.archive.path(), path\n\n        return path, None\n\n    def createcloud(self, **cloud):\n        \"\"\"\n        Creates a cloud instance from config.\n\n        Args:\n            cloud: cloud configuration\n        \"\"\"\n\n        # Merge keyword args and keys under the cloud parameter\n        config = cloud\n        if \"cloud\" in config and config[\"cloud\"]:\n            config.update(config.pop(\"cloud\"))\n\n        # Create cloud instance from config and return\n        return CloudFactory.create(config) if config else None\n\n    def createann(self):\n        \"\"\"\n        Creates an ANN from config.\n\n        Returns:\n            new ANN, if enabled in config\n        \"\"\"\n\n        # Free existing resources\n        if self.ann:\n            self.ann.close()\n\n        return ANNFactory.create(self.config) if self.config.get(\"path\") or self.defaultallowed() else None\n\n    def createdatabase(self):\n        \"\"\"\n        Creates a database from config. This method will also close any existing database connection.\n\n        Returns:\n            new database, if enabled in config\n        \"\"\"\n\n        # Free existing resources\n        if self.database:\n            self.database.close()\n\n        config = self.config.copy()\n\n        # Create references to callable functions\n        self.functions = Functions(self) if \"functions\" in config else None\n        if self.functions:\n            config[\"functions\"] = self.functions(config)\n\n        # Create database from config and return\n        return DatabaseFactory.create(config)\n\n    def creategraph(self):\n        \"\"\"\n        Creates a graph from config.\n\n        Returns:\n            new graph, if enabled in config\n        \"\"\"\n\n        # Free existing resources\n        if self.graph:\n            self.graph.close()\n\n        if \"graph\" in self.config:\n            # Get or create graph configuration\n            config = self.config[\"graph\"] if \"graph\" in self.config else {}\n\n            # Create configuration with custom columns, if necessary\n            config = self.columns(config)\n            return GraphFactory.create(config)\n\n        return None\n\n    def createids(self, ids=None):\n        \"\"\"\n        Creates indexids when content is disabled.\n\n        Args:\n            ids: optional ids to add\n\n        Returns:\n            new indexids, if content disabled\n        \"\"\"\n\n        # Load index ids when content is disabled\n        return IndexIds(self, ids) if not self.config.get(\"content\") else None\n\n    def createindexes(self):\n        \"\"\"\n        Creates subindexes from config.\n\n        Returns:\n            list of subindexes\n        \"\"\"\n\n        # Free existing resources\n        if self.indexes:\n            self.indexes.close()\n\n        # Load subindexes\n        if \"indexes\" in self.config:\n            indexes = {}\n            for index, config in self.config[\"indexes\"].items():\n                # Create index with shared model cache\n                indexes[index] = Embeddings(config, models=self.models)\n\n            # Wrap as Indexes object\n            return Indexes(self, indexes)\n\n        return None\n\n    def createscoring(self):\n        \"\"\"\n        Creates a scoring from config.\n\n        Returns:\n            new scoring, if enabled in config\n        \"\"\"\n\n        # Free existing resources\n        if self.scoring:\n            self.scoring.close()\n\n        if \"scoring\" in self.config:\n            # Expand scoring to a dictionary, if necessary\n            config = self.config[\"scoring\"]\n            config = config if isinstance(config, dict) else {\"method\": config}\n\n            # Create configuration with custom columns, if necessary\n            config = self.columns(config)\n            return ScoringFactory.create(config, self.models)\n\n        return None\n\n    def hassparse(self):\n        \"\"\"\n        Checks is this embeddings database has an associated sparse index.\n\n        Returns:\n            True if this embeddings has an associated scoring index\n        \"\"\"\n\n        # Create scoring only if scoring is a sparse keyword/vector index\n        return ScoringFactory.issparse(self.config.get(\"scoring\"))\n\n    def columns(self, config):\n        \"\"\"\n        Adds custom text/object column information if it's provided.\n\n        Args:\n            config: input configuration\n\n        Returns:\n            config with column information added\n        \"\"\"\n\n        # Add text/object columns if custom\n        if \"columns\" in self.config:\n            # Work on copy of configuration\n            config = config.copy()\n\n            # Copy columns to config\n            config[\"columns\"] = self.config[\"columns\"]\n\n        return config\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.__init__","title":"<code>__init__(config=None, models=None, **kwargs)</code>","text":"<p>Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>embeddings configuration</p> <code>None</code> <code>models</code> <p>models cache, used for model sharing between embeddings</p> <code>None</code> <code>kwargs</code> <p>additional configuration as keyword args</p> <code>{}</code> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def __init__(self, config=None, models=None, **kwargs):\n    \"\"\"\n    Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized.\n\n    Args:\n        config: embeddings configuration\n        models: models cache, used for model sharing between embeddings\n        kwargs: additional configuration as keyword args\n    \"\"\"\n\n    # Index configuration\n    self.config = None\n\n    # Dimensionality reduction - word vectors only\n    self.reducer = None\n\n    # Dense vector model - transforms data into similarity vectors\n    self.model = None\n\n    # Approximate nearest neighbor index\n    self.ann = None\n\n    # Index ids when content is disabled\n    self.ids = None\n\n    # Document database\n    self.database = None\n\n    # Resolvable functions\n    self.functions = None\n\n    # Graph network\n    self.graph = None\n\n    # Sparse vectors\n    self.scoring = None\n\n    # Query model\n    self.query = None\n\n    # Index archive\n    self.archive = None\n\n    # Subindexes for this embeddings instance\n    self.indexes = None\n\n    # Models cache\n    self.models = models\n\n    # Merge configuration into single dictionary\n    config = {**config, **kwargs} if config and kwargs else kwargs if kwargs else config\n\n    # Set initial configuration\n    self.configure(config)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.batchexplain","title":"<code>batchexplain(queries, texts=None, limit=None)</code>","text":"<p>Explains the importance of each input token in text for a list of queries. This method requires either content to be enabled or texts to be provided.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <p>input queries</p> required <code>texts</code> <p>optional list of (text|list of tokens), otherwise runs search queries</p> <code>None</code> <code>limit</code> <p>optional limit if texts is None</p> <code>None</code> <p>Returns:</p> Type Description <p>list of dict per input text per query where a higher token scores represents higher importance relative to the query</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def batchexplain(self, queries, texts=None, limit=None):\n    \"\"\"\n    Explains the importance of each input token in text for a list of queries. This method requires either content to be enabled\n    or texts to be provided.\n\n    Args:\n        queries: input queries\n        texts: optional list of (text|list of tokens), otherwise runs search queries\n        limit: optional limit if texts is None\n\n    Returns:\n        list of dict per input text per query where a higher token scores represents higher importance relative to the query\n    \"\"\"\n\n    return Explain(self)(queries, texts, limit)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.batchsearch","title":"<code>batchsearch(queries, limit=None, weights=None, index=None, parameters=None, graph=False)</code>","text":"<p>Finds documents most similar to the input query. This method runs an index search, index + database search or a graph search, depending on the embeddings configuration and query.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <p>input queries</p> required <code>limit</code> <p>maximum results</p> <code>None</code> <code>weights</code> <p>hybrid score weights, if applicable</p> <code>None</code> <code>index</code> <p>index name, if applicable</p> <code>None</code> <code>parameters</code> <p>list of dicts of named parameters to bind to placeholders</p> <code>None</code> <code>graph</code> <p>return graph results if True</p> <code>False</code> <p>Returns:</p> Type Description <p>list of (id, score) per query for index search</p> <p>list of dict per query for an index + database search</p> <p>list of graph per query when graph is set to True</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def batchsearch(self, queries, limit=None, weights=None, index=None, parameters=None, graph=False):\n    \"\"\"\n    Finds documents most similar to the input query. This method runs an index search, index + database search\n    or a graph search, depending on the embeddings configuration and query.\n\n    Args:\n        queries: input queries\n        limit: maximum results\n        weights: hybrid score weights, if applicable\n        index: index name, if applicable\n        parameters: list of dicts of named parameters to bind to placeholders\n        graph: return graph results if True\n\n    Returns:\n        list of (id, score) per query for index search\n        list of dict per query for an index + database search\n        list of graph per query when graph is set to True\n    \"\"\"\n\n    # Determine if graphs should be returned\n    graph = graph if self.graph else False\n\n    # Execute search\n    results = Search(self, indexids=graph)(queries, limit, weights, index, parameters)\n\n    # Create subgraphs using results, if necessary\n    return [self.graph.filter(x) if isinstance(x, list) else x for x in results] if graph else results\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.batchsimilarity","title":"<code>batchsimilarity(queries, data)</code>","text":"<p>Computes the similarity between list of queries and list of data. Returns a list of (id, score) sorted by highest score per query, where id is the index in data.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <p>input queries</p> required <code>data</code> <p>list of data</p> required <p>Returns:</p> Type Description <p>list of (id, score) per query</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def batchsimilarity(self, queries, data):\n    \"\"\"\n    Computes the similarity between list of queries and list of data. Returns a list\n    of (id, score) sorted by highest score per query, where id is the index in data.\n\n    Args:\n        queries: input queries\n        data: list of data\n\n    Returns:\n        list of (id, score) per query\n    \"\"\"\n\n    # Convert queries to embedding vectors\n    queries = self.batchtransform(((None, query, None) for query in queries), \"query\")\n    data = self.batchtransform(((None, row, None) for row in data), \"data\")\n\n    # Get vector model\n    model = self.findmodel()\n\n    # Dot product on normalized vectors is equal to cosine similarity\n    scores = model.dot(queries, data)\n\n    # Add index and sort desc based on score\n    return [sorted(enumerate(score), key=lambda x: x[1], reverse=True) for score in scores]\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.batchterms","title":"<code>batchterms(queries)</code>","text":"<p>Extracts keyword terms from a list of queries.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <p>list of queries</p> required <p>Returns:</p> Type Description <p>list of queries reduced down to keyword term strings</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def batchterms(self, queries):\n    \"\"\"\n    Extracts keyword terms from a list of queries.\n\n    Args:\n        queries: list of queries\n\n    Returns:\n        list of queries reduced down to keyword term strings\n    \"\"\"\n\n    return Terms(self)(queries)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.batchtransform","title":"<code>batchtransform(documents, category=None, index=None)</code>","text":"<p>Transforms documents into embeddings vectors.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <p>iterable of (id, data, tags), (id, data) or data</p> required <code>category</code> <p>category for instruction-based embeddings</p> <code>None</code> <code>index</code> <p>index name, if applicable</p> <code>None</code> <p>Returns:</p> Type Description <p>embeddings vectors</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def batchtransform(self, documents, category=None, index=None):\n    \"\"\"\n    Transforms documents into embeddings vectors.\n\n    Args:\n        documents: iterable of (id, data, tags), (id, data) or data\n        category: category for instruction-based embeddings\n        index: index name, if applicable\n\n    Returns:\n        embeddings vectors\n    \"\"\"\n\n    # Initialize default parameters, if necessary\n    self.defaults()\n\n    # Get vector model\n    model = self.findmodel(index)\n\n    # Convert documents into embeddings\n    embeddings = model.batchtransform(Stream(self)(documents), category)\n\n    # Reduce the dimensionality of the embeddings. Scale the embeddings using this\n    # model to reduce the noise of common but less relevant terms.\n    if self.reducer:\n        self.reducer(embeddings)\n\n    return embeddings\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.close","title":"<code>close()</code>","text":"<p>Closes this embeddings index and frees all resources.</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def close(self):\n    \"\"\"\n    Closes this embeddings index and frees all resources.\n    \"\"\"\n\n    self.config, self.archive = None, None\n    self.reducer, self.query = None, None\n    self.ids = None\n\n    # Close ANN\n    if self.ann:\n        self.ann.close()\n        self.ann = None\n\n    # Close database\n    if self.database:\n        self.database.close()\n        self.database, self.functions = None, None\n\n    # Close scoring\n    if self.scoring:\n        self.scoring.close()\n        self.scoring = None\n\n    # Close graph\n    if self.graph:\n        self.graph.close()\n        self.graph = None\n\n    # Close indexes\n    if self.indexes:\n        self.indexes.close()\n        self.indexes = None\n\n    # Close vectors model\n    if self.model:\n        self.model.close()\n        self.model = None\n\n    self.models = None\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.count","title":"<code>count()</code>","text":"<p>Total number of elements in this embeddings index.</p> <p>Returns:</p> Type Description <p>number of elements in this embeddings index</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def count(self):\n    \"\"\"\n    Total number of elements in this embeddings index.\n\n    Returns:\n        number of elements in this embeddings index\n    \"\"\"\n\n    if self.ann:\n        return self.ann.count()\n    if self.scoring:\n        return self.scoring.count()\n    if self.database:\n        return self.database.count()\n    if self.ids:\n        return len([uid for uid in self.ids if uid is not None])\n\n    # Default to 0 when no suitable method found\n    return 0\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.delete","title":"<code>delete(ids)</code>","text":"<p>Deletes from an embeddings index. Returns list of ids deleted.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <p>list of ids to delete</p> required <p>Returns:</p> Type Description <p>list of ids deleted</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def delete(self, ids):\n    \"\"\"\n    Deletes from an embeddings index. Returns list of ids deleted.\n\n    Args:\n        ids: list of ids to delete\n\n    Returns:\n        list of ids deleted\n    \"\"\"\n\n    # List of internal indices for each candidate id to delete\n    indices = []\n\n    # List of deleted ids\n    deletes = []\n\n    if self.database:\n        # Retrieve indexid-id mappings from database\n        ids = self.database.ids(ids)\n\n        # Parse out indices and ids to delete\n        indices = [i for i, _ in ids]\n        deletes = sorted(set(uid for _, uid in ids))\n\n        # Delete ids from database\n        self.database.delete(deletes)\n    elif self.ann or self.scoring:\n        # Find existing ids\n        for uid in ids:\n            indices.extend([index for index, value in enumerate(self.ids) if uid == value])\n\n        # Clear embeddings ids\n        for index in indices:\n            deletes.append(self.ids[index])\n            self.ids[index] = None\n\n    # Delete indices for all indexes and data stores\n    if indices:\n        # Delete ids from ann\n        if self.isdense():\n            self.ann.delete(indices)\n\n        # Delete ids from scoring\n        if self.issparse():\n            self.scoring.delete(indices)\n\n        # Delete ids from subindexes\n        if self.indexes:\n            self.indexes.delete(indices)\n\n        # Delete ids from graph\n        if self.graph:\n            self.graph.delete(indices)\n\n    return deletes\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.exists","title":"<code>exists(path=None, cloud=None, **kwargs)</code>","text":"<p>Checks if an index exists at path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>input path</p> <code>None</code> <code>cloud</code> <p>cloud storage configuration</p> <code>None</code> <code>kwargs</code> <p>additional configuration as keyword args</p> <code>{}</code> <p>Returns:</p> Type Description <p>True if index exists, False otherwise</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def exists(self, path=None, cloud=None, **kwargs):\n    \"\"\"\n    Checks if an index exists at path.\n\n    Args:\n        path: input path\n        cloud: cloud storage configuration\n        kwargs: additional configuration as keyword args\n\n    Returns:\n        True if index exists, False otherwise\n    \"\"\"\n\n    # Check if this exists in a cloud instance\n    cloud = self.createcloud(cloud=cloud, **kwargs)\n    if cloud:\n        return cloud.exists(path)\n\n    # Check if this is an archive file and exists\n    path, apath = self.checkarchive(path)\n    if apath:\n        return os.path.exists(apath)\n\n    # Return true if path has a config.json or config file with an offset set\n    return path and (os.path.exists(f\"{path}/config.json\") or os.path.exists(f\"{path}/config\")) and \"offset\" in Configuration().load(path)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.explain","title":"<code>explain(query, texts=None, limit=None)</code>","text":"<p>Explains the importance of each input token in text for a query. This method requires either content to be enabled or texts to be provided.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>input query</p> required <code>texts</code> <p>optional list of (text|list of tokens), otherwise runs search query</p> <code>None</code> <code>limit</code> <p>optional limit if texts is None</p> <code>None</code> <p>Returns:</p> Type Description <p>list of dict per input text where a higher token scores represents higher importance relative to the query</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def explain(self, query, texts=None, limit=None):\n    \"\"\"\n    Explains the importance of each input token in text for a query. This method requires either content to be enabled\n    or texts to be provided.\n\n    Args:\n        query: input query\n        texts: optional list of (text|list of tokens), otherwise runs search query\n        limit: optional limit if texts is None\n\n    Returns:\n        list of dict per input text where a higher token scores represents higher importance relative to the query\n    \"\"\"\n\n    results = self.batchexplain([query], texts, limit)\n    return results[0] if results else results\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.findmodel","title":"<code>findmodel(index=None)</code>","text":"<p>Finds the primary vector model used by this instance.</p> <p>Returns:</p> Type Description <p>Vectors</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def findmodel(self, index=None):\n    \"\"\"\n    Finds the primary vector model used by this instance.\n\n    Returns:\n        Vectors\n    \"\"\"\n\n    return (\n        self.indexes.findmodel(index)\n        if index and self.indexes\n        else (\n            self.model\n            if self.model\n            else self.scoring.findmodel() if self.scoring and self.scoring.findmodel() else self.indexes.findmodel() if self.indexes else None\n        )\n    )\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.hassparse","title":"<code>hassparse()</code>","text":"<p>Checks is this embeddings database has an associated sparse index.</p> <p>Returns:</p> Type Description <p>True if this embeddings has an associated scoring index</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def hassparse(self):\n    \"\"\"\n    Checks is this embeddings database has an associated sparse index.\n\n    Returns:\n        True if this embeddings has an associated scoring index\n    \"\"\"\n\n    # Create scoring only if scoring is a sparse keyword/vector index\n    return ScoringFactory.issparse(self.config.get(\"scoring\"))\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.index","title":"<code>index(documents, reindex=False, checkpoint=None)</code>","text":"<p>Builds an embeddings index. This method overwrites an existing index.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <p>iterable of (id, data, tags), (id, data) or data</p> required <code>reindex</code> <p>if this is a reindex operation in which case database creation is skipped, defaults to False</p> <code>False</code> <code>checkpoint</code> <p>optional checkpoint directory, enables indexing restart</p> <code>None</code> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def index(self, documents, reindex=False, checkpoint=None):\n    \"\"\"\n    Builds an embeddings index. This method overwrites an existing index.\n\n    Args:\n        documents: iterable of (id, data, tags), (id, data) or data\n        reindex: if this is a reindex operation in which case database creation is skipped, defaults to False\n        checkpoint: optional checkpoint directory, enables indexing restart\n    \"\"\"\n\n    # Initialize index\n    self.initindex(reindex)\n\n    # Create transform and stream\n    transform = Transform(self, Action.REINDEX if reindex else Action.INDEX, checkpoint)\n    stream = Stream(self, Action.REINDEX if reindex else Action.INDEX)\n\n    with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".npy\") as buffer:\n        # Load documents into database and transform to vectors\n        ids, dimensions, embeddings = transform(stream(documents), buffer)\n        if embeddings is not None:\n            # Build LSA model (if enabled). Remove principal components from embeddings.\n            if self.config.get(\"pca\"):\n                self.reducer = Reducer(embeddings, self.config[\"pca\"])\n                self.reducer(embeddings)\n\n            # Save index dimensions\n            self.config[\"dimensions\"] = dimensions\n\n            # Create approximate nearest neighbor index\n            self.ann = self.createann()\n\n            # Add embeddings to the index\n            self.ann.index(embeddings)\n\n        # Save indexids-ids mapping for indexes with no database, except when this is a reindex\n        if ids and not reindex and not self.database:\n            self.ids = self.createids(ids)\n\n    # Index scoring, if necessary\n    # This must occur before graph index in order to be available to the graph\n    if self.issparse():\n        self.scoring.index()\n\n    # Index subindexes, if necessary\n    if self.indexes:\n        self.indexes.index()\n\n    # Index graph, if necessary\n    if self.graph:\n        self.graph.index(Search(self, indexonly=True), Ids(self), self.batchsimilarity)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.info","title":"<code>info()</code>","text":"<p>Prints the current embeddings index configuration.</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def info(self):\n    \"\"\"\n    Prints the current embeddings index configuration.\n    \"\"\"\n\n    if self.config:\n        # Print configuration\n        print(json.dumps(self.config, sort_keys=True, default=str, indent=2))\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.isdense","title":"<code>isdense()</code>","text":"<p>Checks if this instance has an associated ANN instance.</p> <p>Returns:</p> Type Description <p>True if this instance has an associated ANN, False otherwise</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def isdense(self):\n    \"\"\"\n    Checks if this instance has an associated ANN instance.\n\n    Returns:\n        True if this instance has an associated ANN, False otherwise\n    \"\"\"\n\n    return self.ann is not None\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.issparse","title":"<code>issparse()</code>","text":"<p>Checks if this instance has an associated sparse keyword or sparse vectors scoring index.</p> <p>Returns:</p> Type Description <p>True if scoring has an associated sparse keyword/vector index, False otherwise</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def issparse(self):\n    \"\"\"\n    Checks if this instance has an associated sparse keyword or sparse vectors scoring index.\n\n    Returns:\n        True if scoring has an associated sparse keyword/vector index, False otherwise\n    \"\"\"\n\n    return self.scoring and self.scoring.issparse()\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.isweighted","title":"<code>isweighted()</code>","text":"<p>Checks if this instance has an associated scoring instance with term weighting enabled.</p> <p>Returns:</p> Type Description <p>True if term weighting is enabled, False otherwise</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def isweighted(self):\n    \"\"\"\n    Checks if this instance has an associated scoring instance with term weighting enabled.\n\n    Returns:\n        True if term weighting is enabled, False otherwise\n    \"\"\"\n\n    return self.scoring and self.scoring.isweighted()\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.load","title":"<code>load(path=None, cloud=None, config=None, **kwargs)</code>","text":"<p>Loads an existing index from path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>input path</p> <code>None</code> <code>cloud</code> <p>cloud storage configuration</p> <code>None</code> <code>config</code> <p>configuration overrides</p> <code>None</code> <code>kwargs</code> <p>additional configuration as keyword args</p> <code>{}</code> <p>Returns:</p> Type Description <p>Embeddings</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def load(self, path=None, cloud=None, config=None, **kwargs):\n    \"\"\"\n    Loads an existing index from path.\n\n    Args:\n        path: input path\n        cloud: cloud storage configuration\n        config: configuration overrides\n        kwargs: additional configuration as keyword args\n\n    Returns:\n        Embeddings\n    \"\"\"\n\n    # Load from cloud, if configured\n    cloud = self.createcloud(cloud=cloud, **kwargs)\n    if cloud:\n        path = cloud.load(path)\n\n    # Check if this is an archive file and extract\n    path, apath = self.checkarchive(path)\n    if apath:\n        self.archive.load(apath)\n\n    # Load index configuration\n    self.config = Configuration().load(path)\n\n    # Apply config overrides\n    self.config = {**self.config, **config} if config else self.config\n\n    # Approximate nearest neighbor index - stores dense vectors\n    self.ann = self.createann()\n    if self.ann:\n        self.ann.load(f\"{path}/embeddings\")\n\n    # Dimensionality reduction model - word vectors only\n    if self.config.get(\"pca\"):\n        self.reducer = Reducer()\n        self.reducer.load(f\"{path}/lsa\")\n\n    # Index ids when content is disabled\n    self.ids = self.createids()\n    if self.ids:\n        self.ids.load(f\"{path}/ids\")\n\n    # Document database - stores document content\n    self.database = self.createdatabase()\n    if self.database:\n        self.database.load(f\"{path}/documents\")\n\n    # Sparse vectors - stores term sparse arrays\n    self.scoring = self.createscoring()\n    if self.scoring:\n        self.scoring.load(f\"{path}/scoring\")\n\n    # Subindexes\n    self.indexes = self.createindexes()\n    if self.indexes:\n        self.indexes.load(f\"{path}/indexes\")\n\n    # Graph network - stores relationships\n    self.graph = self.creategraph()\n    if self.graph:\n        self.graph.load(f\"{path}/graph\")\n\n    # Dense vectors - transforms data to embeddings vectors\n    self.model = self.loadvectors()\n\n    # Query model\n    self.query = self.loadquery()\n\n    return self\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.reindex","title":"<code>reindex(config=None, function=None, **kwargs)</code>","text":"<p>Recreates embeddings index using config. This method only works if document content storage is enabled.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>new config</p> <code>None</code> <code>function</code> <p>optional function to prepare content for indexing</p> <code>None</code> <code>kwargs</code> <p>additional configuration as keyword args</p> <code>{}</code> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def reindex(self, config=None, function=None, **kwargs):\n    \"\"\"\n    Recreates embeddings index using config. This method only works if document content storage is enabled.\n\n    Args:\n        config: new config\n        function: optional function to prepare content for indexing\n        kwargs: additional configuration as keyword args\n    \"\"\"\n\n    if self.database:\n        # Merge configuration into single dictionary\n        config = {**config, **kwargs} if config and kwargs else config if config else kwargs\n\n        # Keep content and objects parameters to ensure database is preserved\n        config[\"content\"] = self.config[\"content\"]\n        if \"objects\" in self.config:\n            config[\"objects\"] = self.config[\"objects\"]\n\n        # Reset configuration\n        self.configure(config)\n\n        # Reset function references\n        if self.functions:\n            self.functions.reset()\n\n        # Reindex\n        if function:\n            self.index(function(self.database.reindex(self.config)), True)\n        else:\n            self.index(self.database.reindex(self.config), True)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.save","title":"<code>save(path, cloud=None, **kwargs)</code>","text":"<p>Saves an index in a directory at path unless path ends with tar.gz, tar.bz2, tar.xz or zip. In those cases, the index is stored as a compressed file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>output path</p> required <code>cloud</code> <p>cloud storage configuration</p> <code>None</code> <code>kwargs</code> <p>additional configuration as keyword args</p> <code>{}</code> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def save(self, path, cloud=None, **kwargs):\n    \"\"\"\n    Saves an index in a directory at path unless path ends with tar.gz, tar.bz2, tar.xz or zip.\n    In those cases, the index is stored as a compressed file.\n\n    Args:\n        path: output path\n        cloud: cloud storage configuration\n        kwargs: additional configuration as keyword args\n    \"\"\"\n\n    if self.config:\n        # Check if this is an archive file\n        path, apath = self.checkarchive(path)\n\n        # Create output directory, if necessary\n        os.makedirs(path, exist_ok=True)\n\n        # Save index configuration\n        Configuration().save(self.config, path)\n\n        # Save approximate nearest neighbor index\n        if self.ann:\n            self.ann.save(f\"{path}/embeddings\")\n\n        # Save dimensionality reduction model (word vectors only)\n        if self.reducer:\n            self.reducer.save(f\"{path}/lsa\")\n\n        # Save index ids\n        if self.ids:\n            self.ids.save(f\"{path}/ids\")\n\n        # Save document database\n        if self.database:\n            self.database.save(f\"{path}/documents\")\n\n        # Save scoring index\n        if self.scoring:\n            self.scoring.save(f\"{path}/scoring\")\n\n        # Save subindexes\n        if self.indexes:\n            self.indexes.save(f\"{path}/indexes\")\n\n        # Save graph\n        if self.graph:\n            self.graph.save(f\"{path}/graph\")\n\n        # If this is an archive, save it\n        if apath:\n            self.archive.save(apath)\n\n        # Save to cloud, if configured\n        cloud = self.createcloud(cloud=cloud, **kwargs)\n        if cloud:\n            cloud.save(apath if apath else path)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.score","title":"<code>score(documents)</code>","text":"<p>Builds a term weighting scoring index. Only used by word vectors models.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <p>iterable of (id, data, tags), (id, data) or data</p> required Source code in <code>txtai/embeddings/base.py</code> <pre><code>def score(self, documents):\n    \"\"\"\n    Builds a term weighting scoring index. Only used by word vectors models.\n\n    Args:\n        documents: iterable of (id, data, tags), (id, data) or data\n    \"\"\"\n\n    # Build scoring index for word vectors term weighting\n    if self.isweighted():\n        self.scoring.index(Stream(self)(documents))\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.search","title":"<code>search(query, limit=None, weights=None, index=None, parameters=None, graph=False)</code>","text":"<p>Finds documents most similar to the input query. This method runs an index search, index + database search or a graph search, depending on the embeddings configuration and query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>input query</p> required <code>limit</code> <p>maximum results</p> <code>None</code> <code>weights</code> <p>hybrid score weights, if applicable</p> <code>None</code> <code>index</code> <p>index name, if applicable</p> <code>None</code> <code>parameters</code> <p>dict of named parameters to bind to placeholders</p> <code>None</code> <code>graph</code> <p>return graph results if True</p> <code>False</code> <p>Returns:</p> Type Description <p>list of (id, score) for index search</p> <p>list of dict for an index + database search</p> <p>graph when graph is set to True</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def search(self, query, limit=None, weights=None, index=None, parameters=None, graph=False):\n    \"\"\"\n    Finds documents most similar to the input query. This method runs an index search, index + database search\n    or a graph search, depending on the embeddings configuration and query.\n\n    Args:\n        query: input query\n        limit: maximum results\n        weights: hybrid score weights, if applicable\n        index: index name, if applicable\n        parameters: dict of named parameters to bind to placeholders\n        graph: return graph results if True\n\n    Returns:\n        list of (id, score) for index search\n        list of dict for an index + database search\n        graph when graph is set to True\n    \"\"\"\n\n    results = self.batchsearch([query], limit, weights, index, [parameters], graph)\n    return results[0] if results else results\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.similarity","title":"<code>similarity(query, data)</code>","text":"<p>Computes the similarity between query and list of data. Returns a list of (id, score) sorted by highest score, where id is the index in data.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>input query</p> required <code>data</code> <p>list of data</p> required <p>Returns:</p> Type Description <p>list of (id, score)</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def similarity(self, query, data):\n    \"\"\"\n    Computes the similarity between query and list of data. Returns a list of\n    (id, score) sorted by highest score, where id is the index in data.\n\n    Args:\n        query: input query\n        data: list of data\n\n    Returns:\n        list of (id, score)\n    \"\"\"\n\n    return self.batchsimilarity([query], data)[0]\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.terms","title":"<code>terms(query)</code>","text":"<p>Extracts keyword terms from a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>input query</p> required <p>Returns:</p> Type Description <p>query reduced down to keyword terms</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def terms(self, query):\n    \"\"\"\n    Extracts keyword terms from a query.\n\n    Args:\n        query: input query\n\n    Returns:\n        query reduced down to keyword terms\n    \"\"\"\n\n    return self.batchterms([query])[0]\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.transform","title":"<code>transform(document, category=None, index=None)</code>","text":"<p>Transforms document into an embeddings vector.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <p>iterable of (id, data, tags), (id, data) or data</p> required <code>category</code> <p>category for instruction-based embeddings</p> <code>None</code> <code>index</code> <p>index name, if applicable</p> <code>None</code> <p>Returns:</p> Type Description <p>embeddings vector</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def transform(self, document, category=None, index=None):\n    \"\"\"\n    Transforms document into an embeddings vector.\n\n    Args:\n        documents: iterable of (id, data, tags), (id, data) or data\n        category: category for instruction-based embeddings\n        index: index name, if applicable\n\n    Returns:\n        embeddings vector\n    \"\"\"\n\n    return self.batchtransform([document], category, index)[0]\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.Embeddings.upsert","title":"<code>upsert(documents, checkpoint=None)</code>","text":"<p>Runs an embeddings upsert operation. If the index exists, new data is appended to the index, existing data is updated. If the index doesn't exist, this method runs a standard index operation.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <p>iterable of (id, data, tags), (id, data) or data</p> required <code>checkpoint</code> <p>optional checkpoint directory, enables indexing restart</p> <code>None</code> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def upsert(self, documents, checkpoint=None):\n    \"\"\"\n    Runs an embeddings upsert operation. If the index exists, new data is\n    appended to the index, existing data is updated. If the index doesn't exist,\n    this method runs a standard index operation.\n\n    Args:\n        documents: iterable of (id, data, tags), (id, data) or data\n        checkpoint: optional checkpoint directory, enables indexing restart\n    \"\"\"\n\n    # Run standard insert if index doesn't exist or it has no records\n    if not self.count():\n        self.index(documents, checkpoint=checkpoint)\n        return\n\n    # Create transform and stream\n    transform = Transform(self, Action.UPSERT, checkpoint=checkpoint)\n    stream = Stream(self, Action.UPSERT)\n\n    with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".npy\") as buffer:\n        # Load documents into database and transform to vectors\n        ids, _, embeddings = transform(stream(documents), buffer)\n        if embeddings is not None:\n            # Remove principal components from embeddings, if necessary\n            if self.reducer:\n                self.reducer(embeddings)\n\n            # Append embeddings to the index\n            self.ann.append(embeddings)\n\n        # Save indexids-ids mapping for indexes with no database\n        if ids and not self.database:\n            self.ids = self.createids(self.ids + ids)\n\n    # Scoring upsert, if necessary\n    # This must occur before graph upsert in order to be available to the graph\n    if self.issparse():\n        self.scoring.upsert()\n\n    # Subindexes upsert, if necessary\n    if self.indexes:\n        self.indexes.upsert()\n\n    # Graph upsert, if necessary\n    if self.graph:\n        self.graph.upsert(Search(self, indexonly=True), Ids(self), self.batchsimilarity)\n</code></pre>"},{"location":"embeddings/query/","title":"Query guide","text":"<p>This section covers how to query data with txtai. The simplest way to search for data is building a natural language string with the desired content to find. txtai also supports querying with SQL. We'll cover both methods here.</p>"},{"location":"embeddings/query/#natural-language-queries","title":"Natural language queries","text":"<p>In the simplest case, the query is text and the results are index text that is most similar to the query text.</p> <pre><code>embeddings.search(\"feel good story\")\nembeddings.search(\"wildlife\")\n</code></pre> <p>The queries above search the index for similarity matches on <code>feel good story</code> and <code>wildlife</code>. If content storage is enabled, a list of <code>{**query columns}</code> is returned. Otherwise, a list of <code>(id, score)</code> tuples are returned.</p>"},{"location":"embeddings/query/#sql","title":"SQL","text":"<p>txtai supports more complex queries with SQL. This is only supported if content storage is enabled. txtai has a translation layer that analyzes input SQL statements and combines similarity results with content stored in a relational database.</p> <p>SQL queries are run through <code>embeddings.search</code> like natural language queries but the examples below only show the SQL query for conciseness.</p> <pre><code>embeddings.search(\"SQL query\")\n</code></pre>"},{"location":"embeddings/query/#similar-clause","title":"Similar clause","text":"<p>The similar clause is a txtai function that enables similarity searches with SQL.</p> <pre><code>SELECT id, text, score FROM txtai WHERE similar('feel good story')\n</code></pre> <p>The similar clause takes the following arguments:</p> <pre><code>similar(\"query\", \"number of candidates\", \"index\", \"weights\")\n</code></pre> Argument Description query natural language query to run number of candidates number of candidate results to return index target index name weights hybrid score weights <p>The txtai query layer joins results from two separate components, a relational store and a similarity index. With a similar clause, a similarity search is run and those ids are fed to the underlying database query.</p> <p>The number of candidates should be larger than the desired number of results when applying additional filter clauses. This ensures that <code>limit</code> results are still returned after applying additional filters. If the number of candidates is not specified, it is defaulted as follows:</p> <ul> <li>For a single query filter clause, the default is the query limit</li> <li>With multiple filtering clauses, the default is 10x the query limit</li> </ul> <p>The index name is only applicable when subindexes are enabled. This specifies the index to use for the query.</p> <p>Weights sets the hybrid score weights when an index has both a sparse and dense index.</p>"},{"location":"embeddings/query/#dynamic-columns","title":"Dynamic columns","text":"<p>Content can be indexed in multiple ways when content storage is enabled. Remember that input documents take the form of <code>(id, data, tags)</code> tuples. If data is a string or binary content, it's indexed and searchable with <code>similar()</code> clauses.</p> <p>If data is a dictionary, then all fields in the dictionary are stored and available via SQL. The <code>text</code> field or field specified in the index configuration is indexed and searchable with <code>similar()</code> clauses.</p> <p>For example:</p> <pre><code>embeddings.index([{\"text\": \"text to index\", \"flag\": True,\n                   \"actiondate\": \"2022-01-01\"}])\n</code></pre> <p>With the above input data, queries can now have more complex filters.</p> <pre><code>SELECT text, flag, actiondate FROM txtai WHERE similar('query') AND flag = 1\nAND actiondate &gt;= '2022-01-01'\n</code></pre> <p>txtai's query layer automatically detects columns and translates queries into a format that can be understood by the underlying database.</p> <p>Nested dictionaries/JSON is supported and can be escaped with bracket statements.</p> <pre><code>embeddings.index([{\"text\": \"text to index\",\n                   \"parent\": {\"child element\": \"abc\"}}])\n</code></pre> <pre><code>SELECT text FROM txtai WHERE [parent.child element] = 'abc'\n</code></pre> <p>Note the bracket statement escaping the nested column with spaces in the name.</p>"},{"location":"embeddings/query/#bind-parameters","title":"Bind parameters","text":"<p>txtai has support for SQL bind parameters.</p> <pre><code># Query with a bind parameter for similar clause\nquery = \"SELECT id, text, score FROM txtai WHERE similar(:x)\"\nresults = embeddings.search(query, parameters={\"x\": \"feel good story\"})\n\n# Query with a bind parameter for column filter\nquery = \"SELECT text, flag, actiondate FROM txtai WHERE flag = :x\"\nresults = embeddings.search(query, parameters={\"x\": 1})\n</code></pre>"},{"location":"embeddings/query/#aggregation-queries","title":"Aggregation queries","text":"<p>The goal of txtai's query language is to closely support all functions in the underlying database engine. The main challenge is ensuring dynamic columns are properly escaped into the engines native query function. </p> <p>Aggregation query examples.</p> <pre><code>SELECT count(*) FROM txtai WHERE similar('feel good story') AND score &gt;= 0.15\nSELECT max(length(text)) FROM txtai WHERE similar('feel good story')\nAND score &gt;= 0.15\nSELECT count(*), flag FROM txtai GROUP BY flag ORDER BY count(*) DESC\n</code></pre>"},{"location":"embeddings/query/#binary-objects","title":"Binary objects","text":"<p>txtai has support for storing and retrieving binary objects. Binary objects can be retrieved as shown in the example below.</p> <pre><code># Create embeddings index with content and object storage enabled\nembeddings = Embeddings(content=True, objects=True)\n\n# Get an image\nrequest = open(\"demo.gif\", \"rb\")\n\n# Insert record\nembeddings.index([(\"txtai\", {\"text\": \"txtai executes machine-learning workflows.\",\n                             \"object\": request.read()})])\n\n# Query txtai and get associated object\nquery = \"SELECT object FROM txtai WHERE similar('machine learning') LIMIT 1\"\nresult = embeddings.search(query)[0][\"object\"]\n\n# Query binary content with a bind parameter\nquery = \"SELECT object FROM txtai WHERE similar(:x) LIMIT 1\"\nresults = embeddings.search(query, parameters={\"x\": request.read()})\n</code></pre>"},{"location":"embeddings/query/#custom-sql-functions","title":"Custom SQL functions","text":"<p>Custom, user-defined SQL functions extend selection, filtering and ordering clauses with additional logic. For example, the following snippet defines a function that translates text using a translation pipeline.</p> <pre><code># Translation pipeline\ntranslate = Translation()\n\n# Create embeddings index\nembeddings = Embeddings(path=\"sentence-transformers/nli-mpnet-base-v2\",\n                        content=True,\n                        functions=[translate]})\n\n# Run a search using a custom SQL function\nembeddings.search(\"\"\"\nSELECT\n  text,\n  translation(text, 'de', null) 'text (DE)',\n  translation(text, 'es', null) 'text (ES)',\n  translation(text, 'fr', null) 'text (FR)'\nFROM txtai WHERE similar('feel good story')\nLIMIT 1\n\"\"\")\n</code></pre>"},{"location":"embeddings/query/#expressions","title":"Expressions","text":"<p>Expression shortcuts expand into more complex SQL snippets. This is useful for making SQL queries more concise. Indexing is also available on expressions as a performance improvement.</p> <p>The following example indexes a json extraction field (<code>filepath</code>) and the length of each field.</p> <pre><code># Create embeddings index\nembeddings = Embeddings(path=\"sentence-transformers/nli-mpnet-base-v2\",\n                        content=True,\n                        expressions=[\n                          {\"name\": \"filepath\", \"index\": True},\n                          {\"name\": \"textlength\", \"expression\": \"length(text)\", \"index\": True}\n                        ])\n\nembeddings.search(\"SELECT textlength, filepath FROM txtai LIMIT 1\")\n</code></pre>"},{"location":"embeddings/query/#query-translation","title":"Query translation","text":"<p>Natural language queries with filters can be converted to txtai-compatible SQL statements with query translation. For example:</p> <pre><code>embeddings.search(\"feel good story since yesterday\")\n</code></pre> <p>can be converted to a SQL statement with a similar clause and date filter.</p> <pre><code>select id, text, score from txtai where similar('feel good story') and\nentry &gt;= date('now', '-1 day')\n</code></pre> <p>This requires setting a query translation model. The default query translation model is t5-small-txtsql but this can easily be finetuned to handle different use cases.</p>"},{"location":"embeddings/query/#hybrid-search","title":"Hybrid search","text":"<p>When an embeddings database has both a sparse and dense index, both indexes will be queried and the results will be equally weighted unless otherwise specified.</p> <pre><code>embeddings.search(\"query\", weights=0.5)\nembeddings.search(\"SELECT id, text, score FROM txtai WHERE similar('query', 0.5)\")\n</code></pre>"},{"location":"embeddings/query/#graph-search","title":"Graph search","text":"<p>If an embeddings database has an associated graph network, graph searches can be run. The search syntax below uses openCypher. Follow the preceding link to learn more about this syntax.</p> <p>Additionally, standard embeddings searches can be returned as graphs.</p> <pre><code># Find all paths between id: 0 and id: 5 between 1 and 3 hops away\nembeddings.graph.search(\"\"\"\nMATCH P=({id: 0})-[*1..3]-&gt;({id: 5})\nRETURN P\n\"\"\")\n\n# Standard embeddings search as graph\nembeddings.search(\"query\", graph=True)\n</code></pre>"},{"location":"embeddings/query/#subindexes","title":"Subindexes","text":"<p>Subindexes can be queried as follows:</p> <pre><code># Query with index parameter\nembeddings.search(\"query\", index=\"subindex1\")\n\n# Specify with SQL\nembeddings.search(\"\"\"\nSELECT id, text, score FROM txtai\nWHERE similar('query', 'subindex1')\n\"\"\")\n</code></pre>"},{"location":"embeddings/query/#combined-index-architecture","title":"Combined index architecture","text":"<p>txtai has multiple storage and indexing components. Content is stored in an underlying database along with an approximate nearest neighbor (ANN) index, keyword index and graph network. These components combine to deliver similarity search alongside traditional structured search.</p> <p>The ANN index stores ids and vectors for each input element. When a natural language query is run, the query is translated into a vector and a similarity query finds the best matching ids. When a database is added into the mix, an additional step is executed. This step takes those ids and effectively inserts them as part of the underlying database query. The same steps apply with keyword indexes except a term frequency index is used to find the best matching ids.</p> <p>Dynamic columns are supported via the underlying engine. For SQLite, data is stored as JSON and dynamic columns are converted into <code>json_extract</code> clauses. Client-server databases are supported via SQLAlchemy and dynamic columns are supported provided the underlying engine has JSON support.</p>"},{"location":"embeddings/configuration/","title":"Configuration","text":"<p>The following describes available embeddings configuration. These parameters are set in the Embeddings constructor via either the <code>config</code> parameter or as keyword arguments.</p> <p>Configuration is designed to be optional and set only when needed. Out of the box, sensible defaults are picked to get up and running fast. For example:</p> <pre><code>from txtai import Embeddings\n\nembeddings = Embeddings()\n</code></pre> <p>Creates a new embeddings instance, using all-MiniLM-L6-v2 as the vector model, Faiss as the ANN index backend and content disabled.</p> <pre><code>from txtai import Embeddings\n\nembeddings = Embeddings(content=True)\n</code></pre> <p>Is the same as above except it adds in SQLite for content storage. </p> <p>The following sections link to all the available configuration options.</p>"},{"location":"embeddings/configuration/#ann","title":"ANN","text":"<p>The default vector index backend is Faiss.</p>"},{"location":"embeddings/configuration/#cloud","title":"Cloud","text":"<p>Embeddings databases can optionally be synced with cloud storage.</p>"},{"location":"embeddings/configuration/#database","title":"Database","text":"<p>Content storage is disabled by default. When enabled, SQLite is the default storage engine.</p>"},{"location":"embeddings/configuration/#general","title":"General","text":"<p>General configuration that doesn't fit elsewhere.</p>"},{"location":"embeddings/configuration/#graph","title":"Graph","text":"<p>An accomplying graph index can be created with an embeddings database. This enables topic modeling, path traversal and more. NetworkX is the default graph index.</p>"},{"location":"embeddings/configuration/#scoring","title":"Scoring","text":"<p>Sparse keyword indexing and word vectors term weighting.</p>"},{"location":"embeddings/configuration/#vectors","title":"Vectors","text":"<p>Vector search is enabled by converting text and other binary data into embeddings vectors. These vectors are then stored in an ANN index. The vector model is optional and a default model is used when not provided.</p>"},{"location":"embeddings/configuration/ann/","title":"ANN","text":"<p>Approximate Nearest Neighbor (ANN) index configuration for storing vector embeddings.</p>"},{"location":"embeddings/configuration/ann/#backend","title":"backend","text":"<pre><code>backend: faiss|hnsw|annoy|ggml|numpy|torch|pgvector|sqlite|custom\n</code></pre> <p>Sets the ANN backend. Defaults to <code>faiss</code>. Additional backends are available via the ann extras package. Set custom backends via setting this parameter to the fully resolvable class string.</p> <p>Backend-specific settings are set with a corresponding configuration object having the same name as the backend (i.e. annoy, faiss, or hnsw). These are optional and set to defaults if omitted.</p>"},{"location":"embeddings/configuration/ann/#faiss","title":"faiss","text":"<pre><code>faiss:\n    components: comma separated list of components - defaults to \"IDMap,Flat\" for small\n                indices and \"IVFx,Flat\" for larger indexes where\n                x = min(4 * sqrt(embeddings count), embeddings count / 39)\n                automatically calculates number of IVF cells when omitted (supports \"IVF,Flat\")\n    nprobe: search probe setting (int) - defaults to x/16 (as defined above)\n            for larger indexes\n    nflip: same as nprobe - only used with binary hash indexes\n    quantize: store vectors with x-bit precision vs 32-bit (boolean|int)\n              true sets 8-bit precision, false disables, int sets specified\n              precision\n    mmap: load as on-disk index (boolean) - trade query response time for a\n          smaller RAM footprint, defaults to false\n    sample: percent of data to use for model training (0.0 - 1.0)\n            reduces indexing time for larger (&gt;1M+ row) indexes, defaults to 1.0\n</code></pre> <p>Faiss supports both floating point and binary indexes. Floating point indexes are the default. Binary indexes are used when indexing scalar-quantized datasets.</p> <p>See the following Faiss documentation links for more information.</p> <ul> <li>Guidelines for choosing an index</li> <li>Index configuration summary</li> <li>Index Factory</li> <li>Binary Indexes</li> <li>Search Tuning</li> </ul> <p>Note: For macOS users, an existing bug in an upstream package restricts the number of processing threads to 1. This limitation is managed internally to prevent system crashes.</p>"},{"location":"embeddings/configuration/ann/#hnsw","title":"hnsw","text":"<pre><code>hnsw:\n    efconstruction:  ef_construction param for init_index (int) - defaults to 200\n    m: M param for init_index (int) - defaults to 16\n    randomseed: random-seed param for init_index (int) - defaults to 100\n    efsearch: ef search param (int) - defaults to None and not set\n</code></pre> <p>See Hnswlib documentation for more information on these parameters.</p>"},{"location":"embeddings/configuration/ann/#annoy","title":"annoy","text":"<pre><code>annoy:\n    ntrees: number of trees (int) - defaults to 10\n    searchk: search_k search setting (int) - defaults to -1\n</code></pre> <p>See Annoy documentation for more information on these parameters. Note that annoy indexes can not be modified after creation, upserts/deletes and other modifications are not supported.</p>"},{"location":"embeddings/configuration/ann/#ggml","title":"ggml","text":"<pre><code>ggml:\n    gpu: enable GPU - defaults to True\n    quantize: sets the tensor quantization - defaults to F32\n    querysize: query buffer size - defaults to 64\n</code></pre> <p>The GGML backend is a k-nearest neighbors backend. It stores tensors using GGML and GGUF. It supports GPU-enabled operations and supports quantization. GGML is the framework used by llama.cpp.</p> <p>See this for a list of quantization types.</p>"},{"location":"embeddings/configuration/ann/#numpy","title":"numpy","text":"<p>The NumPy backend is a k-nearest neighbors backend. It's designed for simplicity and works well with smaller datasets that fit into memory.</p> <pre><code>numpy:\n    safetensors: stores vectors using the safetensors format\n                 defaults to NumPy array storage\n</code></pre>"},{"location":"embeddings/configuration/ann/#torch","title":"torch","text":"<p>The Torch backend is a k-nearest neighbors backend like NumPy. It supports GPU-enabled operations. It also has support for quantization which enables fitting larger arrays into GPU memory.</p> <p>When quantization is enabled, vectors are always stored in safetensors. Note that macOS support for quantization is limited.</p> <pre><code>torch:\n    safetensors: stores vectors using the safetensors format - defaults\n                 to NumPy array storage if quantization is disabled\n    quantize:\n        type: quantization type (fp4, nf4, int8)\n        blocksize: quantization block size parameter\n</code></pre>"},{"location":"embeddings/configuration/ann/#pgvector","title":"pgvector","text":"<pre><code>pgvector:\n    url: database url connection string, alternatively can be set via\n         ANN_URL environment variable\n    schema: database schema to store vectors - defaults to being\n            determined by the database\n    table: database table to store vectors - defaults to `vectors`\n    precision: vector float precision (half or full) - defaults to `full`\n    efconstruction:  ef_construction param (int) - defaults to 200\n    m: M param for init_index (int) - defaults to 16\n</code></pre> <p>The pgvector backend stores embeddings in a Postgres database. See the pgvector documentation for more information on these parameters. See the SQLAlchemy documentation for more information on how to construct url connection strings.</p>"},{"location":"embeddings/configuration/ann/#sqlite","title":"sqlite","text":"<pre><code>sqlite:\n    quantize: store vectors with x-bit precision vs 32-bit (boolean|int)\n              true sets 8-bit precision, false disables, int sets specified\n              precision\n    table: database table to store vectors - defaults to `vectors`\n</code></pre> <p>The SQLite backend stores embeddings in a SQLite database using sqlite-vec. This backend supports 1-bit and 8-bit quantization at the storage level.</p> <p>See this note on how to run this ANN on MacOS.</p>"},{"location":"embeddings/configuration/cloud/","title":"Cloud","text":"<p>The following describes parameters used to sync indexes with cloud storage. Cloud object storage, the Hugging Face Hub and custom providers are all supported.</p> <p>Parameters are set via the embeddings.load and embeddings.save methods.</p>"},{"location":"embeddings/configuration/cloud/#provider","title":"provider","text":"<pre><code>provider: string\n</code></pre> <p>Cloud provider. Can be one of the following:</p> <ul> <li> <p>Cloud object storage. Set to one of these providers. Use the text shown in the <code>Provider Constant</code> column as lower case.</p> </li> <li> <p>Hugging Face Hub. Set to <code>huggingface-hub</code>.</p> </li> <li> <p>Custom providers. Set to the full class path of the custom provider.</p> </li> </ul>"},{"location":"embeddings/configuration/cloud/#container","title":"container","text":"<pre><code>container: string\n</code></pre> <p>Container/bucket/directory/repository name. Embeddings will be stored in the container with the filename specified by the <code>path</code> configuration.</p>"},{"location":"embeddings/configuration/cloud/#cloud-object-storage-configuration","title":"Cloud object storage configuration","text":"<p>In addition to the above common configuration, the cloud object storage provider has the following additional configuration parameters. Note that some cloud providers do not need any of these parameters and can use implicit authentication with service accounts.</p> <p>See the libcloud documentation for more information on these parameters.</p>"},{"location":"embeddings/configuration/cloud/#key","title":"key","text":"<pre><code>key: string\n</code></pre> <p>Provider-specific access key. Can also be set via <code>ACCESS_KEY</code> environment variable. Ensure the configuration file is secured if added to the file. When using implicit authentication, set this to a value such as 'using-implicit-auth'.</p>"},{"location":"embeddings/configuration/cloud/#secret","title":"secret","text":"<pre><code>secret: string\n</code></pre> <p>Provider-specific access secret. Can also be set via <code>ACCESS_SECRET</code> environment variable. Ensure the configuration file is secured if added to the file. When using implicit authentication, this option is not required.</p>"},{"location":"embeddings/configuration/cloud/#prefix","title":"prefix","text":"<pre><code>prefix: string\n</code></pre> <p>Optional object prefix. Object storage doesn't have the concept of a directory but a prefix is similar. For example, a prefix could be <code>base/dir</code>. This helps with organizing data in an object storage bucket.</p> <p>More can be found at the following links.</p> <ul> <li>Organizing objects using prefixes</li> <li>libcloud container method documentation</li> </ul>"},{"location":"embeddings/configuration/cloud/#host","title":"host","text":"<pre><code>host: string\n</code></pre> <p>Optional server host name. Set when using a local cloud storage server.</p>"},{"location":"embeddings/configuration/cloud/#port","title":"port","text":"<pre><code>port: int\n</code></pre> <p>Optional server port. Set when using a local cloud storage server.</p>"},{"location":"embeddings/configuration/cloud/#token","title":"token","text":"<pre><code>token: string\n</code></pre> <p>Optional temporary session token</p>"},{"location":"embeddings/configuration/cloud/#region","title":"region","text":"<pre><code>region: string\n</code></pre> <p>Optional parameter to specify the storage region, provider-specific.</p>"},{"location":"embeddings/configuration/cloud/#hugging-face-hub-configuration","title":"Hugging Face Hub configuration","text":"<p>The huggingface-hub provider supports the following additional configuration parameters. More on these parameters can be found in the Hugging Face Hub's documentation.</p>"},{"location":"embeddings/configuration/cloud/#revision","title":"revision","text":"<pre><code>revision: string\n</code></pre> <p>Optional Git revision id which can be a branch name, a tag, or a commit hash</p>"},{"location":"embeddings/configuration/cloud/#cache","title":"cache","text":"<pre><code>cache: string\n</code></pre> <p>Path to the folder where cached files are stored</p>"},{"location":"embeddings/configuration/cloud/#token_1","title":"token","text":"<pre><code>token: string|boolean\n</code></pre> <p>Token to be used for the download. If set to True, the token will be read from the Hugging Face config folder.</p>"},{"location":"embeddings/configuration/database/","title":"Database","text":"<p>Databases store metadata, text and binary content.</p>"},{"location":"embeddings/configuration/database/#content","title":"content","text":"<pre><code>content: boolean|sqlite|duckdb|client|url|custom\n</code></pre> <p>Enables content storage. When true, the default storage engine, <code>sqlite</code> will be used to save metadata.</p> <p>Client-server connections are supported with either <code>client</code> or a full connection URL. When set to <code>client</code>, the CLIENT_URL environment variable must be set to the full connection URL. See the SQLAlchemy documentation for more information on how to construct connection strings for client-server databases.</p> <p>Add custom storage engines via setting this parameter to the fully resolvable class string.</p> <p>Content storage specific settings are set with a corresponding configuration object having the same name as the content storage engine (i.e. duckdb or sqlite). These are optional and set to defaults if omitted.</p>"},{"location":"embeddings/configuration/database/#client","title":"client","text":"<pre><code>schema:  default database schema for the session - defaults to being\n         determined by the database\n</code></pre> <p>Additional settings for client-server databases. Also supported when the <code>content=url</code>.</p>"},{"location":"embeddings/configuration/database/#sqlite","title":"sqlite","text":"<pre><code>sqlite:\n    wal: enable write-ahead logging - allows concurrent read/write operations,\n         defaults to false\n</code></pre> <p>Additional settings for SQLite.</p>"},{"location":"embeddings/configuration/database/#objects","title":"objects","text":"<pre><code>objects: boolean|image|pickle\n</code></pre> <p>Enables object storage. Supports storing binary content. Requires content storage to also be enabled.</p> <p>Object encoding options are:</p> <ul> <li><code>standard</code>: Default encoder when boolean set. Encodes and decodes objects as byte arrays.</li> <li><code>image</code>: Image encoder. Encodes and decodes objects as image objects.</li> <li><code>pickle</code>: Pickle encoder. Encodes and decodes objects with the pickle module. Supports arbitrary objects.</li> </ul>"},{"location":"embeddings/configuration/database/#functions","title":"functions","text":"<pre><code>functions: list\n</code></pre> <p>List of functions with user-defined SQL functions. Each list element must be one of the following:</p> <ul> <li>function</li> <li>callable object</li> <li>dict with fields for name, argcount, function and deterministic</li> </ul> <p>An example can be found here.</p>"},{"location":"embeddings/configuration/database/#expressions","title":"expressions","text":"<pre><code>expressions: list\n</code></pre> <p>List of expression shortcuts. Each list element must be a dict with the following fields.</p> <ul> <li><code>name</code>: name of the expression</li> <li><code>expression</code>: SQL expression, defaults to <code>name</code> when empty</li> <li><code>index</code>: if this expression should have a database index, defaults to False when not provided</li> </ul> <p>The expression can be a json data column, sql function or anything that can be run as a SQL snippet.</p>"},{"location":"embeddings/configuration/database/#query","title":"query","text":"<pre><code>query:\n    path: sets the path for the query model - this can be any model on the\n          Hugging Face Model Hub or a local file path.\n    prefix: text prefix to prepend to all inputs\n    maxlength: maximum generated sequence length\n</code></pre> <p>Query translation model. Translates natural language queries to txtai compatible SQL statements.</p>"},{"location":"embeddings/configuration/general/","title":"General","text":"<p>General configuration options.</p>"},{"location":"embeddings/configuration/general/#keyword","title":"keyword","text":"<pre><code>keyword: boolean|string\n</code></pre> <p>Enables sparse keyword indexing for this embeddings.</p> <p>When set to a boolean, this parameter creates a BM25 index for full text search. When set to a string, it expects a keyword method.</p> <p>It also implicitly disables the defaults setting for vector search.</p>"},{"location":"embeddings/configuration/general/#sparse","title":"sparse","text":"<pre><code>sparse: boolean|path\n</code></pre> <p>Enables sparse vector indexing for this embeddings.</p> <p>When set to <code>True</code>, this parameter creates a sparse vector index using the default sparse index model. When set to a string, it expects a local or Hugging Face model path.</p> <p>It also implicitly disables the defaults setting for vector search.</p>"},{"location":"embeddings/configuration/general/#dense","title":"dense","text":"<pre><code>dense: boolean|string\n</code></pre> <p>Alias for the vector model path. When set to <code>True</code>, the default transformers vector model is used.</p>"},{"location":"embeddings/configuration/general/#hybrid","title":"hybrid","text":"<pre><code>hybrid: boolean\n</code></pre> <p>Enables hybrid (sparse + dense) indexing for this embeddings.</p> <p>When enabled, this parameter creates a BM25 index for full text search. It has no effect on the defaults or path settings.</p>"},{"location":"embeddings/configuration/general/#defaults","title":"defaults","text":"<pre><code>defaults: boolean\n</code></pre> <p>Uses default vector model path when enabled (default setting is True) and <code>path</code> is not provided. See this link for an example.</p>"},{"location":"embeddings/configuration/general/#indexes","title":"indexes","text":"<pre><code>indexes: dict\n</code></pre> <p>Key value pairs defining subindexes for this embeddings. Each key is the index name and the value is the full configuration. This configuration can use any of the available configurations in a standard embeddings instance.</p>"},{"location":"embeddings/configuration/general/#autoid","title":"autoid","text":"<pre><code>format: int|uuid function\n</code></pre> <p>Sets the auto id generation method. When this is not set, an autogenerated numeric sequence is used. This also supports UUID generation functions. For example, setting this value to <code>uuid4</code> will generate random UUIDs. Setting this to <code>uuid5</code> will generate deterministic UUIDs for each input data row.</p>"},{"location":"embeddings/configuration/general/#columns","title":"columns","text":"<pre><code>columns:\n    text: name of the text column\n    object: name of the object column\n    store: limit json data fields to this list of columns\n</code></pre> <p>Sets the <code>text</code> and <code>object</code> column names. Defaults to <code>text</code> and <code>object</code> if not provided.</p> <p><code>store</code> sets a list of columns to store in the JSON data field. When this isn't provided, all columns are stored (default). When <code>store</code> is set to <code>None</code>, no JSON columns are stored. This is useful is a field is only needed at indexing time but not search time.</p>"},{"location":"embeddings/configuration/general/#format","title":"format","text":"<pre><code>format: json|pickle\n</code></pre> <p>Sets the configuration storage format. Defaults to <code>json</code>.</p>"},{"location":"embeddings/configuration/graph/","title":"Graph","text":"<p>Enable graph storage via the <code>graph</code> parameter. This component requires the graph extras package.</p> <p>When enabled, a graph network is built using the embeddings index. Graph nodes are synced with each embeddings index operation (index/upsert/delete). Graph edges are created using the embeddings index upon completion of each index/upsert/delete embeddings index call.</p>"},{"location":"embeddings/configuration/graph/#backend","title":"backend","text":"<pre><code>backend: networkx|rdbms|custom\n</code></pre> <p>Sets the graph backend. Defaults to <code>networkx</code>.</p> <p>Add custom graph storage engines via setting this parameter to the fully resolvable class string.</p> <p>The <code>rdbms</code> backend has the following additional settings.</p>"},{"location":"embeddings/configuration/graph/#rdbms","title":"rdbms","text":"<pre><code>url: database url connection string, alternatively can be set via the\n     GRAPH_URL environment variable\nschema: database schema to store graph - defaults to being\n        determined by the database\nnodes: table to store node data, defaults to `nodes`\nedges: table to store edge data, defaults to `edges`\n</code></pre>"},{"location":"embeddings/configuration/graph/#batchsize","title":"batchsize","text":"<pre><code>batchsize: int\n</code></pre> <p>Batch query size, used to query embeddings index - defaults to 256.</p>"},{"location":"embeddings/configuration/graph/#limit","title":"limit","text":"<pre><code>limit: int\n</code></pre> <p>Maximum number of results to return per embeddings query - defaults to 15.</p>"},{"location":"embeddings/configuration/graph/#minscore","title":"minscore","text":"<pre><code>minscore: float\n</code></pre> <p>Minimum score required to consider embeddings query matches - defaults to 0.1.</p>"},{"location":"embeddings/configuration/graph/#approximate","title":"approximate","text":"<pre><code>approximate: boolean\n</code></pre> <p>When true, queries only run for nodes without edges - defaults to true.</p>"},{"location":"embeddings/configuration/graph/#topics","title":"topics","text":"<pre><code>topics:\n    algorithm: community detection algorithm (string), options are\n               louvain (default), greedy, lpa\n    level: controls number of topics (string), options are best (default) or first\n    resolution: controls number of topics (int), larger values create more\n                topics (int), defaults to 100\n    labels: scoring index method used to build topic labels (string)\n            options are bm25 (default), tfidf, sif\n    terms: number of frequent terms to use for topic labels (int), defaults to 4\n    stopwords: optional list of stop words to exclude from topic labels\n    categories: optional list of categories used to group topics, allows\n                granular topics with broad categories grouping topics\n</code></pre> <p>Enables topic modeling. Defaults are tuned so that in most cases these values don't need to be changed (except for categories). These parameters are available for advanced use cases where one wants full control over the community detection process.</p>"},{"location":"embeddings/configuration/graph/#copyattributes","title":"copyattributes","text":"<pre><code>copyattributes: boolean|list\n</code></pre> <p>Copy these attributes from input dictionaries in the <code>insert</code> method. If this is set to <code>True</code>, all attributes are copied. Otherwise, only the attributes specified in this list are copied to the graph as attributes.</p>"},{"location":"embeddings/configuration/scoring/","title":"Scoring","text":"<p>Enable scoring support via the <code>scoring</code> parameter.</p> <p>This scoring instance can serve two purposes, depending on the settings.</p> <p>One use case is building sparse/keyword indexes. This occurs when the <code>terms</code> parameter is set to <code>True</code>.</p> <p>The other use case is with word vector term weighting. This feature has been available since the initial version but isn't quite as common anymore.</p> <p>The following covers the available options.</p>"},{"location":"embeddings/configuration/scoring/#method","title":"method","text":"<pre><code>method: bm25|tfidf|sif|pgtext|sparse|custom\n</code></pre> <p>Sets the scoring method. Add custom scoring via setting this parameter to the fully resolvable class string.</p>"},{"location":"embeddings/configuration/scoring/#pgtext","title":"pgtext","text":"<pre><code>schema: database schema to store keyword index - defaults to being\n        determined by the database\n</code></pre> <p>Additional settings for Postgres full-text keyword indexes.</p>"},{"location":"embeddings/configuration/scoring/#sparse","title":"sparse","text":"<pre><code>path: sparse vector model path\nvectormethod: vector embeddings method\nvectornormalize: enable vector embeddings normalization (boolean)\ngpu: boolean|int|string|device\nnormalize: enable score normalization (boolean|float|string|dict)\nbatch: Sets the transform batch size\nencodebatch: Sets the encode batch size\nvectors: additional model init args\nencodeargs: additional encode() args\nbackend: ivfsparse|pgsparse\n</code></pre> <p>Sparse vector scoring options. The sparse scoring instance combines a sparse vector model with a sparse approximate nearest neighbor index (ANN). This method supports both vector normalization and score normalization.</p> <p>Vector normalization normalizes all vectors to have a magnitude of 1. By extension, all generated scores will be 0 to 1.</p> <p>Score normalization scales the output between 0 and 1. This setting supports:</p> <ul> <li><code>True</code> for default scale normalization</li> <li><code>float</code> normalize using this as the scale factor</li> <li><code>\"bayes\"</code> for Bayesian normalization using dynamic candidate score statistics</li> <li><code>{method: \"bayes\", alpha: 1.0, beta: null}</code> for Bayesian normalization with optional custom parameters</li> </ul>"},{"location":"embeddings/configuration/scoring/#ivfsparse","title":"ivfsparse","text":"<pre><code>ivfsparse:\n  sample: percent of data to use for model training (0.0 - 1.0)\n  nfeatures: top n features to use for model training (int)\n  nlist: desired number of clusters (int)\n  nprobe: search probe setting (int)\n  minpoints: minimum number of points for a cluster (int)\n</code></pre> <p>Inverted file (IVF) index with flat vector file storage and sparse array support.</p>"},{"location":"embeddings/configuration/scoring/#pgsparse","title":"pgsparse","text":"<p>Sparse ANN backed by Postgres. Supports same options as the pgvector ANN.</p>"},{"location":"embeddings/configuration/scoring/#terms","title":"terms","text":"<pre><code>terms: boolean|dict\n</code></pre> <p>Enables term frequency sparse arrays for a scoring instance. This is the backend for sparse keyword indexes.</p> <p>Supports a <code>dict</code> with the parameters <code>cachelimit</code> and <code>cutoff</code>.</p> <p><code>cachelimit</code> is the maximum amount of resident memory in bytes to use during indexing before flushing to disk. This parameter is an <code>int</code>.</p> <p><code>cutoff</code> is used during search to determine what constitutes a common term. This parameter is a <code>float</code>, i.e. 0.1 for a cutoff of 10%.</p> <p>When <code>terms</code> is set to <code>True</code>, default parameters are used for the <code>cachelimit</code> and <code>cutoff</code>. Normally, these defaults are sufficient.</p>"},{"location":"embeddings/configuration/scoring/#normalize","title":"normalize","text":"<pre><code>normalize: boolean|str|dict\n</code></pre> <p>Enables normalized scoring (ranging from 0 to 1). This setting supports:</p> <ul> <li><code>True</code> for standard score normalization</li> <li><code>\"bayes\"</code> | <code>\"bb25\"</code> for Bayesian normalization using dynamic candidate score statistics</li> <li><code>{method: \"bayes\", alpha: 1.0, beta: null}</code> for Bayesian normalization with optional custom parameters</li> </ul> <p>When standard normalization is enabled, statistics from the index are used to calculate normalized scores. When Bayesian/BB25 normalization is enabled, it uses positive-score candidates, dynamic <code>beta=median(scores)</code>, adaptive <code>alpha_eff=alpha/std(scores)</code> and a sigmoid transform (likelihood-only variant with flat prior) to map scores to <code>[0, 1]</code>.</p> <p>Bayesian normalization references:</p> <ul> <li>https://github.com/instructkr/bb25</li> <li>https://github.com/cognica-io/bayesian-bm25</li> </ul>"},{"location":"embeddings/configuration/scoring/#tokenizer","title":"tokenizer","text":"<pre><code>tokenizer: dict\n</code></pre> <p>Set tokenization rules. Passes these arguments to the underlying Tokenization pipeline.</p>"},{"location":"embeddings/configuration/vectors/","title":"Vectors","text":"<p>The following covers available vector model configuration options.</p>"},{"location":"embeddings/configuration/vectors/#path","title":"path","text":"<pre><code>path: string\n</code></pre> <p>Sets the path for a vectors model. When using a transformers/sentence-transformers model, this can be any model on the Hugging Face Hub or a local file path. Otherwise, it must be a local file path to a word embeddings model.</p>"},{"location":"embeddings/configuration/vectors/#method","title":"method","text":"<pre><code>method: transformers|sentence-transformers|llama.cpp|litellm|model2vec|external|words\n</code></pre> <p>Embeddings method to use. If the method is not provided, it is inferred using the <code>path</code>.</p> <p><code>sentence-transformers</code>, <code>llama.cpp</code>, <code>litellm</code>, <code>model2vec</code> and <code>words</code> require the vectors extras package to be installed.</p>"},{"location":"embeddings/configuration/vectors/#transformers","title":"transformers","text":"<p>Builds embeddings using a transformers model. While this can be any transformers model, it works best with models trained to build embeddings.</p> <p><code>mean</code>, <code>cls</code> and <code>late</code> pooling are supported and automatically inferred from the model. The pooling method can be overwritten by changing the method from <code>transformers</code> to <code>meanpooling</code>, <code>clspooling</code> or <code>latepooling</code> respectively.</p> <p>Setting <code>maxlength</code> to <code>True</code> enables truncating inputs to the <code>max_seq_length</code>. Setting <code>maxlength</code> to an integer will truncate inputs to that value. When omitted (default), the <code>maxlength</code> will be set to either the model or tokenizer maxlength.</p>"},{"location":"embeddings/configuration/vectors/#sentence-transformers","title":"sentence-transformers","text":"<p>Same as transformers but loads models with the sentence-transformers library.</p>"},{"location":"embeddings/configuration/vectors/#llamacpp","title":"llama.cpp","text":"<p>Builds embeddings using a llama.cpp model. Supports both local and remote GGUF paths on the HF Hub.</p>"},{"location":"embeddings/configuration/vectors/#litellm","title":"litellm","text":"<p>Builds embeddings using a LiteLLM model. See the LiteLLM documentation for the options available with LiteLLM models.</p>"},{"location":"embeddings/configuration/vectors/#model2vec","title":"model2vec","text":"<p>Builds embeddings using a Model2Vec model. Model2Vec is a knowledge-distilled version of a transformers model with static vectors.</p>"},{"location":"embeddings/configuration/vectors/#words","title":"words","text":"<p>Builds embeddings using a word embeddings model and static vectors. While Transformers models are preferred in most cases, this method can be useful for low resource and historical languages where there isn't much linguistic data available.</p>"},{"location":"embeddings/configuration/vectors/#pca","title":"pca","text":"<pre><code>pca: int\n</code></pre> <p>Removes n principal components from generated embeddings. When enabled, a TruncatedSVD model is built to help with dimensionality reduction. After pooling of vectors creates a single embedding, this method is applied.</p>"},{"location":"embeddings/configuration/vectors/#external","title":"external","text":"<p>Embeddings are created via an external model or API. Requires setting the transform parameter to a function that translates data into embeddings.</p>"},{"location":"embeddings/configuration/vectors/#transform","title":"transform","text":"<pre><code>transform: function\n</code></pre> <p>When method is <code>external</code>, this function transforms input content into embeddings. The input to this function is a list of data. This method must return either a numpy array or list of numpy arrays.</p>"},{"location":"embeddings/configuration/vectors/#gpu","title":"gpu","text":"<pre><code>gpu: boolean|int|string|device\n</code></pre> <p>Set the target device. Supports true/false, device id, device string and torch device instance. This is automatically derived if omitted.</p> <p>The <code>sentence-transformers</code> method supports encoding with multiple GPUs. This can be enabled by setting the gpu parameter to <code>all</code>.</p>"},{"location":"embeddings/configuration/vectors/#batch","title":"batch","text":"<pre><code>batch: int\n</code></pre> <p>Sets the transform batch size. This parameter controls how input streams are chunked and vectorized.</p>"},{"location":"embeddings/configuration/vectors/#encodebatch","title":"encodebatch","text":"<pre><code>encodebatch: int\n</code></pre> <p>Sets the encode batch size. This parameter controls the underlying vector model batch size. This often corresponds to a GPU batch size, which controls GPU memory usage.</p>"},{"location":"embeddings/configuration/vectors/#dimensionality","title":"dimensionality","text":"<pre><code>dimensionality: int\n</code></pre> <p>Enables truncation of vectors to this dimensionality. This is only useful for models trained to store more important information in earlier dimensions such as Matryoshka Representation Learning (MRL).</p>"},{"location":"embeddings/configuration/vectors/#quantize","title":"quantize","text":"<pre><code>quantize: int|boolean\n</code></pre> <p>Enables scalar vector quantization at the specified precision. Supports 1-bit through 8-bit quantization. Scalar quantization transforms continuous floating point values to discrete unsigned integers. The <code>faiss</code>, <code>pgvector</code>, <code>numpy</code> and <code>torch</code> ANN backends support storing these vectors.</p> <p>This parameter supports booleans for backwards compatability. When set to true/false, this flag sets faiss.quantize.</p> <p>In addition to vector-level quantization, some ANN backends have the ability to quantize vectors at the storage layer. See the ANN configuration options for more.</p>"},{"location":"embeddings/configuration/vectors/#instructions","title":"instructions","text":"<pre><code>instructions:\n    query: prefix for queries\n    data: prefix for indexing\n</code></pre> <p>Instruction-based models use prefixes to modify how embeddings are computed. This is especially useful with asymmetric search, which is when the query and indexed data are of vastly different lengths. In other words, short queries with long documents.</p> <p><code>txtai</code> automatically loads prompts stored in <code>config_sentence_transformers.json</code> except if this parameter is set. For some older models such as E5-base, instructions still need to be provided via this parameter.</p>"},{"location":"embeddings/configuration/vectors/#models","title":"models","text":"<pre><code>models: dict\n</code></pre> <p>Loads and stores vector models in this cache. This is primarily used with subindexes but can be set on any embeddings instance. This prevents the same model from being loaded multiple times when working with multiple embeddings instances.</p>"},{"location":"embeddings/configuration/vectors/#tokenize","title":"tokenize","text":"<pre><code>tokenize: boolean\n</code></pre> <p>Enables string tokenization (defaults to false). This method applies tokenization rules that only work with English language text. It's not recommended for use with recent vector models.</p>"},{"location":"embeddings/configuration/vectors/#vectors_1","title":"vectors","text":"<pre><code>vectors: dict\n</code></pre> <p>Passes these additional parameters to the underlying vector model.</p>"},{"location":"embeddings/configuration/vectors/#muvera","title":"muvera","text":"<pre><code>muvera:\n    repetitions: defaults 20\n    hashes: defaults to 5\n    projection: defaults 16\n</code></pre> <p>Settings to control the size of MUVERA fixed dimensional outputs. Default is 20 * 2^5 * 16 = 10,240 dimensions.</p>"},{"location":"embeddings/configuration/vectors/#trust_remote_code","title":"trust_remote_code","text":"<pre><code>trust_remote_code: boolean\n</code></pre> <p>Parameter for trusting the code from Hugging Face models with custom implementations.</p>"},{"location":"pipeline/","title":"Pipeline","text":"<p>txtai provides a generic pipeline processing framework with the only interface requirement being a <code>__call__</code> method. Pipelines are flexible and process various types of data. Pipelines can wrap machine learning models as well as other processes.</p> <p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/#list-of-pipelines","title":"List of pipelines","text":"<p>The following is a list of the current pipelines available in txtai. All pipelines use default models when otherwise not specified. See the model guide for the current model recommendations. All pipelines are designed to work with local models via the Transformers library.</p> <p>The <code>LLM</code> and <code>RAG</code> pipelines also have integrations for llama.cpp and hosted API models via LiteLLM. The <code>LLM</code> pipeline can be prompted to accomplish many of the same tasks (i.e. summarization, translation, classification).</p> <ul> <li>Audio<ul> <li>AudioMixer</li> <li>AudioStream</li> <li>Microphone</li> <li>TextToAudio</li> <li>TextToSpeech</li> <li>Transcription</li> </ul> </li> <li>Data Processing<ul> <li>FileToHTML</li> <li>HTMLToMarkdown</li> <li>Segmentation</li> <li>Tabular</li> <li>Text extraction</li> <li>Tokenizer</li> </ul> </li> <li>Image<ul> <li>Caption</li> <li>Image Hash</li> <li>Objects</li> </ul> </li> <li>Text<ul> <li>Entity</li> <li>Labeling</li> <li>LLM</li> <li>RAG</li> <li>Reranker</li> <li>Similarity</li> <li>Summary</li> <li>Translation</li> </ul> </li> <li>Training<ul> <li>HF ONNX</li> <li>ML ONNX</li> <li>Trainer</li> </ul> </li> </ul>"},{"location":"pipeline/audio/audiomixer/","title":"Audio Mixer","text":"<p>The Audio Mixer pipeline mixes multiple audio streams into a single stream.</p>"},{"location":"pipeline/audio/audiomixer/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import AudioMixer\n\n# Create and run pipeline\nmixer = AudioMixer()\nmixer(((audio1, rate1), (audio2, rate2)))\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Generative Audio Storytelling with generative audio workflows"},{"location":"pipeline/audio/audiomixer/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/audio/audiomixer/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\naudiomixer:\n\n# Run pipeline with workflow\nworkflow:\n  audiomixer:\n    tasks:\n      - action: audiomixer\n</code></pre>"},{"location":"pipeline/audio/audiomixer/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"audiomixer\", [[[audio1, rate1], [audio2, rate2]]]))\n</code></pre>"},{"location":"pipeline/audio/audiomixer/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"audiomixer\", \"elements\":[[[audio1, rate1], [audio2, rate2]]]}'\n</code></pre>"},{"location":"pipeline/audio/audiomixer/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/audio/audiomixer/#txtai.pipeline.AudioMixer.__init__","title":"<code>__init__(rate=None)</code>","text":"<p>Creates an AudioMixer pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>rate</code> <p>optional target sample rate, otherwise uses input target rate with each audio segment</p> <code>None</code> Source code in <code>txtai/pipeline/audio/audiomixer.py</code> <pre><code>def __init__(self, rate=None):\n    \"\"\"\n    Creates an AudioMixer pipeline.\n\n    Args:\n        rate: optional target sample rate, otherwise uses input target rate with each audio segment\n    \"\"\"\n\n    if not SCIPY:\n        raise ImportError('AudioMixer pipeline is not available - install \"pipeline\" extra to enable.')\n\n    # Target sample rate\n    self.rate = rate\n</code></pre>"},{"location":"pipeline/audio/audiomixer/#txtai.pipeline.AudioMixer.__call__","title":"<code>__call__(segment, scale1=1, scale2=1)</code>","text":"<p>Mixes multiple audio streams into a single stream.</p> <p>Parameters:</p> Name Type Description Default <code>segment</code> <p>((audio1, sample rate), (audio2, sample rate))|list</p> required <code>scale1</code> <p>optional scaling factor for segment1</p> <code>1</code> <code>scale2</code> <p>optional scaling factor for segment2</p> <code>1</code> <p>Returns:</p> Type Description <p>list of (audio, sample rate)</p> Source code in <code>txtai/pipeline/audio/audiomixer.py</code> <pre><code>def __call__(self, segment, scale1=1, scale2=1):\n    \"\"\"\n    Mixes multiple audio streams into a single stream.\n\n    Args:\n        segment: ((audio1, sample rate), (audio2, sample rate))|list\n        scale1: optional scaling factor for segment1\n        scale2: optional scaling factor for segment2\n\n    Returns:\n        list of (audio, sample rate)\n    \"\"\"\n\n    # Convert single element to list\n    segments = [segment] if isinstance(segment, tuple) else segment\n\n    results = []\n    for segment1, segment2 in segments:\n        audio1, rate1 = segment1\n        audio2, rate2 = segment2\n\n        # Resample audio, as necessary\n        target = self.rate if self.rate else rate1\n        audio1 = Signal.resample(audio1, rate1, target)\n        audio2 = Signal.resample(audio2, rate2, target)\n\n        # Mix audio into single segment\n        results.append((Signal.mix(audio1, audio2, scale1, scale2), target))\n\n    # Return single element if single element passed in\n    return results[0] if isinstance(segment, tuple) else results\n</code></pre>"},{"location":"pipeline/audio/audiostream/","title":"Audio Stream","text":"<p>The Audio Stream pipeline is a threaded pipeline that plays audio segments. This pipeline is designed to run on local machines given that it requires access to write to an output device.</p>"},{"location":"pipeline/audio/audiostream/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import AudioStream\n\n# Create and run pipeline\naudio = AudioStream()\naudio(data)\n</code></pre> <p>This pipeline may require additional system dependencies. See this section for more.</p> <p>See the link below for a more detailed example.</p> Notebook Description Speech to Speech RAG \u25b6\ufe0f Full cycle speech to speech workflow with RAG"},{"location":"pipeline/audio/audiostream/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/audio/audiostream/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\naudiostream:\n\n# Run pipeline with workflow\nworkflow:\n  audiostream:\n    tasks:\n      - action: audiostream\n</code></pre>"},{"location":"pipeline/audio/audiostream/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"audiostream\", [[\"numpy data\", \"sample rate\"]]))\n</code></pre>"},{"location":"pipeline/audio/audiostream/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"audiostream\", \"elements\":[[\"numpy data\", \"sample rate\"]]}'\n</code></pre>"},{"location":"pipeline/audio/audiostream/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/audio/audiostream/#txtai.pipeline.AudioStream.__init__","title":"<code>__init__(rate=None)</code>","text":"<p>Creates an AudioStream pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>rate</code> <p>optional target sample rate, otherwise uses input target rate with each audio segment</p> <code>None</code> Source code in <code>txtai/pipeline/audio/audiostream.py</code> <pre><code>def __init__(self, rate=None):\n    \"\"\"\n    Creates an AudioStream pipeline.\n\n    Args:\n        rate: optional target sample rate, otherwise uses input target rate with each audio segment\n    \"\"\"\n\n    if not AUDIOSTREAM:\n        raise ImportError(\n            (\n                'AudioStream pipeline is not available - install \"pipeline\" extra to enable. '\n                \"Also check that the portaudio system library is available.\"\n            )\n        )\n\n    # Target sample rate\n    self.rate = rate\n\n    self.queue = Queue()\n    self.thread = Thread(target=self.play)\n    self.thread.start()\n</code></pre>"},{"location":"pipeline/audio/audiostream/#txtai.pipeline.AudioStream.__call__","title":"<code>__call__(segment)</code>","text":"<p>Queues audio segments for the audio player.</p> <p>Parameters:</p> Name Type Description Default <code>segment</code> <p>(audio, sample rate)|list</p> required <p>Returns:</p> Type Description <p>segment</p> Source code in <code>txtai/pipeline/audio/audiostream.py</code> <pre><code>def __call__(self, segment):\n    \"\"\"\n    Queues audio segments for the audio player.\n\n    Args:\n        segment: (audio, sample rate)|list\n\n    Returns:\n        segment\n    \"\"\"\n\n    # Convert single element to list\n    segments = [segment] if isinstance(segment, tuple) else segment\n\n    for x in segments:\n        self.queue.put(x)\n\n    # Return single element if single element passed in\n    return segments[0] if isinstance(segment, tuple) else segments\n</code></pre>"},{"location":"pipeline/audio/microphone/","title":"Microphone","text":"<p>The Microphone pipeline reads input speech from a microphone device. This pipeline is designed to run on local machines given that it requires access to read from an input device.</p>"},{"location":"pipeline/audio/microphone/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Microphone\n\n# Create and run pipeline\nmicrophone = Microphone()\nmicrophone()\n</code></pre> <p>This pipeline may require additional system dependencies. See this section for more.</p> <p>See the link below for a more detailed example.</p> Notebook Description Speech to Speech RAG \u25b6\ufe0f Full cycle speech to speech workflow with RAG"},{"location":"pipeline/audio/microphone/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/audio/microphone/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nmicrophone:\n\n# Run pipeline with workflow\nworkflow:\n  microphone:\n    tasks:\n      - action: microphone\n</code></pre>"},{"location":"pipeline/audio/microphone/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"microphone\", [\"1\"]))\n</code></pre>"},{"location":"pipeline/audio/microphone/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"microphone\", \"elements\":[\"1\"]}'\n</code></pre>"},{"location":"pipeline/audio/microphone/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/audio/microphone/#txtai.pipeline.Microphone.__init__","title":"<code>__init__(rate=16000, vadmode=3, vadframe=20, vadthreshold=0.6, voicestart=300, voiceend=3400, active=5, pause=8)</code>","text":"<p>Creates a new Microphone pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>rate</code> <p>sample rate to record audio in, defaults to 16000 (16 kHz)</p> <code>16000</code> <code>vadmode</code> <p>aggressiveness of the voice activity detector (1 - 3), defaults to 3, which is the most aggressive filter</p> <code>3</code> <code>vadframe</code> <p>voice activity detector frame size in ms, defaults to 20</p> <code>20</code> <code>vadthreshold</code> <p>percentage of frames (0.0 - 1.0) that must be voice to be considered speech, defaults to 0.6</p> <code>0.6</code> <code>voicestart</code> <p>starting frequency to use for voice filtering, defaults to 300</p> <code>300</code> <code>voiceend</code> <p>ending frequency to use for voice filtering, defaults to 3400</p> <code>3400</code> <code>active</code> <p>minimum number of active speech chunks to require before considering this speech, defaults to 5</p> <code>5</code> <code>pause</code> <p>number of non-speech chunks to keep before considering speech complete, defaults to 8</p> <code>8</code> Source code in <code>txtai/pipeline/audio/microphone.py</code> <pre><code>def __init__(self, rate=16000, vadmode=3, vadframe=20, vadthreshold=0.6, voicestart=300, voiceend=3400, active=5, pause=8):\n    \"\"\"\n    Creates a new Microphone pipeline.\n\n    Args:\n        rate: sample rate to record audio in, defaults to 16000 (16 kHz)\n        vadmode: aggressiveness of the voice activity detector (1 - 3), defaults to 3, which is the most aggressive filter\n        vadframe: voice activity detector frame size in ms, defaults to 20\n        vadthreshold: percentage of frames (0.0 - 1.0) that must be voice to be considered speech, defaults to 0.6\n        voicestart: starting frequency to use for voice filtering, defaults to 300\n        voiceend: ending frequency to use for voice filtering, defaults to 3400\n        active: minimum number of active speech chunks to require before considering this speech, defaults to 5\n        pause: number of non-speech chunks to keep before considering speech complete, defaults to 8\n    \"\"\"\n\n    if not MICROPHONE:\n        raise ImportError(\n            (\n                'Microphone pipeline is not available - install \"pipeline\" extra to enable. '\n                \"Also check that the portaudio system library is available.\"\n            )\n        )\n\n    # Sample rate\n    self.rate = rate\n\n    # Voice activity detector\n    self.vad = webrtcvad.Vad(vadmode)\n    self.vadframe = vadframe\n    self.vadthreshold = vadthreshold\n\n    # Voice spectrum\n    self.voicestart = voicestart\n    self.voiceend = voiceend\n\n    # Audio chunks counts\n    self.active = active\n    self.pause = pause\n</code></pre>"},{"location":"pipeline/audio/microphone/#txtai.pipeline.Microphone.__call__","title":"<code>__call__(device=None)</code>","text":"<p>Reads audio from an input device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <p>optional input device id, otherwise uses system default</p> <code>None</code> <p>Returns:</p> Type Description <p>list of (audio, sample rate)</p> Source code in <code>txtai/pipeline/audio/microphone.py</code> <pre><code>def __call__(self, device=None):\n    \"\"\"\n    Reads audio from an input device.\n\n    Args:\n        device: optional input device id, otherwise uses system default\n\n    Returns:\n        list of (audio, sample rate)\n    \"\"\"\n\n    # Listen for audio\n    audio = self.listen(device[0] if isinstance(device, list) else device)\n\n    # Return single element if single element passed in\n    return (audio, self.rate) if device is None or not isinstance(device, list) else [(audio, self.rate)]\n</code></pre>"},{"location":"pipeline/audio/texttoaudio/","title":"Text To Audio","text":"<p>The Text To Audio pipeline generates audio from text.</p>"},{"location":"pipeline/audio/texttoaudio/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import TextToAudio\n\n# Create and run pipeline\ntta = TextToAudio()\ntta(\"Describe the audio to generate here\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Generative Audio Storytelling with generative audio workflows"},{"location":"pipeline/audio/texttoaudio/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/audio/texttoaudio/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ntexttoaudio:\n\n# Run pipeline with workflow\nworkflow:\n  tta:\n    tasks:\n      - action: texttoaudio\n</code></pre>"},{"location":"pipeline/audio/texttoaudio/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"tta\", [\"Describe the audio to generate here\"]))\n</code></pre>"},{"location":"pipeline/audio/texttoaudio/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"tta\", \"elements\":[\"Describe the audio to generate here\"]}'\n</code></pre>"},{"location":"pipeline/audio/texttoaudio/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/audio/texttoaudio/#txtai.pipeline.TextToAudio.__init__","title":"<code>__init__(path=None, quantize=False, gpu=True, model=None, rate=None, **kwargs)</code>","text":"Source code in <code>txtai/pipeline/audio/texttoaudio.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None, rate=None, **kwargs):\n    if not SCIPY:\n        raise ImportError('TextToAudio pipeline is not available - install \"pipeline\" extra to enable.')\n\n    # Call parent constructor\n    super().__init__(\"text-to-audio\", path, quantize, gpu, model, **kwargs)\n\n    # Target sample rate, defaults to model sample rate\n    self.rate = rate\n</code></pre>"},{"location":"pipeline/audio/texttoaudio/#txtai.pipeline.TextToAudio.__call__","title":"<code>__call__(text, maxlength=512)</code>","text":"<p>Generates audio from text.</p> <p>This method supports text as a string or a list. If the input is a string, the return type is a single audio output. If text is a list, the return type is a list.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <code>maxlength</code> <p>maximum audio length to generate</p> <code>512</code> <p>Returns:</p> Type Description <p>list of (audio, sample rate)</p> Source code in <code>txtai/pipeline/audio/texttoaudio.py</code> <pre><code>def __call__(self, text, maxlength=512):\n    \"\"\"\n    Generates audio from text.\n\n    This method supports text as a string or a list. If the input is a string,\n    the return type is a single audio output. If text is a list, the return type is a list.\n\n    Args:\n        text: text|list\n        maxlength: maximum audio length to generate\n\n    Returns:\n        list of (audio, sample rate)\n    \"\"\"\n\n    # Format inputs\n    texts = [text] if isinstance(text, str) else text\n\n    # Run pipeline\n    results = [self.convert(x) for x in self.pipeline(texts, forward_params={\"max_new_tokens\": maxlength})]\n\n    # Extract results\n    return results[0] if isinstance(text, str) else results\n</code></pre>"},{"location":"pipeline/audio/texttospeech/","title":"Text To Speech","text":"<p>The Text To Speech pipeline generates speech from text.</p>"},{"location":"pipeline/audio/texttospeech/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import TextToSpeech\n\n# Create and run pipeline with default model\ntts = TextToSpeech()\ntts(\"Say something here\")\n\n# Stream audio - incrementally generates snippets of audio\nyield from tts(\n  \"Say something here. And say something else.\".split(),\n  stream=True\n)\n\n# Generate audio using a speaker id\ntts = TextToSpeech(\"neuml/vctk-vits-onnx\")\ntts(\"Say something here\", speaker=15)\n\n# Generate audio using speaker embeddings\ntts = TextToSpeech(\"neuml/txtai-speecht5-onnx\")\ntts(\"Say something here\", speaker=np.array(...))\n</code></pre> <p>See the links below for a more detailed example.</p> Notebook Description Text to speech generation Generate speech from text Speech to Speech RAG \u25b6\ufe0f Full cycle speech to speech workflow with RAG Generative Audio Storytelling with generative audio workflows <p>This pipeline is backed by ONNX models from the Hugging Face Hub. The following models are currently available.</p> <ul> <li>kokoro-base-onnx | fp16 | int8</li> <li>ljspeech-jets-onnx</li> <li>ljspeech-vits-onnx</li> <li>vctk-vits-onnx</li> <li>txtai-speecht5-onnx</li> </ul>"},{"location":"pipeline/audio/texttospeech/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/audio/texttospeech/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ntexttospeech:\n\n# Run pipeline with workflow\nworkflow:\n  tts:\n    tasks:\n      - action: texttospeech\n</code></pre>"},{"location":"pipeline/audio/texttospeech/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"tts\", [\"Say something here\"]))\n</code></pre>"},{"location":"pipeline/audio/texttospeech/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"tts\", \"elements\":[\"Say something here\"]}'\n</code></pre>"},{"location":"pipeline/audio/texttospeech/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/audio/texttospeech/#txtai.pipeline.TextToSpeech.__init__","title":"<code>__init__(path=None, maxtokens=512, rate=22050)</code>","text":"<p>Creates a new TextToSpeech pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>optional model path</p> <code>None</code> <code>maxtokens</code> <p>maximum number of tokens model can process, defaults to 512</p> <code>512</code> <code>rate</code> <p>target sample rate, defaults to 22050</p> <code>22050</code> Source code in <code>txtai/pipeline/audio/texttospeech.py</code> <pre><code>def __init__(self, path=None, maxtokens=512, rate=22050):\n    \"\"\"\n    Creates a new TextToSpeech pipeline.\n\n    Args:\n        path: optional model path\n        maxtokens: maximum number of tokens model can process, defaults to 512\n        rate: target sample rate, defaults to 22050\n    \"\"\"\n\n    if not TTS:\n        raise ImportError('TextToSpeech pipeline is not available - install \"pipeline\" extra to enable')\n\n    # Default path\n    path = path if path else \"neuml/ljspeech-jets-onnx\"\n\n    # Target sample rate\n    self.rate = rate\n\n    # Load target tts pipeline\n    self.pipeline = None\n    if self.hasfile(path, \"model.onnx\") and self.hasfile(path, \"config.yaml\"):\n        self.pipeline = ESPnet(path, maxtokens, self.providers())\n    elif self.hasfile(path, \"model.onnx\") and self.hasfile(path, \"voices.json\"):\n        self.pipeline = Kokoro(path, maxtokens, self.providers())\n    else:\n        self.pipeline = SpeechT5(path, maxtokens, self.providers())\n</code></pre>"},{"location":"pipeline/audio/texttospeech/#txtai.pipeline.TextToSpeech.__call__","title":"<code>__call__(text, stream=False, speaker=1, encoding=None, **kwargs)</code>","text":"<p>Generates speech from text. Text longer than maxtokens will be batched and returned as a single waveform per text input.</p> <p>This method supports text as a string or a list. If the input is a string, the return type is audio. If text is a list, the return type is a list.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <code>stream</code> <p>stream response if True, defaults to False</p> <code>False</code> <code>speaker</code> <p>speaker id, defaults to 1</p> <code>1</code> <code>encoding</code> <p>optional audio encoding format</p> <code>None</code> <code>kwargs</code> <p>additional keyword args</p> <code>{}</code> <p>Returns:</p> Type Description <p>list of (audio, sample rate) or list of audio depending on encoding parameter</p> Source code in <code>txtai/pipeline/audio/texttospeech.py</code> <pre><code>def __call__(self, text, stream=False, speaker=1, encoding=None, **kwargs):\n    \"\"\"\n    Generates speech from text. Text longer than maxtokens will be batched and returned\n    as a single waveform per text input.\n\n    This method supports text as a string or a list. If the input is a string,\n    the return type is audio. If text is a list, the return type is a list.\n\n    Args:\n        text: text|list\n        stream: stream response if True, defaults to False\n        speaker: speaker id, defaults to 1\n        encoding: optional audio encoding format\n        kwargs: additional keyword args\n\n    Returns:\n        list of (audio, sample rate) or list of audio depending on encoding parameter\n    \"\"\"\n\n    # Convert results to a list if necessary\n    texts = [text] if isinstance(text, str) else text\n\n    # Streaming response\n    if stream:\n        return self.stream(texts, speaker, encoding)\n\n    # Transform text to speech\n    results = [self.execute(x, speaker, encoding, **kwargs) for x in texts]\n\n    # Return results\n    return results[0] if isinstance(text, str) else results\n</code></pre>"},{"location":"pipeline/audio/transcription/","title":"Transcription","text":"<p>The Transcription pipeline converts speech in audio files to text.</p>"},{"location":"pipeline/audio/transcription/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Transcription\n\n# Create and run pipeline\ntranscribe = Transcription()\ntranscribe(\"path to wav file\")\n</code></pre> <p>This pipeline may require additional system dependencies. See this section for more.</p> <p>See the links below for a more detailed example.</p> Notebook Description Transcribe audio to text Convert audio files to text Speech to Speech RAG \u25b6\ufe0f Full cycle speech to speech workflow with RAG"},{"location":"pipeline/audio/transcription/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/audio/transcription/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ntranscription:\n\n# Run pipeline with workflow\nworkflow:\n  transcribe:\n    tasks:\n      - action: transcription\n</code></pre>"},{"location":"pipeline/audio/transcription/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"transcribe\", [\"path to wav file\"]))\n</code></pre>"},{"location":"pipeline/audio/transcription/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"transcribe\", \"elements\":[\"path to wav file\"]}'\n</code></pre>"},{"location":"pipeline/audio/transcription/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/audio/transcription/#txtai.pipeline.Transcription.__init__","title":"<code>__init__(path=None, quantize=False, gpu=True, model=None, **kwargs)</code>","text":"Source code in <code>txtai/pipeline/audio/transcription.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):\n    if not TRANSCRIPTION:\n        raise ImportError(\n            'Transcription pipeline is not available - install \"pipeline\" extra to enable. Also check that libsndfile is available.'\n        )\n\n    # Call parent constructor\n    super().__init__(\"automatic-speech-recognition\", path, quantize, gpu, model, **kwargs)\n</code></pre>"},{"location":"pipeline/audio/transcription/#txtai.pipeline.Transcription.__call__","title":"<code>__call__(audio, rate=None, chunk=10, join=True, **kwargs)</code>","text":"<p>Transcribes audio files or data to text.</p> <p>This method supports a single audio element or a list of audio. If the input is audio, the return type is a string. If text is a list, a list of strings is returned</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <p>audio|list</p> required <code>rate</code> <p>sample rate, only required with raw audio data</p> <code>None</code> <code>chunk</code> <p>process audio in chunk second sized segments</p> <code>10</code> <code>join</code> <p>if True (default), combine each chunk back together into a single text output.   When False, chunks are returned as a list of dicts, each having raw associated audio and   sample rate in addition to text</p> <code>True</code> <code>kwargs</code> <p>generate keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <p>list of transcribed text</p> Source code in <code>txtai/pipeline/audio/transcription.py</code> <pre><code>def __call__(self, audio, rate=None, chunk=10, join=True, **kwargs):\n    \"\"\"\n    Transcribes audio files or data to text.\n\n    This method supports a single audio element or a list of audio. If the input is audio, the return\n    type is a string. If text is a list, a list of strings is returned\n\n    Args:\n        audio: audio|list\n        rate: sample rate, only required with raw audio data\n        chunk: process audio in chunk second sized segments\n        join: if True (default), combine each chunk back together into a single text output.\n              When False, chunks are returned as a list of dicts, each having raw associated audio and\n              sample rate in addition to text\n        kwargs: generate keyword arguments\n\n    Returns:\n        list of transcribed text\n    \"\"\"\n\n    # Convert single element to list\n    values = [audio] if self.isaudio(audio) else audio\n\n    # Read input audio\n    speech = self.read(values, rate)\n\n    # Apply transformation rules and store results\n    results = self.batchprocess(speech, chunk, **kwargs) if chunk and not join else self.process(speech, chunk, **kwargs)\n\n    # Return single element if single element passed in\n    return results[0] if self.isaudio(audio) else results\n</code></pre>"},{"location":"pipeline/data/filetohtml/","title":"File To HTML","text":"<p>The File To HTML pipeline transforms files to HTML. It supports the following text extraction backends.</p>"},{"location":"pipeline/data/filetohtml/#apache-tika","title":"Apache Tika","text":"<p>Apache Tika detects and extracts metadata and text from over a thousand different file types. See this link for a list of supported document formats.</p> <p>Apache Tika requires Java to be installed. An alternative to that is starting a separate Apache Tika service via this Docker Image and setting these environment variables.</p>"},{"location":"pipeline/data/filetohtml/#docling","title":"Docling","text":"<p>Docling parses documents and exports them to the desired format with ease and speed. This is a library that has rapidly gained popularity starting in late 2024. Docling excels in parsing formatting elements from PDFs (tables, sections etc).</p> <p>See this link for a list of supported document formats.</p>"},{"location":"pipeline/data/filetohtml/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import FileToHTML\n\n# Create and run pipeline\nhtml = FileToHTML()\nhtml(\"/path/to/file\")\n</code></pre>"},{"location":"pipeline/data/filetohtml/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/data/filetohtml/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nfiletohtml:\n\n# Run pipeline with workflow\nworkflow:\n  html:\n    tasks:\n      - action: filetohtml\n</code></pre>"},{"location":"pipeline/data/filetohtml/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"html\", [\"/path/to/file\"]))\n</code></pre>"},{"location":"pipeline/data/filetohtml/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"html\", \"elements\":[\"/path/to/file\"]}'\n</code></pre>"},{"location":"pipeline/data/filetohtml/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/data/filetohtml/#txtai.pipeline.FileToHTML.__init__","title":"<code>__init__(backend='available')</code>","text":"<p>Creates a new File to HTML pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <p>backend to use to extract content, supports \"tika\", \"docling\" or \"available\" (default) which finds the first available</p> <code>'available'</code> Source code in <code>txtai/pipeline/data/filetohtml.py</code> <pre><code>def __init__(self, backend=\"available\"):\n    \"\"\"\n    Creates a new File to HTML pipeline.\n\n    Args:\n        backend: backend to use to extract content, supports \"tika\", \"docling\" or \"available\" (default) which finds the first available\n    \"\"\"\n\n    # Lowercase backend parameter\n    backend = backend.lower() if backend else None\n\n    # Check for available backend\n    if backend == \"available\":\n        backend = \"tika\" if Tika.available() else \"docling\" if Docling.available() else None\n\n    # Create backend instance\n    self.backend = Tika() if backend == \"tika\" else Docling() if backend == \"docling\" else None\n</code></pre>"},{"location":"pipeline/data/filetohtml/#txtai.pipeline.FileToHTML.__call__","title":"<code>__call__(path)</code>","text":"<p>Converts file at path to HTML. Returns None if no backend is available.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>input file path</p> required <p>Returns:</p> Type Description <p>html if a backend is available, otherwise returns None</p> Source code in <code>txtai/pipeline/data/filetohtml.py</code> <pre><code>def __call__(self, path):\n    \"\"\"\n    Converts file at path to HTML. Returns None if no backend is available.\n\n    Args:\n        path: input file path\n\n    Returns:\n        html if a backend is available, otherwise returns None\n    \"\"\"\n\n    return self.backend(path) if self.backend else None\n</code></pre>"},{"location":"pipeline/data/htmltomd/","title":"HTML To Markdown","text":"<p>The HTML To Markdown pipeline transforms HTML to Markdown.</p> <p>Markdown formatting is applied for headings, blockquotes, lists, code, tables and text. Visual formatting is also included (bold, italic etc).</p> <p>This pipeline searches for the best node that has relevant text, often found with an <code>article</code>, <code>main</code> or <code>body</code> tag.</p> <p>The HTML to Markdown pipeline requires the BeautifulSoup4 library to be installed.</p>"},{"location":"pipeline/data/htmltomd/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import HTMLToMarkdown\n\n# Create and run pipeline\nmd = HTMLToMarkdown()\nmd(\"&lt;html&gt;&lt;body&gt;This is a test&lt;/body&gt;&lt;/html&gt;\")\n</code></pre>"},{"location":"pipeline/data/htmltomd/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/data/htmltomd/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nhtmltomarkdown:\n\n# Run pipeline with workflow\nworkflow:\n  markdown:\n    tasks:\n      - action: htmltomarkdown\n</code></pre>"},{"location":"pipeline/data/htmltomd/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"markdown\", [\"&lt;html&gt;&lt;body&gt;This is a test&lt;/body&gt;&lt;/html&gt;\"]))\n</code></pre>"},{"location":"pipeline/data/htmltomd/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"markdown\", \"elements\":[\"&lt;html&gt;&lt;body&gt;This is a test&lt;/body&gt;&lt;/html&gt;\"]}'\n</code></pre>"},{"location":"pipeline/data/htmltomd/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/data/htmltomd/#txtai.pipeline.HTMLToMarkdown.__init__","title":"<code>__init__(paragraphs=False, sections=False)</code>","text":"<p>Create a new Extract instance.</p> <p>Parameters:</p> Name Type Description Default <code>paragraphs</code> <p>True if paragraph parsing enabled, False otherwise</p> <code>False</code> <code>sections</code> <p>True if section parsing enabled, False otherwise</p> <code>False</code> Source code in <code>txtai/pipeline/data/htmltomd.py</code> <pre><code>def __init__(self, paragraphs=False, sections=False):\n    \"\"\"\n    Create a new Extract instance.\n\n    Args:\n        paragraphs: True if paragraph parsing enabled, False otherwise\n        sections: True if section parsing enabled, False otherwise\n    \"\"\"\n\n    if not SOUP:\n        raise ImportError('HTMLToMarkdown pipeline is not available - install \"pipeline\" extra to enable')\n\n    self.paragraphs = paragraphs\n    self.sections = sections\n</code></pre>"},{"location":"pipeline/data/htmltomd/#txtai.pipeline.HTMLToMarkdown.__call__","title":"<code>__call__(html)</code>","text":"<p>Transforms input HTML into Markdown formatted text.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <p>input html</p> required <p>Returns:</p> Type Description <p>markdown formatted text</p> Source code in <code>txtai/pipeline/data/htmltomd.py</code> <pre><code>def __call__(self, html):\n    \"\"\"\n    Transforms input HTML into Markdown formatted text.\n\n    Args:\n        html: input html\n\n    Returns:\n        markdown formatted text\n    \"\"\"\n\n    # HTML Parser\n    soup = BeautifulSoup(html, features=\"html.parser\")\n\n    # Ignore script and style tags\n    for script in soup.find_all([\"script\", \"style\"]):\n        script.decompose()\n\n    # Check for article sections\n    article = next((x for x in [\"article\", \"main\"] if soup.find(x)), None)\n\n    # Extract text from each section element\n    nodes = []\n    for node in soup.find_all(article if article else \"body\"):\n        # Skip article sections without at least 1 paragraph\n        if not article or node.find(\"p\"):\n            nodes.append(self.process(node, article))\n\n    # Return extracted text, fallback to default text extraction if no nodes found\n    return \"\\n\".join(self.metadata(soup) + nodes) if nodes else self.default(soup)\n</code></pre>"},{"location":"pipeline/data/segmentation/","title":"Segmentation","text":"<p>The Segmentation pipeline segments text into semantic units.</p>"},{"location":"pipeline/data/segmentation/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Segmentation\n\n# Create and run pipeline\nsegment = Segmentation(sentences=True)\nsegment(\"This is a test. And another test.\")\n\n# Load third-party chunkers\nsegment = Segmentation(chunker=\"semantic\")\nsegment(\"This is a test. And another test.\")\n</code></pre> <p>The Segmentation pipeline supports segmenting <code>sentences</code>, <code>lines</code>, <code>paragraphs</code> and <code>sections</code> using a rules-based approach. Each of these modes can be set when creating the pipeline. Third-party chunkers are also supported via the <code>chunker</code> parameter.</p>"},{"location":"pipeline/data/segmentation/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/data/segmentation/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nsegmentation:\n  sentences: true\n\n# Run pipeline with workflow\nworkflow:\n  segment:\n    tasks:\n      - action: segmentation\n</code></pre>"},{"location":"pipeline/data/segmentation/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"segment\", [\"This is a test. And another test.\"]))\n</code></pre>"},{"location":"pipeline/data/segmentation/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"segment\", \"elements\":[\"This is a test. And another test.\"]}'\n</code></pre>"},{"location":"pipeline/data/segmentation/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/data/segmentation/#txtai.pipeline.Segmentation.__init__","title":"<code>__init__(sentences=False, lines=False, paragraphs=False, minlength=None, join=False, sections=False, cleantext=True, chunker=None, tuples=False, **kwargs)</code>","text":"<p>Creates a new Segmentation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>sentences</code> <p>tokenize text into sentences if True, defaults to False</p> <code>False</code> <code>lines</code> <p>tokenizes text into lines if True, defaults to False</p> <code>False</code> <code>paragraphs</code> <p>tokenizes text into paragraphs if True, defaults to False</p> <code>False</code> <code>minlength</code> <p>require at least minlength characters per text element, defaults to None</p> <code>None</code> <code>join</code> <p>joins tokenized sections back together if True, defaults to False</p> <code>False</code> <code>sections</code> <p>tokenizes text into sections if True, defaults to False. Splits using section or page breaks, depending on what's available</p> <code>False</code> <code>cleantext</code> <p>apply text cleaning rules, defaults to True</p> <code>True</code> <code>chunker</code> <p>creates a third-party chunker to tokenize text if set, defaults to None</p> <code>None</code> <code>tuples</code> <p>return (input, output) tuples, defaults to False</p> <code>False</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/pipeline/data/segmentation.py</code> <pre><code>def __init__(\n    self,\n    sentences=False,\n    lines=False,\n    paragraphs=False,\n    minlength=None,\n    join=False,\n    sections=False,\n    cleantext=True,\n    chunker=None,\n    tuples=False,\n    **kwargs,\n):\n    \"\"\"\n    Creates a new Segmentation pipeline.\n\n    Args:\n        sentences: tokenize text into sentences if True, defaults to False\n        lines: tokenizes text into lines if True, defaults to False\n        paragraphs: tokenizes text into paragraphs if True, defaults to False\n        minlength: require at least minlength characters per text element, defaults to None\n        join: joins tokenized sections back together if True, defaults to False\n        sections: tokenizes text into sections if True, defaults to False. Splits using section or page breaks, depending on what's available\n        cleantext: apply text cleaning rules, defaults to True\n        chunker: creates a third-party chunker to tokenize text if set, defaults to None\n        tuples: return (input, output) tuples, defaults to False\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    if not NLTK and sentences:\n        raise ImportError('NLTK is not available - install \"pipeline\" extra to enable')\n\n    if not CHONKIE and chunker:\n        raise ImportError('Chonkie is not available - install \"pipeline\" extra to enable')\n\n    self.sentences = sentences\n    self.lines = lines\n    self.paragraphs = paragraphs\n    self.sections = sections\n    self.minlength = minlength\n    self.join = join\n    self.cleantext = cleantext\n\n    # Create a third-party chunker, if applicable\n    self.chunker = self.createchunker(chunker, **kwargs) if chunker else None\n\n    # Return (input, output) tuples as output\n    self.tuples = tuples\n</code></pre>"},{"location":"pipeline/data/segmentation/#txtai.pipeline.Segmentation.__call__","title":"<code>__call__(text)</code>","text":"<p>Segments text into semantic units.</p> <p>This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <p>Returns:</p> Type Description <p>segmented text</p> Source code in <code>txtai/pipeline/data/segmentation.py</code> <pre><code>def __call__(self, text):\n    \"\"\"\n    Segments text into semantic units.\n\n    This method supports text as a string or a list. If the input is a string, the return\n    type is text|list. If text is a list, a list of returned, this could be a\n    list of text or a list of lists depending on the tokenization strategy.\n\n    Args:\n        text: text|list\n\n    Returns:\n        segmented text\n    \"\"\"\n\n    # Get inputs\n    texts = [text] if not isinstance(text, list) else text\n\n    # Extract text for each input file\n    results = []\n    for value in texts:\n        # Get text\n        result = self.text(value)\n\n        # Parse and add extracted results\n        result = self.parse(result)\n\n        # Wrap as tuple\n        if self.tuples:\n            result = [(value, x) for x in result] if isinstance(result, list) else (value, result)\n\n        results.append(result)\n\n    return results[0] if isinstance(text, str) else results\n</code></pre>"},{"location":"pipeline/data/tabular/","title":"Tabular","text":"<p>The Tabular pipeline splits tabular data into rows and columns. The tabular pipeline is most useful in creating (id, text, tag) tuples to load into Embedding indexes. </p>"},{"location":"pipeline/data/tabular/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Tabular\n\n# Create and run pipeline\ntabular = Tabular(\"id\", [\"text\"])\ntabular(\"path to csv file\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Transform tabular data with composable workflows Transform, index and search tabular data"},{"location":"pipeline/data/tabular/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/data/tabular/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ntabular:\n    idcolumn: id\n    textcolumns:\n      - text\n\n# Run pipeline with workflow\nworkflow:\n  tabular:\n    tasks:\n      - action: tabular\n</code></pre>"},{"location":"pipeline/data/tabular/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"tabular\", [\"path to csv file\"]))\n</code></pre>"},{"location":"pipeline/data/tabular/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"tabular\", \"elements\":[\"path to csv file\"]}'\n</code></pre>"},{"location":"pipeline/data/tabular/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/data/tabular/#txtai.pipeline.Tabular.__init__","title":"<code>__init__(idcolumn=None, textcolumns=None, content=False)</code>","text":"<p>Creates a new Tabular pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>idcolumn</code> <p>column name to use for row id</p> <code>None</code> <code>textcolumns</code> <p>list of columns to combine as a text field</p> <code>None</code> <code>content</code> <p>if True, a dict per row is generated with all fields. If content is a list, a subset of fields      is included in the generated rows.</p> <code>False</code> Source code in <code>txtai/pipeline/data/tabular.py</code> <pre><code>def __init__(self, idcolumn=None, textcolumns=None, content=False):\n    \"\"\"\n    Creates a new Tabular pipeline.\n\n    Args:\n        idcolumn: column name to use for row id\n        textcolumns: list of columns to combine as a text field\n        content: if True, a dict per row is generated with all fields. If content is a list, a subset of fields\n                 is included in the generated rows.\n    \"\"\"\n\n    if not PANDAS:\n        raise ImportError('Tabular pipeline is not available - install \"pipeline\" extra to enable')\n\n    self.idcolumn = idcolumn\n    self.textcolumns = textcolumns\n    self.content = content\n</code></pre>"},{"location":"pipeline/data/tabular/#txtai.pipeline.Tabular.__call__","title":"<code>__call__(data)</code>","text":"<p>Splits data into rows and columns.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>input data</p> required <p>Returns:</p> Type Description <p>list of (id, text, tag)</p> Source code in <code>txtai/pipeline/data/tabular.py</code> <pre><code>def __call__(self, data):\n    \"\"\"\n    Splits data into rows and columns.\n\n    Args:\n        data: input data\n\n    Returns:\n        list of (id, text, tag)\n    \"\"\"\n\n    items = [data] if not isinstance(data, list) else data\n\n    # Combine all rows into single return element\n    results = []\n    dicts = []\n\n    for item in items:\n        # File path\n        if isinstance(item, str):\n            _, extension = os.path.splitext(item)\n            extension = extension.replace(\".\", \"\").lower()\n\n            if extension == \"csv\":\n                df = pd.read_csv(item)\n\n            results.append(self.process(df))\n\n        # Dict\n        if isinstance(item, dict):\n            dicts.append(item)\n\n        # List of dicts\n        elif isinstance(item, list):\n            df = pd.DataFrame(item)\n            results.append(self.process(df))\n\n    if dicts:\n        df = pd.DataFrame(dicts)\n        results.extend(self.process(df))\n\n    return results[0] if not isinstance(data, list) else results\n</code></pre>"},{"location":"pipeline/data/textractor/","title":"Textractor","text":"<p>The Textractor pipeline extracts and splits text from documents. This pipeline extends the Segmentation pipeline.</p> <p>Each document goes through the following process.</p> <ul> <li>Content is retrieved if it's not local</li> <li>If the document <code>mime-type</code> isn't plain text or HTML, it's converted to HTML via the FiletoHTML pipeline</li> <li>HTML is converted to Markdown via the HTMLToMarkdown pipeline</li> <li>Content is split/chunked based on the segmentation parameters and returned</li> </ul> <p>The backend parameter sets the FileToHTML pipeline backend. If a backend isn't available, this pipeline assumes input is HTML content and only converts it to Markdown.</p> <p>See the FiletoHTML and HTMLToMarkdown pipelines to learn more on the dependencies necessary for each of those pipelines.</p>"},{"location":"pipeline/data/textractor/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Textractor\n\n# Create and run pipeline\ntextract = Textractor()\ntextract(\"https://github.com/neuml/txtai\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Extract text from documents Extract text from PDF, Office, HTML and more Chunking your data for RAG Extract, chunk and index content for effective retrieval"},{"location":"pipeline/data/textractor/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/data/textractor/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ntextractor:\n\n# Run pipeline with workflow\nworkflow:\n  textract:\n    tasks:\n      - action: textractor\n</code></pre>"},{"location":"pipeline/data/textractor/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"textract\", [\"https://github.com/neuml/txtai\"]))\n</code></pre>"},{"location":"pipeline/data/textractor/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"textract\", \"elements\":[\"https://github.com/neuml/txtai\"]}'\n</code></pre>"},{"location":"pipeline/data/textractor/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/data/textractor/#txtai.pipeline.Textractor.__init__","title":"<code>__init__(sentences=False, lines=False, paragraphs=False, minlength=None, join=False, sections=False, cleantext=True, chunker=None, headers=None, backend='available', **kwargs)</code>","text":"Source code in <code>txtai/pipeline/data/textractor.py</code> <pre><code>def __init__(\n    self,\n    sentences=False,\n    lines=False,\n    paragraphs=False,\n    minlength=None,\n    join=False,\n    sections=False,\n    cleantext=True,\n    chunker=None,\n    headers=None,\n    backend=\"available\",\n    **kwargs\n):\n    super().__init__(sentences, lines, paragraphs, minlength, join, sections, cleantext, chunker, **kwargs)\n\n    # Get backend parameter - handle legacy tika flag\n    backend = \"tika\" if \"tika\" in kwargs and kwargs[\"tika\"] else None if \"tika\" in kwargs else backend\n\n    # File to HTML pipeline\n    self.html = FileToHTML(backend) if backend else None\n\n    # HTML to Markdown pipeline\n    self.markdown = HTMLToMarkdown(self.paragraphs, self.sections)\n\n    # HTTP headers\n    self.headers = headers if headers else {}\n</code></pre>"},{"location":"pipeline/data/textractor/#txtai.pipeline.Textractor.__call__","title":"<code>__call__(text)</code>","text":"<p>Segments text into semantic units.</p> <p>This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <p>Returns:</p> Type Description <p>segmented text</p> Source code in <code>txtai/pipeline/data/segmentation.py</code> <pre><code>def __call__(self, text):\n    \"\"\"\n    Segments text into semantic units.\n\n    This method supports text as a string or a list. If the input is a string, the return\n    type is text|list. If text is a list, a list of returned, this could be a\n    list of text or a list of lists depending on the tokenization strategy.\n\n    Args:\n        text: text|list\n\n    Returns:\n        segmented text\n    \"\"\"\n\n    # Get inputs\n    texts = [text] if not isinstance(text, list) else text\n\n    # Extract text for each input file\n    results = []\n    for value in texts:\n        # Get text\n        result = self.text(value)\n\n        # Parse and add extracted results\n        result = self.parse(result)\n\n        # Wrap as tuple\n        if self.tuples:\n            result = [(value, x) for x in result] if isinstance(result, list) else (value, result)\n\n        results.append(result)\n\n    return results[0] if isinstance(text, str) else results\n</code></pre>"},{"location":"pipeline/data/tokenizer/","title":"Tokenizer","text":"<p>The Tokenizer pipeline splits text into tokens. This is primarily used for keyword / term indexing.</p> <p>Note: Transformers-based models have their own tokenizers and this pipeline isn't designed for working with Transformers models.</p>"},{"location":"pipeline/data/tokenizer/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Tokenizer\n\n# Create and run pipeline\ntokenizer = Tokenizer()\ntokenizer(\"text to tokenize\")\n\n# Whitespace tokenization\ntokenizer = Tokenizer(whitespace=True)\ntokenizer(\"text to tokenize\")\n\n# Tokenize using a regular expression\ntokenizer = Tokenizer(regexp=r\"\\w{5,}\")\ntokenizer(\"text to tokenize\")\n\n# Tokenize into trigrams like pg_trgm\ntokenizer = Tokenizer(ngrams={\n  \"ngrams\": 3, \"lpad\": \"  \", \"rpad\": \" \", \"unique\": True\n})\ntokenize(\"text to tokenize\")\n\n# Tokenize into edge ngrams\ntokenizer = Tokenizer(ngrams={\"nmin\": 2, \"nmax\": 5, \"edge\": True})\ntokenizer(\"text to tokenize\")\n</code></pre>"},{"location":"pipeline/data/tokenizer/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/data/tokenizer/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ntokenizer:\n\n# Run pipeline with workflow\nworkflow:\n  tokenizer:\n    tasks:\n      - action: tokenizer\n</code></pre>"},{"location":"pipeline/data/tokenizer/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"tokenizer\", [\"text to tokenize\"]))\n</code></pre>"},{"location":"pipeline/data/tokenizer/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"tokenizer\", \"elements\":[\"text\"]}'\n</code></pre>"},{"location":"pipeline/data/tokenizer/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/data/tokenizer/#txtai.pipeline.Tokenizer.__init__","title":"<code>__init__(lowercase=True, emoji=True, alphanum=False, stopwords=False, whitespace=False, regexp=None, ngrams=None)</code>","text":"<p>Creates a new tokenizer. The default parameters segment text per Unicode Standard Annex #29.</p> <p>Parameters:</p> Name Type Description Default <code>lowercase</code> <p>lower cases all tokens if True, defaults to True</p> <code>True</code> <code>emoji</code> <p>tokenize emoji in text if True, defaults to True</p> <code>True</code> <code>alphanum</code> <p>requires 2+ character alphanumeric tokens if True, defaults to False</p> <code>False</code> <code>stopwords</code> <p>removes provided stop words if a list, removes default English stop words if True, defaults to False</p> <code>False</code> <code>whitespace</code> <p>tokenize on whitespace if True, defaults to False</p> <code>False</code> <code>regexp</code> <p>tokenize using the provided regular expression, defaults to None</p> <code>None</code> <code>ngrams</code> <p>tokenize into ngrams, defaults to None, supports int or dict</p> <code>None</code> Source code in <code>txtai/pipeline/data/tokenizer.py</code> <pre><code>def __init__(self, lowercase=True, emoji=True, alphanum=False, stopwords=False, whitespace=False, regexp=None, ngrams=None):\n    \"\"\"\n    Creates a new tokenizer. The default parameters segment text per Unicode Standard Annex #29.\n\n    Args:\n        lowercase: lower cases all tokens if True, defaults to True\n        emoji: tokenize emoji in text if True, defaults to True\n        alphanum: requires 2+ character alphanumeric tokens if True, defaults to False\n        stopwords: removes provided stop words if a list, removes default English stop words if True, defaults to False\n        whitespace: tokenize on whitespace if True, defaults to False\n        regexp: tokenize using the provided regular expression, defaults to None\n        ngrams: tokenize into ngrams, defaults to None, supports int or dict\n    \"\"\"\n\n    # Lowercase\n    self.lowercase = lowercase\n\n    # Text segmentation\n    self.alphanum, self.whitespace, self.regexp, self.ngrams, self.segment = None, whitespace, None, None, None\n    if alphanum:\n        # Alphanumeric regex that accepts tokens that meet following rules:\n        #  - Strings to be at least 2 characters long AND\n        #  - At least 1 non-trailing alpha character in string\n        # Note: The standard Python re module is much faster than regex for this expression\n        self.alphanum = re.compile(r\"^\\d*[a-z][\\-.0-9:_a-z]{1,}$\")\n    elif regexp:\n        # Regular expression for tokenization\n        self.regexp = regex.compile(regexp)\n    elif ngrams:\n        # Ngram tokenization configuration\n        self.ngrams = ngrams if isinstance(ngrams, dict) else {\"ngrams\": ngrams}\n    else:\n        # Text segmentation per Unicode Standard Annex #29\n        pattern = r\"\\w\\p{Extended_Pictographic}\\p{WB:RegionalIndicator}\" if emoji else r\"\\w\"\n        self.segment = regex.compile(rf\"[{pattern}](?:\\B\\S)*\", flags=regex.WORD)\n\n    # Stop words\n    self.stopwords = stopwords if isinstance(stopwords, list) else Tokenizer.STOP_WORDS if stopwords else False\n</code></pre>"},{"location":"pipeline/data/tokenizer/#txtai.pipeline.Tokenizer.__call__","title":"<code>__call__(text)</code>","text":"<p>Tokenizes text into a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>input text</p> required <p>Returns:</p> Type Description <p>list of tokens</p> Source code in <code>txtai/pipeline/data/tokenizer.py</code> <pre><code>def __call__(self, text):\n    \"\"\"\n    Tokenizes text into a list of tokens.\n\n    Args:\n        text: input text\n\n    Returns:\n        list of tokens\n    \"\"\"\n\n    # Check for None and skip processing\n    if text is None:\n        return None\n\n    # Lowercase\n    text = text.lower() if self.lowercase else text\n\n    if self.alphanum:\n        # Text segmentation using standard split\n        tokens = [token.strip(string.punctuation) for token in text.split()]\n\n        # Filter on alphanumeric strings.\n        tokens = [token for token in tokens if re.match(self.alphanum, token)]\n    elif self.whitespace:\n        # Text segmentation using whitespace\n        tokens = text.split()\n    elif self.regexp:\n        # Text segmentation using a custom regular expression\n        tokens = regex.findall(self.regexp, text)\n    elif self.ngrams:\n        # Ngram tokenizer\n        tokens = self.ngramtokenize(text)\n    else:\n        # Text segmentation per Unicode Standard Annex #29\n        tokens = regex.findall(self.segment, text)\n\n    # Stop words\n    if self.stopwords:\n        tokens = [token for token in tokens if token not in self.stopwords]\n\n    return tokens\n</code></pre>"},{"location":"pipeline/image/caption/","title":"Caption","text":"<p>The caption pipeline reads a list of images and returns a list of captions for those images.</p>"},{"location":"pipeline/image/caption/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Caption\n\n# Create and run pipeline\ncaption = Caption()\ncaption(\"path to image file\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Generate image captions and detect objects Captions and object detection for images"},{"location":"pipeline/image/caption/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/image/caption/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ncaption:\n\n# Run pipeline with workflow\nworkflow:\n  caption:\n    tasks:\n      - action: caption\n</code></pre>"},{"location":"pipeline/image/caption/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"caption\", [\"path to image file\"]))\n</code></pre>"},{"location":"pipeline/image/caption/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"caption\", \"elements\":[\"path to image file\"]}'\n</code></pre>"},{"location":"pipeline/image/caption/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/image/caption/#txtai.pipeline.Caption.__init__","title":"<code>__init__(path=None, quantize=False, gpu=True, model=None, **kwargs)</code>","text":"Source code in <code>txtai/pipeline/image/caption.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):\n    if not PIL:\n        raise ImportError('Captions pipeline is not available - install \"pipeline\" extra to enable')\n\n    # Call parent constructor\n    super().__init__(\"image-to-text\", path, quantize, gpu, model, **kwargs)\n</code></pre>"},{"location":"pipeline/image/caption/#txtai.pipeline.Caption.__call__","title":"<code>__call__(images)</code>","text":"<p>Builds captions for images.</p> <p>This method supports a single image or a list of images. If the input is an image, the return type is a string. If text is a list, a list of strings is returned</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>image|list</p> required <p>Returns:</p> Type Description <p>list of captions</p> Source code in <code>txtai/pipeline/image/caption.py</code> <pre><code>def __call__(self, images):\n    \"\"\"\n    Builds captions for images.\n\n    This method supports a single image or a list of images. If the input is an image, the return\n    type is a string. If text is a list, a list of strings is returned\n\n    Args:\n        images: image|list\n\n    Returns:\n        list of captions\n    \"\"\"\n\n    # Convert single element to list\n    values = [images] if not isinstance(images, list) else images\n\n    # Open images if file strings\n    values = [Image.open(image) if isinstance(image, str) else image for image in values]\n\n    # Get and clean captions\n    captions = []\n    for result in self.pipeline(values):\n        text = \" \".join([r[\"generated_text\"] for r in result]).strip()\n        captions.append(text)\n\n    # Return single element if single element passed in\n    return captions[0] if not isinstance(images, list) else captions\n</code></pre>"},{"location":"pipeline/image/imagehash/","title":"ImageHash","text":"<p>The image hash pipeline generates perceptual image hashes. These hashes can be used to detect near-duplicate images. This method is not backed by machine learning models and not intended to find conceptually similar images.</p>"},{"location":"pipeline/image/imagehash/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import ImageHash\n\n# Create and run pipeline\nihash = ImageHash()\nihash(\"path to image file\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Near duplicate image detection Identify duplicate and near-duplicate images"},{"location":"pipeline/image/imagehash/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/image/imagehash/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nimagehash:\n\n# Run pipeline with workflow\nworkflow:\n  imagehash:\n    tasks:\n      - action: imagehash\n</code></pre>"},{"location":"pipeline/image/imagehash/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"imagehash\", [\"path to image file\"]))\n</code></pre>"},{"location":"pipeline/image/imagehash/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"imagehash\", \"elements\":[\"path to image file\"]}'\n</code></pre>"},{"location":"pipeline/image/imagehash/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/image/imagehash/#txtai.pipeline.ImageHash.__init__","title":"<code>__init__(algorithm='average', size=8, strings=True)</code>","text":"<p>Creates an ImageHash pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <p>image hashing algorithm (average, perceptual, difference, wavelet, color)</p> <code>'average'</code> <code>size</code> <p>hash size</p> <code>8</code> <code>strings</code> <p>outputs hex strings if True (default), otherwise the pipeline returns numpy arrays</p> <code>True</code> Source code in <code>txtai/pipeline/image/imagehash.py</code> <pre><code>def __init__(self, algorithm=\"average\", size=8, strings=True):\n    \"\"\"\n    Creates an ImageHash pipeline.\n\n    Args:\n        algorithm: image hashing algorithm (average, perceptual, difference, wavelet, color)\n        size: hash size\n        strings: outputs hex strings if True (default), otherwise the pipeline returns numpy arrays\n    \"\"\"\n\n    if not PIL:\n        raise ImportError('ImageHash pipeline is not available - install \"pipeline\" extra to enable')\n\n    self.algorithm = algorithm\n    self.size = size\n    self.strings = strings\n</code></pre>"},{"location":"pipeline/image/imagehash/#txtai.pipeline.ImageHash.__call__","title":"<code>__call__(images)</code>","text":"<p>Generates perceptual image hashes.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>image|list</p> required <p>Returns:</p> Type Description <p>list of hashes</p> Source code in <code>txtai/pipeline/image/imagehash.py</code> <pre><code>def __call__(self, images):\n    \"\"\"\n    Generates perceptual image hashes.\n\n    Args:\n        images: image|list\n\n    Returns:\n        list of hashes\n    \"\"\"\n\n    # Convert single element to list\n    values = [images] if not isinstance(images, list) else images\n\n    # Open images if file strings\n    values = [Image.open(image) if isinstance(image, str) else image for image in values]\n\n    # Convert images to hashes\n    hashes = [self.ihash(image) for image in values]\n\n    # Return single element if single element passed in\n    return hashes[0] if not isinstance(images, list) else hashes\n</code></pre>"},{"location":"pipeline/image/objects/","title":"Objects","text":"<p>The Objects pipeline reads a list of images and returns a list of detected objects.</p>"},{"location":"pipeline/image/objects/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Objects\n\n# Create and run pipeline\nobjects = Objects()\nobjects(\"path to image file\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Generate image captions and detect objects Captions and object detection for images"},{"location":"pipeline/image/objects/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/image/objects/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nobjects:\n\n# Run pipeline with workflow\nworkflow:\n  objects:\n    tasks:\n      - action: objects\n</code></pre>"},{"location":"pipeline/image/objects/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"objects\", [\"path to image file\"]))\n</code></pre>"},{"location":"pipeline/image/objects/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"objects\", \"elements\":[\"path to image file\"]}'\n</code></pre>"},{"location":"pipeline/image/objects/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/image/objects/#txtai.pipeline.Objects.__init__","title":"<code>__init__(path=None, quantize=False, gpu=True, model=None, classification=False, threshold=0.9, **kwargs)</code>","text":"Source code in <code>txtai/pipeline/image/objects.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None, classification=False, threshold=0.9, **kwargs):\n    if not PIL:\n        raise ImportError('Objects pipeline is not available - install \"pipeline\" extra to enable')\n\n    super().__init__(\"image-classification\" if classification else \"object-detection\", path, quantize, gpu, model, **kwargs)\n\n    self.classification = classification\n    self.threshold = threshold\n</code></pre>"},{"location":"pipeline/image/objects/#txtai.pipeline.Objects.__call__","title":"<code>__call__(images, flatten=False, workers=0)</code>","text":"<p>Applies object detection/image classification models to images. Returns a list of (label, score).</p> <p>This method supports a single image or a list of images. If the input is an image, the return type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is returned with a row per image.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>image|list</p> required <code>flatten</code> <p>flatten output to a list of objects</p> <code>False</code> <code>workers</code> <p>number of concurrent workers to use for processing data, defaults to None</p> <code>0</code> <p>Returns:</p> Type Description <p>list of (label, score)</p> Source code in <code>txtai/pipeline/image/objects.py</code> <pre><code>def __call__(self, images, flatten=False, workers=0):\n    \"\"\"\n    Applies object detection/image classification models to images. Returns a list of (label, score).\n\n    This method supports a single image or a list of images. If the input is an image, the return\n    type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is\n    returned with a row per image.\n\n    Args:\n        images: image|list\n        flatten: flatten output to a list of objects\n        workers: number of concurrent workers to use for processing data, defaults to None\n\n    Returns:\n        list of (label, score)\n    \"\"\"\n\n    # Convert single element to list\n    values = [images] if not isinstance(images, list) else images\n\n    # Open images if file strings\n    values = [Image.open(image) if isinstance(image, str) else image for image in values]\n\n    # Run pipeline\n    results = (\n        self.pipeline(values, num_workers=workers)\n        if self.classification\n        else self.pipeline(values, threshold=self.threshold, num_workers=workers)\n    )\n\n    # Build list of (id, score)\n    outputs = []\n    for result in results:\n        # Convert to (label, score) tuples\n        result = [(x[\"label\"], x[\"score\"]) for x in result if x[\"score\"] &gt; self.threshold]\n\n        # Sort by score descending\n        result = sorted(result, key=lambda x: x[1], reverse=True)\n\n        # Deduplicate labels\n        unique = set()\n        elements = []\n        for label, score in result:\n            if label not in unique:\n                elements.append(label if flatten else (label, score))\n                unique.add(label)\n\n        outputs.append(elements)\n\n    # Return single element if single element passed in\n    return outputs[0] if not isinstance(images, list) else outputs\n</code></pre>"},{"location":"pipeline/llm/llm/","title":"LLM","text":"<p>The LLM pipeline runs prompts through a large language model (LLM). This pipeline autodetects the LLM framework based on the model path.</p>"},{"location":"pipeline/llm/llm/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai import LLM\n\n# Create LLM pipeline\nllm = LLM()\n\n# Run prompt\nllm(\n  \"\"\"\n  Answer the following question using the provided context.\n\n  Question:\n  What are the applications of txtai?\n\n  Context:\n  txtai is an open-source platform for semantic search and\n  workflows powered by language models.\n  \"\"\"\n)\n\n# Prompts with chat templating can be directly passed\n# The template format varies by model\nllm(\n  \"\"\"\n  &lt;|im_start|&gt;system\n  You are a friendly assistant.&lt;|im_end|&gt;\n  &lt;|im_start|&gt;user\n  Answer the following question...&lt;|im_end|&gt;\n  &lt;|im_start|&gt;assistant\n  \"\"\"\n)\n\n# Chat messages automatically handle templating\nllm([\n  {\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\n  {\"role\": \"user\", \"content\": \"Answer the following question...\"}\n])\n\n# When there is no system prompt passed to instruction tuned models\n# the default role is inferred `defaultrole=\"auto\"`\nllm(\"Answer the following question...\")\n\n# To always generate chat messages for string inputs\nllm(\"Answer the following question...\", defaultrole=\"user\")\n\n# To never generate chat messages for string inputs\nllm(\"Answer the following question...\", defaultrole=\"prompt\")\n</code></pre> <p>The LLM pipeline automatically detects the underlying LLM framework. This can also be manually set. The following methods are supported.</p> <ul> <li>Hugging Face Transformers</li> <li>llama.cpp</li> <li>LLM APIs via LiteLLM</li> <li>OpenCode server</li> </ul> <p><code>llama.cpp</code> models support both local and remote GGUF paths on the HF Hub. See the LiteLLM documentation for the options available with LiteLLM models. See the OpenCode documentation for more on how to integrate the LLM pipeline with a running OpenCode instance.</p> <pre><code>from txtai import LLM\n\n# Transformers\nllm = LLM(\"openai/gpt-oss-20b\")\nllm = LLM(\"openai/gpt-oss-20b\", method=\"transformers\")\n\n# llama.cpp\nllm = LLM(\"unsloth/gpt-oss-20b-GGUF/gpt-oss-20b-Q4_K_M.gguf\")\nllm = LLM(\"unsloth/gpt-oss-20b-GGUF/gpt-oss-20b-Q4_K_M.gguf\",\n           method=\"llama.cpp\")\n\n# LiteLLM\nllm = LLM(\"ollama/gpt-oss\")\nllm = LLM(\"ollama/gpt-oss\", method=\"litellm\")\n\n# Custom Ollama endpoint\nllm = LLM(\"ollama/gpt-oss\", api_base=\"http://localhost:11434\")\n\n# Custom OpenAI-compatible endpoint\nllm = LLM(\"openai/gpt-oss\", api_base=\"http://localhost:4000\")\n\n# LLM APIs - must also set API key via environment variable\nllm = LLM(\"gpt-5.2\")\nllm = LLM(\"claude-opus-4-5-20251101\")\nllm = LLM(\"gemini/gemini-3-pro-preview\")\n\n# Local OpenCode server started via `opencode serve`\nllm = LLM(\"opencode\")\nllm = LLM(\"opencode/big-pickle\", url=\"http://localhost:4000\")\n</code></pre> <p>Models can be externally loaded and passed to pipelines. This is useful for models that are not yet supported by Transformers and/or need special initialization.</p> <pre><code>import torch\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom txtai import LLM\n\n# Load Qwen3 0.6B\npath = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(\n  path,\n  dtype=torch.bfloat16,\n)\ntokenizer = AutoTokenizer.from_pretrained(path)\n\nllm = LLM((model, tokenizer))\n</code></pre> <p>See the links below for more detailed examples.</p> Notebook Description Prompt-driven search with LLMs Embeddings-guided and Prompt-driven search with Large Language Models (LLMs) Prompt templates and task chains Build model prompts and connect tasks together with workflows Build RAG pipelines with txtai \u25b6\ufe0f Guide on retrieval augmented generation including how to create citations Integrate LLM frameworks Integrate llama.cpp, LiteLLM and custom generation frameworks Generate knowledge with Semantic Graphs and RAG Knowledge exploration and discovery with Semantic Graphs and RAG Build knowledge graphs with LLMs Build knowledge graphs with LLM-driven entity extraction Advanced RAG with graph path traversal Graph path traversal to collect complex sets of data for advanced RAG Advanced RAG with guided generation Retrieval Augmented and Guided Generation RAG with llama.cpp and external API services RAG with additional vector and LLM frameworks How RAG with txtai works Create RAG processes, API services and Docker instances Speech to Speech RAG \u25b6\ufe0f Full cycle speech to speech workflow with RAG Analyzing Hugging Face Posts with Graphs and Agents Explore a rich dataset with Graph Analysis and Agents Granting autonomy to agents Agents that iteratively solve problems as they see fit Getting started with LLM APIs Generate embeddings and run LLMs with OpenAI, Claude, Gemini, Bedrock and more Analyzing LinkedIn Company Posts with Graphs and Agents Exploring how to improve social media engagement with AI Parsing the stars with txtai Explore an astronomical knowledge graph of known stars, planets, galaxies Chunking your data for RAG Extract, chunk and index content for effective retrieval Medical RAG Research with txtai Analyze PubMed article metadata with RAG GraphRAG with Wikipedia and GPT OSS Deep graph search powered RAG RAG is more than Vector Search Context retrieval via Web, SQL and other sources OpenCode as a txtai LLM Integrate OpenCode with the txtai ecosystem Agentic College Search Identify a list of strong engineering colleges TxtAI got skills Integrate skill.md files with your agent"},{"location":"pipeline/llm/llm/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/llm/llm/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nllm:\n\n# Run pipeline with workflow\nworkflow:\n  llm:\n    tasks:\n      - action: llm\n</code></pre> <p>Similar to the Python example above, the underlying Hugging Face pipeline parameters and model parameters can be set in pipeline configuration.</p> <pre><code>llm:\n  path: Qwen/Qwen3-0.6B\n  dtype: torch.bfloat16\n</code></pre>"},{"location":"pipeline/llm/llm/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"llm\", [\n  \"\"\"\n  Answer the following question using the provided context.\n\n  Question:\n  What are the applications of txtai? \n\n  Context:\n  txtai is an open-source platform for semantic search and\n  workflows powered by language models.\n  \"\"\"\n]))\n</code></pre>"},{"location":"pipeline/llm/llm/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"llm\", \"elements\": [\"Answer the following question...\"]}'\n</code></pre>"},{"location":"pipeline/llm/llm/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/llm/llm/#txtai.pipeline.LLM.__init__","title":"<code>__init__(path=None, method=None, **kwargs)</code>","text":"<p>Creates a new LLM.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>model path</p> <code>None</code> <code>method</code> <p>llm model framework, infers from path if not provided</p> <code>None</code> <code>kwargs</code> <p>model keyword arguments</p> <code>{}</code> Source code in <code>txtai/pipeline/llm/llm.py</code> <pre><code>def __init__(self, path=None, method=None, **kwargs):\n    \"\"\"\n    Creates a new LLM.\n\n    Args:\n        path: model path\n        method: llm model framework, infers from path if not provided\n        kwargs: model keyword arguments\n    \"\"\"\n\n    # Default LLM if not provided\n    path = path if path else \"ibm-granite/granite-4.0-350m\"\n\n    # Generation instance\n    self.generator = GenerationFactory.create(path, method, **kwargs)\n</code></pre>"},{"location":"pipeline/llm/llm/#txtai.pipeline.LLM.__call__","title":"<code>__call__(text, maxlength=512, stream=False, stop=None, defaultrole='auto', stripthink=None, **kwargs)</code>","text":"<p>Generates content. Supports the following input formats:</p> <ul> <li>String or list of strings (instruction-tuned models must follow chat templates)</li> <li>List of dictionaries with <code>role</code> and <code>content</code> key-values or lists of lists</li> </ul> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <code>maxlength</code> <p>maximum sequence length</p> <code>512</code> <code>stream</code> <p>stream response if True, defaults to False</p> <code>False</code> <code>stop</code> <p>list of stop strings, defaults to None</p> <code>None</code> <code>defaultrole</code> <p>default role to apply to text inputs (<code>auto</code> to infer (default), <code>user</code> for user chat messages or <code>prompt</code> for raw prompts)</p> <code>'auto'</code> <code>stripthink</code> <p>strip thinking tags, defaults to False if stream is enabled, True otherwise</p> <code>None</code> <code>kwargs</code> <p>additional generation keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <p>generated content</p> Source code in <code>txtai/pipeline/llm/llm.py</code> <pre><code>def __call__(self, text, maxlength=512, stream=False, stop=None, defaultrole=\"auto\", stripthink=None, **kwargs):\n    \"\"\"\n    Generates content. Supports the following input formats:\n\n      - String or list of strings (instruction-tuned models must follow chat templates)\n      - List of dictionaries with `role` and `content` key-values or lists of lists\n\n    Args:\n        text: text|list\n        maxlength: maximum sequence length\n        stream: stream response if True, defaults to False\n        stop: list of stop strings, defaults to None\n        defaultrole: default role to apply to text inputs (`auto` to infer (default), `user` for user chat messages or `prompt` for raw prompts)\n        stripthink: strip thinking tags, defaults to False if stream is enabled, True otherwise\n        kwargs: additional generation keyword arguments\n\n    Returns:\n        generated content\n    \"\"\"\n\n    # Debug logging\n    logger.debug(text)\n\n    # Default stripthink to False when streaming, True otherwise\n    stripthink = not stream if stripthink is None else stripthink\n\n    # Run LLM generation\n    return self.generator(text, maxlength, stream, stop, defaultrole, stripthink, **kwargs)\n</code></pre>"},{"location":"pipeline/llm/rag/","title":"RAG","text":"<p>The Retrieval Augmented Generation (RAG) pipeline joins a prompt, context data store and generative model together to extract knowledge.</p> <p>The data store can be an embeddings database or a similarity instance with associated input text. The generative model can be a prompt-driven large language model (LLM), an extractive question-answering model or a custom pipeline.</p>"},{"location":"pipeline/llm/rag/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai import Embeddings, RAG\n\n# Input data\ndata = [\n  \"US tops 5 million confirmed virus cases\",\n  \"Canada's last fully intact ice shelf has suddenly collapsed, \" +\n  \"forming a Manhattan-sized iceberg\",\n  \"Beijing mobilises invasion craft along coast as Taiwan tensions escalate\",\n  \"The National Park Service warns against sacrificing slower friends \" +\n  \"in a bear attack\",\n  \"Maine man wins $1M from $25 lottery ticket\",\n  \"Make huge profits without work, earn up to $100,000 a day\"\n]\n\n# Build embeddings index\nembeddings = Embeddings(content=True)\nembeddings.index(data)\n\n# Create the RAG pipeline\nrag = RAG(embeddings, \"Qwen/Qwen3-0.6B\", template=\"\"\"\n  Answer the following question using the provided context.\n\n  Question:\n  {question}\n\n  Context:\n  {context}\n\"\"\")\n\n# Run RAG pipeline\nrag(\"What was won?\")\n\n# Prompts with chat templating can be directly passed\n# The template format varies by model\nrag = RAG(embeddings, \"Qwen/Qwen3-0.6B\", template=\"\"\"\n  &lt;|im_start|&gt;system\n  You are a friendly assistant.&lt;|im_end|&gt;\n  &lt;|im_start|&gt;user\n  Answer the following question using the provided context.\n\n  Question:\n  {question}\n\n  Context:\n  {context}\n  &lt;|im_start|&gt;assistant\n  \"\"\"\n)\nrag(\"What was won?\")\n\n# Inputs are automatically converted to chat messages when a\n# system prompt is provided\nrag = RAG(\n  embeddings,\n  \"openai/gpt-oss-20b\",\n  system=\"You are a friendly assistant\",\n  template=\"\"\"\n  Answer the following question using the provided context.\n\n  Question:\n  {question}\n\n  Context:\n  {context}\n\"\"\")\nrag(\"What was won?\")\n\n# LLM options can be passed as additional arguments\n#  - Streaming RAG response with `stream=True`\n#  - String inputs are always converted to user messages with `defaultrole=\"user\"`\n#  - Thinking text is removed with `stripthink=True`\nrag(\"What was won?\", stream=True, defaultrole=\"user\", stripThink=True)\n</code></pre> <p>See the Embeddings and LLM pages for additional configuration options.</p> <p>Check out this RAG Quickstart Example. Additional examples are listed below.</p> Notebook Description Prompt-driven search with LLMs Embeddings-guided and Prompt-driven search with Large Language Models (LLMs) Build RAG pipelines with txtai \u25b6\ufe0f Guide on retrieval augmented generation including how to create citations Integrate LLM frameworks Integrate llama.cpp, LiteLLM and custom generation frameworks Generate knowledge with Semantic Graphs and RAG Knowledge exploration and discovery with Semantic Graphs and RAG Advanced RAG with graph path traversal Graph path traversal to collect complex sets of data for advanced RAG Advanced RAG with guided generation Retrieval Augmented and Guided Generation RAG with llama.cpp and external API services RAG with additional vector and LLM frameworks How RAG with txtai works Create RAG processes, API services and Docker instances Speech to Speech RAG \u25b6\ufe0f Full cycle speech to speech workflow with RAG Parsing the stars with txtai Explore an astronomical knowledge graph of known stars, planets, galaxies Chunking your data for RAG Extract, chunk and index content for effective retrieval Medical RAG Research with txtai Analyze PubMed article metadata with RAG GraphRAG with Wikipedia and GPT OSS Deep graph search powered RAG RAG is more than Vector Search Context retrieval via Web, SQL and other sources"},{"location":"pipeline/llm/rag/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/llm/rag/#configyml","title":"config.yml","text":"<pre><code># Allow documents to be indexed\nwritable: True\n\n# Content is required for extractor pipeline\nembeddings:\n  content: True\n\nrag:\n  path: Qwen/Qwen3-0.6B\n  template: |\n    Answer the following question using the provided context.\n\n    Question:\n    {question}\n\n    Context:\n    {context}\n\nworkflow:\n  search:\n    tasks:\n      - action: rag\n</code></pre>"},{"location":"pipeline/llm/rag/#run-with-workflows","title":"Run with Workflows","text":"<p>Built in tasks make using the extractor pipeline easier.</p> <pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\napp.add([\n  \"US tops 5 million confirmed virus cases\",\n  \"Canada's last fully intact ice shelf has suddenly collapsed, \" +\n  \"forming a Manhattan-sized iceberg\",\n  \"Beijing mobilises invasion craft along coast as Taiwan tensions escalate\",\n  \"The National Park Service warns against sacrificing slower friends \" +\n  \"in a bear attack\",\n  \"Maine man wins $1M from $25 lottery ticket\",\n  \"Make huge profits without work, earn up to $100,000 a day\"\n])\napp.index()\n\nlist(app.workflow(\"search\", [\"What was won?\"]))\n</code></pre>"},{"location":"pipeline/llm/rag/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"search\", \"elements\": [\"What was won\"]}'\n</code></pre>"},{"location":"pipeline/llm/rag/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/llm/rag/#txtai.pipeline.RAG.__init__","title":"<code>__init__(similarity, path, quantize=False, gpu=True, model=None, tokenizer=None, minscore=None, mintokens=None, context=None, task=None, output='default', template=None, separator=' ', system=None, **kwargs)</code>","text":"<p>Builds a new RAG pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>similarity</code> <p>similarity instance (embeddings or similarity pipeline)</p> required <code>path</code> <p>path to model, supports a LLM, Questions or custom pipeline</p> required <code>quantize</code> <p>True if model should be quantized before inference, False otherwise.</p> <code>False</code> <code>gpu</code> <p>if gpu inference should be used (only works if GPUs are available)</p> <code>True</code> <code>model</code> <p>optional existing pipeline model to wrap</p> <code>None</code> <code>tokenizer</code> <p>Tokenizer class</p> <code>None</code> <code>minscore</code> <p>minimum score to include context match, defaults to None</p> <code>None</code> <code>mintokens</code> <p>minimum number of tokens to include context match, defaults to None</p> <code>None</code> <code>context</code> <p>topn context matches to include, defaults to 3</p> <code>None</code> <code>task</code> <p>model task (language-generation, sequence-sequence or question-answering), defaults to auto-detect</p> <code>None</code> <code>output</code> <p>output format, 'default' returns (name, answer), 'flatten' returns answers and 'reference' returns (name, answer, reference)</p> <code>'default'</code> <code>template</code> <p>prompt template, it must have a parameter for {question} and {context}, defaults to \"{question} {context}\"</p> <code>None</code> <code>separator</code> <p>context separator</p> <code>' '</code> <code>system</code> <p>system prompt, defaults to None</p> <code>None</code> <code>kwargs</code> <p>additional keyword arguments to pass to pipeline model</p> <code>{}</code> Source code in <code>txtai/pipeline/llm/rag.py</code> <pre><code>def __init__(\n    self,\n    similarity,\n    path,\n    quantize=False,\n    gpu=True,\n    model=None,\n    tokenizer=None,\n    minscore=None,\n    mintokens=None,\n    context=None,\n    task=None,\n    output=\"default\",\n    template=None,\n    separator=\" \",\n    system=None,\n    **kwargs,\n):\n    \"\"\"\n    Builds a new RAG pipeline.\n\n    Args:\n        similarity: similarity instance (embeddings or similarity pipeline)\n        path: path to model, supports a LLM, Questions or custom pipeline\n        quantize: True if model should be quantized before inference, False otherwise.\n        gpu: if gpu inference should be used (only works if GPUs are available)\n        model: optional existing pipeline model to wrap\n        tokenizer: Tokenizer class\n        minscore: minimum score to include context match, defaults to None\n        mintokens: minimum number of tokens to include context match, defaults to None\n        context: topn context matches to include, defaults to 3\n        task: model task (language-generation, sequence-sequence or question-answering), defaults to auto-detect\n        output: output format, 'default' returns (name, answer), 'flatten' returns answers and 'reference' returns (name, answer, reference)\n        template: prompt template, it must have a parameter for {question} and {context}, defaults to \"{question} {context}\"\n        separator: context separator\n        system: system prompt, defaults to None\n        kwargs: additional keyword arguments to pass to pipeline model\n    \"\"\"\n\n    # Similarity instance\n    self.similarity = similarity\n\n    # Model can be a LLM, Questions or custom pipeline\n    self.model = self.load(path, quantize, gpu, model, task, **kwargs)\n\n    # Tokenizer class use default method if not set\n    self.tokenizer = tokenizer if tokenizer else Tokenizer() if hasattr(self.similarity, \"scoring\") and self.similarity.isweighted() else None\n\n    # Minimum score to include context match\n    self.minscore = minscore if minscore is not None else 0.0\n\n    # Minimum number of tokens to include context match\n    self.mintokens = mintokens if mintokens is not None else 0.0\n\n    # Top n context matches to include for context\n    self.context = context if context else 3\n\n    # Output format\n    self.output = output\n\n    # Prompt template\n    self.template = template if template else \"{question} {context}\"\n\n    # Context separator\n    self.separator = separator\n\n    # System prompt template\n    self.system = system\n</code></pre>"},{"location":"pipeline/llm/rag/#txtai.pipeline.RAG.__call__","title":"<code>__call__(queue, texts=None, **kwargs)</code>","text":"<p>Finds answers to input questions. This method runs queries to find the top n best matches and uses that as the context. A model is then run against the context for each input question, with the answer returned.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <p>input question queue (name, query, question, snippet), can be list of tuples/dicts/strings or a single input element</p> required <code>texts</code> <p>optional list of text for context, otherwise runs embeddings search</p> <code>None</code> <code>kwargs</code> <p>additional keyword arguments to pass to pipeline model</p> <code>{}</code> <p>Returns:</p> Type Description <p>list of answers matching input format (tuple or dict) containing fields as specified by output format</p> Source code in <code>txtai/pipeline/llm/rag.py</code> <pre><code>def __call__(self, queue, texts=None, **kwargs):\n    \"\"\"\n    Finds answers to input questions. This method runs queries to find the top n best matches and uses that as the context.\n    A model is then run against the context for each input question, with the answer returned.\n\n    Args:\n        queue: input question queue (name, query, question, snippet), can be list of tuples/dicts/strings or a single input element\n        texts: optional list of text for context, otherwise runs embeddings search\n        kwargs: additional keyword arguments to pass to pipeline model\n\n    Returns:\n        list of answers matching input format (tuple or dict) containing fields as specified by output format\n    \"\"\"\n\n    # Save original queue format\n    inputs = queue\n\n    # Convert queue to list, if necessary\n    queue = queue if isinstance(queue, list) else [queue]\n\n    # Convert dictionary inputs to tuples\n    if queue and isinstance(queue[0], dict):\n        # Convert dict to tuple\n        queue = [tuple(row.get(x) for x in [\"name\", \"query\", \"question\", \"snippet\"]) for row in queue]\n\n    if queue and isinstance(queue[0], str):\n        # Convert string questions to tuple\n        queue = [(None, row, row, None) for row in queue]\n\n    # Rank texts by similarity for each query\n    results = self.query([query for _, query, _, _ in queue], texts)\n\n    # Build question-context pairs\n    names, queries, questions, contexts, topns, snippets = [], [], [], [], [], []\n    for x, (name, query, question, snippet) in enumerate(queue):\n        # Get top n best matching segments\n        topn = sorted(results[x], key=lambda y: y[2], reverse=True)[: self.context]\n\n        # Generate context using ordering from texts, if available, otherwise order by score\n        context = self.separator.join(text for _, text, _ in (sorted(topn, key=lambda y: y[0]) if texts else topn))\n\n        names.append(name)\n        queries.append(query)\n        questions.append(question)\n        contexts.append(context)\n        topns.append(topn)\n        snippets.append(snippet)\n\n    # Run pipeline and return answers\n    answers = self.answers(questions, contexts, **kwargs)\n\n    # Apply output formatting to answers and return\n    return self.apply(inputs, names, queries, answers, topns, snippets) if isinstance(answers, list) else answers\n</code></pre>"},{"location":"pipeline/text/entity/","title":"Entity","text":"<p>The Entity pipeline applies a token classifier to text and extracts entity/label combinations.</p>"},{"location":"pipeline/text/entity/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Entity\n\n# Create and run pipeline\nentity = Entity()\nentity(\"Canada's last fully intact ice shelf has suddenly collapsed, \" \\\n       \"forming a Manhattan-sized iceberg\")\n\n# Extract entities using a GLiNER model which supports dynamic labels\nentity = Entity(\"gliner-community/gliner_medium-v2.5\")\nentity(\"Canada's last fully intact ice shelf has suddenly collapsed, \" \\\n       \"forming a Manhattan-sized iceberg\", labels=[\"country\", \"city\"])\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Entity extraction workflows Identify entity/label combinations Parsing the stars with txtai Explore an astronomical knowledge graph of known stars, planets, galaxies"},{"location":"pipeline/text/entity/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/entity/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nentity:\n\n# Run pipeline with workflow\nworkflow:\n  entity:\n    tasks:\n      - action: entity\n</code></pre>"},{"location":"pipeline/text/entity/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"entity\", [\"Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\"]))\n</code></pre>"},{"location":"pipeline/text/entity/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"entity\", \"elements\": [\"Canadas last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\"]}'\n</code></pre>"},{"location":"pipeline/text/entity/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/entity/#txtai.pipeline.Entity.__init__","title":"<code>__init__(path=None, quantize=False, gpu=True, model=None, **kwargs)</code>","text":"Source code in <code>txtai/pipeline/text/entity.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):\n    # Create a new entity pipeline\n    self.gliner = self.isgliner(path)\n    if self.gliner:\n        if not GLINER:\n            raise ImportError('GLiNER is not available - install \"pipeline\" extra to enable')\n\n        # GLiNER entity pipeline\n        self.pipeline = GLiNER.from_pretrained(path)\n        self.pipeline = self.pipeline.to(Models.device(Models.deviceid(gpu)))\n    else:\n        # Standard entity pipeline\n        super().__init__(\"token-classification\", path, quantize, gpu, model, **kwargs)\n</code></pre>"},{"location":"pipeline/text/entity/#txtai.pipeline.Entity.__call__","title":"<code>__call__(text, labels=None, aggregate='simple', flatten=None, join=False, workers=0)</code>","text":"<p>Applies a token classifier to text and extracts entity/label combinations.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <code>labels</code> <p>list of entity type labels to accept, defaults to None which accepts all</p> <code>None</code> <code>aggregate</code> <p>method to combine multi token entities - options are \"simple\" (default), \"first\", \"average\" or \"max\"</p> <code>'simple'</code> <code>flatten</code> <p>flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number.</p> <code>None</code> <code>join</code> <p>joins flattened output into a string if True, ignored if flatten not set</p> <code>False</code> <code>workers</code> <p>number of concurrent workers to use for processing data, defaults to None</p> <code>0</code> <p>Returns:</p> Type Description <p>list of (entity, entity type, score) or list of entities depending on flatten parameter</p> Source code in <code>txtai/pipeline/text/entity.py</code> <pre><code>def __call__(self, text, labels=None, aggregate=\"simple\", flatten=None, join=False, workers=0):\n    \"\"\"\n    Applies a token classifier to text and extracts entity/label combinations.\n\n    Args:\n        text: text|list\n        labels: list of entity type labels to accept, defaults to None which accepts all\n        aggregate: method to combine multi token entities - options are \"simple\" (default), \"first\", \"average\" or \"max\"\n        flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number.\n        join: joins flattened output into a string if True, ignored if flatten not set\n        workers: number of concurrent workers to use for processing data, defaults to None\n\n    Returns:\n        list of (entity, entity type, score) or list of entities depending on flatten parameter\n    \"\"\"\n\n    # Run token classification pipeline\n    results = self.execute(text, labels, aggregate, workers)\n\n    # Convert results to a list if necessary\n    if isinstance(text, str):\n        results = [results]\n\n    # Score threshold when flatten is set\n    threshold = 0.0 if isinstance(flatten, bool) else flatten\n\n    # Extract entities if flatten set, otherwise extract (entity, entity type, score) tuples\n    outputs = []\n    for result in results:\n        if flatten:\n            output = [r[\"word\"] for r in result if self.accept(r[\"entity_group\"], labels) and r[\"score\"] &gt;= threshold]\n            outputs.append(\" \".join(output) if join else output)\n        else:\n            outputs.append([(r[\"word\"], r[\"entity_group\"], float(r[\"score\"])) for r in result if self.accept(r[\"entity_group\"], labels)])\n\n    return outputs[0] if isinstance(text, str) else outputs\n</code></pre>"},{"location":"pipeline/text/labels/","title":"Labels","text":"<p>The Labels pipeline uses a text classification model to apply labels to input text. This pipeline can classify text using either a zero shot model (dynamic labeling) or a standard text classification model (fixed labeling).</p>"},{"location":"pipeline/text/labels/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Labels\n\n# Create and run pipeline\nlabels = Labels()\nlabels(\n    [\"Great news\", \"That's rough\"],\n    [\"positive\", \"negative\"]\n)\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling"},{"location":"pipeline/text/labels/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/labels/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nlabels:\n\n# Run pipeline with workflow\nworkflow:\n  labels:\n    tasks:\n      - action: labels\n        args: [[\"positive\", \"negative\"]]\n</code></pre>"},{"location":"pipeline/text/labels/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"labels\", [\"Great news\", \"That's rough\"]))\n</code></pre>"},{"location":"pipeline/text/labels/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"labels\", \"elements\": [\"Great news\", \"Thats rough\"]}'\n</code></pre>"},{"location":"pipeline/text/labels/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/labels/#txtai.pipeline.Labels.__init__","title":"<code>__init__(path=None, quantize=False, gpu=True, model=None, dynamic=True, **kwargs)</code>","text":"Source code in <code>txtai/pipeline/text/labels.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None, dynamic=True, **kwargs):\n    super().__init__(\"zero-shot-classification\" if dynamic else \"text-classification\", path, quantize, gpu, model, **kwargs)\n\n    # Set if labels are dynamic (zero shot) or fixed (standard text classification)\n    self.dynamic = dynamic\n</code></pre>"},{"location":"pipeline/text/labels/#txtai.pipeline.Labels.__call__","title":"<code>__call__(text, labels=None, multilabel=False, flatten=None, workers=0, **kwargs)</code>","text":"<p>Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned.</p> <p>This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <code>labels</code> <p>list of labels</p> <code>None</code> <code>multilabel</code> <p>labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None</p> <code>False</code> <code>flatten</code> <p>flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number.</p> <code>None</code> <code>workers</code> <p>number of concurrent workers to use for processing data, defaults to None</p> <code>0</code> <code>kwargs</code> <p>additional keyword args</p> <code>{}</code> <p>Returns:</p> Type Description <p>list of (id, score) or list of labels depending on flatten parameter</p> Source code in <code>txtai/pipeline/text/labels.py</code> <pre><code>def __call__(self, text, labels=None, multilabel=False, flatten=None, workers=0, **kwargs):\n    \"\"\"\n    Applies a text classifier to text. Returns a list of (id, score) sorted by highest score,\n    where id is the index in labels. For zero shot classification, a list of labels is required.\n    For text classification models, a list of labels is optional, otherwise all trained labels are returned.\n\n    This method supports text as a string or a list. If the input is a string, the return\n    type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is\n    returned with a row per string.\n\n    Args:\n        text: text|list\n        labels: list of labels\n        multilabel: labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None\n        flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number.\n        workers: number of concurrent workers to use for processing data, defaults to None\n        kwargs: additional keyword args\n\n    Returns:\n        list of (id, score) or list of labels depending on flatten parameter\n    \"\"\"\n\n    if self.dynamic:\n        # Run zero shot classification pipeline\n        results = self.pipeline(text, labels, multi_label=multilabel, truncation=True, num_workers=workers)\n    else:\n        # Set classification function based on inputs\n        function = \"none\" if multilabel is None else \"sigmoid\" if multilabel or len(self.labels()) == 1 else \"softmax\"\n\n        # Run text classification pipeline\n        results = self.pipeline(text, top_k=None, function_to_apply=function, num_workers=workers, **kwargs)\n\n    # Convert results to a list if necessary\n    if isinstance(text, str):\n        results = [results]\n\n    # Build list of outputs and return\n    outputs = self.outputs(results, labels, flatten)\n    return outputs[0] if isinstance(text, str) else outputs\n</code></pre>"},{"location":"pipeline/text/reranker/","title":"Reranker","text":"<p>The Reranker pipeline runs embeddings queries and re-ranks them using a similarity pipeline. </p>"},{"location":"pipeline/text/reranker/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai import Embeddings\nfrom txtai.pipeline import Reranker, Similarity\n\n# Embeddings instance\nembeddings = Embeddings()\nembeddings.load(provider=\"huggingface-hub\", container=\"neuml/txtai-wikipedia\")\n\n# Similarity instance\nsimilarity = Similarity(path=\"colbert-ir/colbertv2.0\", lateencode=True)\n\n# Reranking pipeline\nreranker = Reranker(embeddings, similarity)\nreranker(\"Tell me about AI\")\n</code></pre> <p>Note: Content must be enabled with the embeddings instance for this to work properly.</p> <p>See the link below for a more detailed example.</p> Notebook Description What's new in txtai 9.0 Learned sparse vectors, late interaction models and rerankers"},{"location":"pipeline/text/reranker/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/reranker/#configyml","title":"config.yml","text":"<pre><code>embeddings:\n\nsimilarity:\n\n# Create pipeline using lower case class name\nreranker:\n\n# Run pipeline with workflow\nworkflow:\n  translate:\n    tasks:\n      - reranker\n</code></pre>"},{"location":"pipeline/text/reranker/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"reranker\", [\"Tell me about AI\"]))\n</code></pre>"},{"location":"pipeline/text/reranker/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"rerank\", \"elements\":[\"Tell me about AI\"]}'\n</code></pre>"},{"location":"pipeline/text/reranker/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/reranker/#txtai.pipeline.Reranker.__init__","title":"<code>__init__(embeddings, similarity)</code>","text":"<p>Creates a Reranker pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <p>embeddings instance (content must be enabled)</p> required <code>similarity</code> <p>similarity instance</p> required Source code in <code>txtai/pipeline/text/reranker.py</code> <pre><code>def __init__(self, embeddings, similarity):\n    \"\"\"\n    Creates a Reranker pipeline.\n\n    Args:\n        embeddings: embeddings instance (content must be enabled)\n        similarity: similarity instance\n    \"\"\"\n\n    self.embeddings, self.similarity = embeddings, similarity\n</code></pre>"},{"location":"pipeline/text/reranker/#txtai.pipeline.Reranker.__call__","title":"<code>__call__(query, limit=3, factor=10, **kwargs)</code>","text":"<p>Runs an embeddings search and re-ranks the results using a Similarity pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>query text|list</p> required <code>limit</code> <p>maximum results</p> <code>3</code> <code>factor</code> <p>factor to multiply limit by for the initial embeddings search</p> <code>10</code> <code>kwargs</code> <p>additional arguments to pass to embeddings search</p> <code>{}</code> <p>Returns:</p> Type Description <p>list of query results rescored using a Similarity pipeline</p> Source code in <code>txtai/pipeline/text/reranker.py</code> <pre><code>def __call__(self, query, limit=3, factor=10, **kwargs):\n    \"\"\"\n    Runs an embeddings search and re-ranks the results using a Similarity pipeline.\n\n    Args:\n        query: query text|list\n        limit: maximum results\n        factor: factor to multiply limit by for the initial embeddings search\n        kwargs: additional arguments to pass to embeddings search\n\n    Returns:\n        list of query results rescored using a Similarity pipeline\n    \"\"\"\n\n    queries = [query] if not isinstance(query, list) else query\n\n    # Run searches\n    results = self.embeddings.batchsearch(queries, limit * factor, **kwargs)\n\n    # Re-rank using similarity pipeline\n    ranked = []\n    for x, result in enumerate(results):\n        texts = [row[\"text\"] for row in result]\n\n        # Score results and merge\n        for uid, score in self.similarity(queries[x], texts):\n            result[uid][\"score\"] = score\n\n        # Sort and take top n sorted results\n        ranked.append(sorted(result, key=lambda row: row[\"score\"], reverse=True)[:limit])\n\n    return ranked[0] if isinstance(query, str) else ranked\n</code></pre>"},{"location":"pipeline/text/similarity/","title":"Similarity","text":"<p>The Similarity pipeline computes similarity between queries and list of text using a text classifier.</p> <p>This pipeline supports both standard text classification models and zero-shot classification models. The pipeline uses the queries as labels for the input text. The results are transposed to get scores per query/label vs scores per input text. </p> <p>Cross-encoder models are supported via the <code>crossencode=True</code> constructor parameter. Late interaction (i.e. ColBERT) models are also supported via the <code>lateencode=True</code> constructor parameter. CrossEncoder and LateEncoder pipelines back each of these models and can be instantiated directly as well.</p>"},{"location":"pipeline/text/similarity/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Similarity\n\n# Create and run pipeline\nsimilarity = Similarity()\nsimilarity(\"feel good story\", [\n    \"Maine man wins $1M from $25 lottery ticket\", \n    \"Don't sacrifice slower friends in a bear attack\"\n])\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Add semantic search to Elasticsearch Add semantic search to existing search systems"},{"location":"pipeline/text/similarity/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/similarity/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nsimilarity:\n</code></pre>"},{"location":"pipeline/text/similarity/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\napp.similarity(\"feel good story\", [\n    \"Maine man wins $1M from $25 lottery ticket\", \n    \"Don't sacrifice slower friends in a bear attack\"\n])\n</code></pre>"},{"location":"pipeline/text/similarity/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/similarity\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"feel good story\", \"texts\": [\"Maine man wins $1M from $25 lottery ticket\", \"Dont sacrifice slower friends in a bear attack\"]}'\n</code></pre>"},{"location":"pipeline/text/similarity/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/similarity/#txtai.pipeline.Similarity.__init__","title":"<code>__init__(path=None, quantize=False, gpu=True, model=None, dynamic=True, crossencode=False, lateencode=False, **kwargs)</code>","text":"Source code in <code>txtai/pipeline/text/similarity.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None, dynamic=True, crossencode=False, lateencode=False, **kwargs):\n    self.crossencoder, self.lateencoder = None, None\n\n    if lateencode:\n        # Load a late interaction encoder if lateencode set to True\n        self.lateencoder = LateEncoder(path=path, gpu=gpu, **kwargs)\n    else:\n        # Use zero-shot classification if dynamic is True and crossencode is False, otherwise use standard text classification\n        super().__init__(path, quantize, gpu, model, False if crossencode else dynamic, **kwargs)\n\n        # Load as a cross-encoder if crossencode set to True\n        self.crossencoder = CrossEncoder(model=self.pipeline) if crossencode else None\n</code></pre>"},{"location":"pipeline/text/similarity/#txtai.pipeline.Similarity.__call__","title":"<code>__call__(query, texts, multilabel=True, **kwargs)</code>","text":"<p>Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts.</p> <p>This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>query text|list</p> required <code>texts</code> <p>list of text</p> required <code>multilabel</code> <p>labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None</p> <code>True</code> <code>kwargs</code> <p>additional keyword args</p> <code>{}</code> <p>Returns:</p> Type Description <p>list of (id, score)</p> Source code in <code>txtai/pipeline/text/similarity.py</code> <pre><code>def __call__(self, query, texts, multilabel=True, **kwargs):\n    \"\"\"\n    Computes the similarity between query and list of text. Returns a list of\n    (id, score) sorted by highest score, where id is the index in texts.\n\n    This method supports query as a string or a list. If the input is a string,\n    the return type is a 1D list of (id, score). If text is a list, a 2D list\n    of (id, score) is returned with a row per string.\n\n    Args:\n        query: query text|list\n        texts: list of text\n        multilabel: labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None\n        kwargs: additional keyword args\n\n    Returns:\n        list of (id, score)\n    \"\"\"\n\n    if self.crossencoder:\n        # pylint: disable=E1102\n        return self.crossencoder(query, texts, multilabel)\n\n    if self.lateencoder:\n        return self.lateencoder(query, texts)\n\n    # Call Labels pipeline for texts using input query as the candidate label\n    scores = super().__call__(texts, [query] if isinstance(query, str) else query, multilabel, **kwargs)\n\n    # Sort on query index id\n    scores = [[score for _, score in sorted(row)] for row in scores]\n\n    # Transpose axes to get a list of text scores for each query\n    scores = np.array(scores).T.tolist()\n\n    # Build list of (id, score) per query sorted by highest score\n    scores = [sorted(enumerate(row), key=lambda x: x[1], reverse=True) for row in scores]\n\n    return scores[0] if isinstance(query, str) else scores\n</code></pre>"},{"location":"pipeline/text/summary/","title":"Summary","text":"<p>The Summary pipeline summarizes text. This pipeline runs a text2text model that abstractively creates a summary of the input text.</p>"},{"location":"pipeline/text/summary/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Summary\n\n# Create and run pipeline\nsummary = Summary()\nsummary(\"Enter long, detailed text to summarize here\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Building abstractive text summaries Run abstractive text summarization"},{"location":"pipeline/text/summary/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/summary/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nsummary:\n\n# Run pipeline with workflow\nworkflow:\n  summary:\n    tasks:\n      - action: summary\n</code></pre>"},{"location":"pipeline/text/summary/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"summary\", [\"Enter long, detailed text to summarize here\"]))\n</code></pre>"},{"location":"pipeline/text/summary/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"summary\", \"elements\":[\"Enter long, detailed text to summarize here\"]}'\n</code></pre>"},{"location":"pipeline/text/summary/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/summary/#txtai.pipeline.Summary.__init__","title":"<code>__init__(path=None, quantize=False, gpu=True, model=None, **kwargs)</code>","text":"Source code in <code>txtai/pipeline/text/summary.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None, **kwargs):\n    super().__init__(\"summarization\", path, quantize, gpu, model, **kwargs)\n</code></pre>"},{"location":"pipeline/text/summary/#txtai.pipeline.Summary.__call__","title":"<code>__call__(text, minlength=None, maxlength=None, workers=0)</code>","text":"<p>Runs a summarization model against a block of text.</p> <p>This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <code>minlength</code> <p>minimum length for summary</p> <code>None</code> <code>maxlength</code> <p>maximum length for summary</p> <code>None</code> <code>workers</code> <p>number of concurrent workers to use for processing data, defaults to None</p> <code>0</code> <p>Returns:</p> Type Description <p>summary text</p> Source code in <code>txtai/pipeline/text/summary.py</code> <pre><code>def __call__(self, text, minlength=None, maxlength=None, workers=0):\n    \"\"\"\n    Runs a summarization model against a block of text.\n\n    This method supports text as a string or a list. If the input is a string, the return\n    type is text. If text is a list, a list of text is returned with a row per block of text.\n\n    Args:\n        text: text|list\n        minlength: minimum length for summary\n        maxlength: maximum length for summary\n        workers: number of concurrent workers to use for processing data, defaults to None\n\n    Returns:\n        summary text\n    \"\"\"\n\n    # Validate text length greater than max length\n    check = maxlength if maxlength else self.maxlength()\n\n    # Skip text shorter than max length\n    texts = text if isinstance(text, list) else [text]\n    params = [(x, text if len(text) &gt;= check else None) for x, text in enumerate(texts)]\n\n    # Build keyword arguments\n    kwargs = self.args(minlength, maxlength)\n\n    inputs = [text for _, text in params if text]\n    if inputs:\n        # Run summarization pipeline\n        results = self.pipeline(inputs, num_workers=workers, **kwargs)\n\n        # Pull out summary text\n        results = iter([self.clean(x[\"summary_text\"]) for x in results])\n        results = [next(results) if text else texts[x] for x, text in params]\n    else:\n        # Return original\n        results = texts\n\n    return results[0] if isinstance(text, str) else results\n</code></pre>"},{"location":"pipeline/text/translation/","title":"Translation","text":"<p>The Translation pipeline translates text between languages. It supports over 100+ languages. Automatic source language detection is built-in. This pipeline detects the language of each input text row, loads a model for the source-target combination and translates text to the target language.</p>"},{"location":"pipeline/text/translation/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Translation\n\n# Create and run pipeline\ntranslate = Translation()\ntranslate(\"This is a test translation into Spanish\", \"es\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Translate text between languages Streamline machine translation and language detection"},{"location":"pipeline/text/translation/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/translation/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ntranslation:\n\n# Run pipeline with workflow\nworkflow:\n  translate:\n    tasks:\n      - action: translation\n        args: [\"es\"]\n</code></pre>"},{"location":"pipeline/text/translation/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"translate\", [\"This is a test translation into Spanish\"]))\n</code></pre>"},{"location":"pipeline/text/translation/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n  -X POST \"http://localhost:8000/workflow\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"translate\", \"elements\":[\"This is a test translation into Spanish\"]}'\n</code></pre>"},{"location":"pipeline/text/translation/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/translation/#txtai.pipeline.Translation.__init__","title":"<code>__init__(path=None, quantize=False, gpu=True, batch=64, langdetect=None, findmodels=True)</code>","text":"<p>Constructs a new language translation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>optional path to model, accepts Hugging Face model hub id or local path,   uses default model for task if not provided</p> <code>None</code> <code>quantize</code> <p>if model should be quantized, defaults to False</p> <code>False</code> <code>gpu</code> <p>True/False if GPU should be enabled, also supports a GPU device id</p> <code>True</code> <code>batch</code> <p>batch size used to incrementally process content</p> <code>64</code> <code>langdetect</code> <p>set a custom language detection function, method must take a list of strings and return         language codes for each, uses default language detector if not provided</p> <code>None</code> <code>findmodels</code> <p>True/False if the Hugging Face Hub will be searched for source-target translation models</p> <code>True</code> Source code in <code>txtai/pipeline/text/translation.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, batch=64, langdetect=None, findmodels=True):\n    \"\"\"\n    Constructs a new language translation pipeline.\n\n    Args:\n        path: optional path to model, accepts Hugging Face model hub id or local path,\n              uses default model for task if not provided\n        quantize: if model should be quantized, defaults to False\n        gpu: True/False if GPU should be enabled, also supports a GPU device id\n        batch: batch size used to incrementally process content\n        langdetect: set a custom language detection function, method must take a list of strings and return\n                    language codes for each, uses default language detector if not provided\n        findmodels: True/False if the Hugging Face Hub will be searched for source-target translation models\n    \"\"\"\n\n    # Call parent constructor\n    super().__init__(path if path else \"facebook/m2m100_418M\", quantize, gpu, batch)\n\n    # Language detection\n    self.detector = None\n    self.langdetect = langdetect\n    self.findmodels = findmodels\n\n    # Language models\n    self.models = {}\n    self.ids = None\n</code></pre>"},{"location":"pipeline/text/translation/#txtai.pipeline.Translation.__call__","title":"<code>__call__(texts, target='en', source=None, showmodels=False)</code>","text":"<p>Translates text from source language into target language.</p> <p>This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <p>text|list</p> required <code>target</code> <p>target language code, defaults to \"en\"</p> <code>'en'</code> <code>source</code> <p>source language code, detects language if not provided</p> <code>None</code> <p>Returns:</p> Type Description <p>list of translated text</p> Source code in <code>txtai/pipeline/text/translation.py</code> <pre><code>def __call__(self, texts, target=\"en\", source=None, showmodels=False):\n    \"\"\"\n    Translates text from source language into target language.\n\n    This method supports texts as a string or a list. If the input is a string,\n    the return type is string. If text is a list, the return type is a list.\n\n    Args:\n        texts: text|list\n        target: target language code, defaults to \"en\"\n        source: source language code, detects language if not provided\n\n    Returns:\n        list of translated text\n    \"\"\"\n\n    values = [texts] if not isinstance(texts, list) else texts\n\n    # Detect source languages\n    languages = self.detect(values) if not source else [source] * len(values)\n    unique = set(languages)\n\n    # Build a dict from language to list of (index, text)\n    langdict = {}\n    for x, lang in enumerate(languages):\n        if lang not in langdict:\n            langdict[lang] = []\n        langdict[lang].append((x, values[x]))\n\n    results = {}\n    for language in unique:\n        # Get all indices and text values for a language\n        inputs = langdict[language]\n\n        # Translate text in batches\n        outputs = []\n        for chunk in self.batch([text for _, text in inputs], self.batchsize):\n            outputs.extend(self.translate(chunk, language, target, showmodels))\n\n        # Store output value\n        for y, (x, _) in enumerate(inputs):\n            if showmodels:\n                model, op = outputs[y]\n                results[x] = (op.strip(), language, model)\n            else:\n                results[x] = outputs[y].strip()\n\n    # Return results in same order as input\n    results = [results[x] for x in sorted(results)]\n    return results[0] if isinstance(texts, str) else results\n</code></pre>"},{"location":"pipeline/train/hfonnx/","title":"HFOnnx","text":"<p>Exports a Hugging Face Transformer model to ONNX. Currently, this works best with classification/pooling/qa models. Work is ongoing for sequence to sequence models (summarization, transcription, translation).</p>"},{"location":"pipeline/train/hfonnx/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import HFOnnx, Labels\n\n# Model path\npath = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n# Export model to ONNX\nonnx = HFOnnx()\nmodel = onnx(path, \"text-classification\", \"model.onnx\", True)\n\n# Run inference and validate\nlabels = Labels((model, path), dynamic=False)\nlabels(\"I am happy\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Export and run models with ONNX Export models with ONNX, run natively in JavaScript, Java and Rust"},{"location":"pipeline/train/hfonnx/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/train/hfonnx/#txtai.pipeline.HFOnnx.__call__","title":"<code>__call__(path, task='default', output=None, quantize=False, opset=14)</code>","text":"<p>Exports a Hugging Face Transformer model to ONNX.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple</p> required <code>task</code> <p>optional model task or category, determines the model type and outputs, defaults to export hidden state</p> <code>'default'</code> <code>output</code> <p>optional output model path, defaults to return byte array if None</p> <code>None</code> <code>quantize</code> <p>if model should be quantized (requires onnx to be installed), defaults to False</p> <code>False</code> <code>opset</code> <p>onnx opset, defaults to 14</p> <code>14</code> <p>Returns:</p> Type Description <p>path to model output or model as bytes depending on output parameter</p> Source code in <code>txtai/pipeline/train/hfonnx.py</code> <pre><code>def __call__(self, path, task=\"default\", output=None, quantize=False, opset=14):\n    \"\"\"\n    Exports a Hugging Face Transformer model to ONNX.\n\n    Args:\n        path: path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple\n        task: optional model task or category, determines the model type and outputs, defaults to export hidden state\n        output: optional output model path, defaults to return byte array if None\n        quantize: if model should be quantized (requires onnx to be installed), defaults to False\n        opset: onnx opset, defaults to 14\n\n    Returns:\n        path to model output or model as bytes depending on output parameter\n    \"\"\"\n\n    inputs, outputs, model = self.parameters(task)\n\n    if isinstance(path, (list, tuple)):\n        model, tokenizer = path\n        model = model.cpu()\n    else:\n        model = model(path)\n        tokenizer = AutoTokenizer.from_pretrained(path)\n\n    # Generate dummy inputs\n    dummy = dict(tokenizer([\"test inputs\"], return_tensors=\"pt\"))\n\n    # Default to BytesIO if no output file provided\n    output = output if output else BytesIO()\n\n    # Export model to ONNX\n    export(\n        model,\n        (dummy,),\n        output,\n        opset_version=opset,\n        do_constant_folding=True,\n        input_names=list(inputs.keys()),\n        output_names=list(outputs.keys()),\n        dynamic_axes=dict(chain(inputs.items(), outputs.items())),\n        dynamo=False,\n    )\n\n    # Quantize model\n    if quantize:\n        if not ONNX_RUNTIME:\n            raise ImportError('onnxruntime is not available - install \"pipeline\" extra to enable')\n\n        output = self.quantization(output)\n\n    if isinstance(output, BytesIO):\n        # Reset stream and return bytes\n        output.seek(0)\n        output = output.read()\n\n    return output\n</code></pre>"},{"location":"pipeline/train/mlonnx/","title":"MLOnnx","text":"<p>Exports a traditional machine learning model (i.e. scikit-learn) to ONNX.</p>"},{"location":"pipeline/train/mlonnx/#example","title":"Example","text":"<p>See the link below for a detailed example.</p> Notebook Description Export and run other machine learning models Export and run models from scikit-learn, PyTorch and more"},{"location":"pipeline/train/mlonnx/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/train/mlonnx/#txtai.pipeline.MLOnnx.__call__","title":"<code>__call__(model, task='default', output=None, opset=12)</code>","text":"<p>Exports a machine learning model to ONNX using ONNXMLTools.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>model to export</p> required <code>task</code> <p>optional model task or category</p> <code>'default'</code> <code>output</code> <p>optional output model path, defaults to return byte array if None</p> <code>None</code> <code>opset</code> <p>onnx opset, defaults to 12</p> <code>12</code> <p>Returns:</p> Type Description <p>path to model output or model as bytes depending on output parameter</p> Source code in <code>txtai/pipeline/train/mlonnx.py</code> <pre><code>def __call__(self, model, task=\"default\", output=None, opset=12):\n    \"\"\"\n    Exports a machine learning model to ONNX using ONNXMLTools.\n\n    Args:\n        model: model to export\n        task: optional model task or category\n        output: optional output model path, defaults to return byte array if None\n        opset: onnx opset, defaults to 12\n\n    Returns:\n        path to model output or model as bytes depending on output parameter\n    \"\"\"\n\n    # Convert scikit-learn model to ONNX\n    model = convert_sklearn(model, task, initial_types=[(\"input_ids\", StringTensorType([None, None]))], target_opset=opset)\n\n    # Prune model graph down to only output probabilities\n    model = select_model_inputs_outputs(model, outputs=\"probabilities\")\n\n    # pylint: disable=E1101\n    # Rename output to logits for consistency with other models\n    model.graph.output[0].name = \"logits\"\n\n    # Find probabilities output node and rename to logits\n    for node in model.graph.node:\n        for x, _ in enumerate(node.output):\n            if node.output[x] == \"probabilities\":\n                node.output[x] = \"logits\"\n\n    # Save model to specified output path or return bytes\n    model = save_onnx_model(model, output)\n    return output if output else model\n</code></pre>"},{"location":"pipeline/train/trainer/","title":"HFTrainer","text":"<p>Trains a new Hugging Face Transformer model using the Trainer framework.</p>"},{"location":"pipeline/train/trainer/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>import pandas as pd\n\nfrom datasets import load_dataset\n\nfrom txtai.pipeline import HFTrainer\n\ntrainer = HFTrainer()\n\n# Pandas DataFrame\ndf = pd.read_csv(\"training.csv\")\nmodel, tokenizer = trainer(\"bert-base-uncased\", df)\n\n# Hugging Face dataset\nds = load_dataset(\"glue\", \"sst2\")\nmodel, tokenizer = trainer(\"bert-base-uncased\", ds[\"train\"], columns=(\"sentence\", \"label\"))\n\n# List of dicts\ndt = [{\"text\": \"sentence 1\", \"label\": 0}, {\"text\": \"sentence 2\", \"label\": 1}]]\nmodel, tokenizer = trainer(\"bert-base-uncased\", dt)\n\n# Support additional TrainingArguments\nmodel, tokenizer = trainer(\"bert-base-uncased\", dt, \n                            learning_rate=3e-5, num_train_epochs=5)\n</code></pre> <p>All TrainingArguments are supported as function arguments to the trainer call.</p> <p>See the links below for more detailed examples.</p> Notebook Description Train a text labeler Build text sequence classification models Train without labels Use zero-shot classifiers to train new models Train a QA model Build and fine-tune question-answering models Train a language model from scratch Build new language models"},{"location":"pipeline/train/trainer/#training-tasks","title":"Training tasks","text":"<p>The HFTrainer pipeline builds and/or fine-tunes models for following training tasks.</p> Task Description language-generation Causal language model for text generation (e.g. GPT) language-modeling Masked language model for general tasks (e.g. BERT) question-answering Extractive question-answering model, typically with the SQuAD dataset sequence-sequence Sequence-Sequence model (e.g. T5) text-classification Classify text with a set of labels token-detection ELECTRA-style pre-training with replaced token detection"},{"location":"pipeline/train/trainer/#peft","title":"PEFT","text":"<p>Parameter-Efficient Fine-Tuning (PEFT) is supported through Hugging Face's PEFT library. Quantization is provided through bitsandbytes. See the examples below.</p> <pre><code>from txtai.pipeline import HFTrainer\n\ntrainer = HFTrainer()\ntrainer(..., quantize=True, lora=True)\n</code></pre> <p>When these parameters are set to True, they use default configuration. This can also be customized.</p> <pre><code>quantize = {\n    \"load_in_4bit\": True,\n    \"bnb_4bit_use_double_quant\": True,\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_compute_dtype\": \"bfloat16\"\n}\n\nlora = {\n    \"r\": 16,\n    \"lora_alpha\": 8,\n    \"target_modules\": \"all-linear\",\n    \"lora_dropout\": 0.05,\n    \"bias\": \"none\"\n}\n\ntrainer(..., quantize=quantize, lora=lora)\n</code></pre> <p>The parameters also accept <code>transformers.BitsAndBytesConfig</code> and <code>peft.LoraConfig</code> instances.</p> <p>See the following PEFT documentation links for more information.</p> <ul> <li>Quantization</li> <li>LoRA</li> </ul>"},{"location":"pipeline/train/trainer/#merge","title":"Merge","text":"<p>An important parameter for <code>language-generation</code> and <code>language-modeling</code> tasks is <code>merge</code> or the packing of data into chunks.</p> <p>It supports the following options.</p> <ul> <li><code>concat</code> (default) - text is split into chunks up to maxlength, data can be split across multiple chunks</li> <li><code>pack</code> - text is split into chunks up to maxlength, data guaranteed to be in same chunk, chunks can be smaller than maxlength</li> <li><code>None</code> - disables merging</li> </ul> <p>Merging helps reduce training time as data can be processed efficiently without padding. <code>concat</code> maximizes this as it guarantees each chunk will be up to maxlength size. <code>pack</code> is a middle ground where data is combined but records are preserved.</p> <p>For general language modeling tasks like masked language modeling, <code>concat</code> is the best choice. For instruction/prompt fine-tuning, <code>pack</code> or None are the better choices as it guarantees complex logic is not split across chunks.</p>"},{"location":"pipeline/train/trainer/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/train/trainer/#txtai.pipeline.HFTrainer.__call__","title":"<code>__call__(base, train, validation=None, columns=None, maxlength=None, stride=128, task='text-classification', prefix=None, metrics=None, tokenizers=None, checkpoint=None, quantize=None, lora=None, merge='concat', **args)</code>","text":"<p>Builds a new model using arguments.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <p>path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple</p> required <code>train</code> <p>training data</p> required <code>validation</code> <p>validation data</p> <code>None</code> <code>columns</code> <p>tuple of columns to use for text/label, defaults to (text, None, label)</p> <code>None</code> <code>maxlength</code> <p>maximum sequence length, defaults to tokenizer.model_max_length</p> <code>None</code> <code>stride</code> <p>chunk size for splitting data for QA tasks</p> <code>128</code> <code>task</code> <p>optional model task or category, determines the model type, defaults to \"text-classification\"</p> <code>'text-classification'</code> <code>prefix</code> <p>optional source prefix</p> <code>None</code> <code>metrics</code> <p>optional function that computes and returns a dict of evaluation metrics</p> <code>None</code> <code>tokenizers</code> <p>optional number of concurrent tokenizers, defaults to None</p> <code>None</code> <code>checkpoint</code> <p>optional resume from checkpoint flag or path to checkpoint directory, defaults to None</p> <code>None</code> <code>quantize</code> <p>quantization configuration to pass to base model</p> <code>None</code> <code>lora</code> <p>lora configuration to pass to PEFT model</p> <code>None</code> <code>merge</code> <p>determines how chunks are combined for language modeling tasks - \"concat\" (default), \"pack\" or None</p> <code>'concat'</code> <code>args</code> <p>training arguments</p> <code>{}</code> <p>Returns:</p> Type Description <p>(model, tokenizer)</p> Source code in <code>txtai/pipeline/train/hftrainer.py</code> <pre><code>def __call__(\n    self,\n    base,\n    train,\n    validation=None,\n    columns=None,\n    maxlength=None,\n    stride=128,\n    task=\"text-classification\",\n    prefix=None,\n    metrics=None,\n    tokenizers=None,\n    checkpoint=None,\n    quantize=None,\n    lora=None,\n    merge=\"concat\",\n    **args\n):\n    \"\"\"\n    Builds a new model using arguments.\n\n    Args:\n        base: path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple\n        train: training data\n        validation: validation data\n        columns: tuple of columns to use for text/label, defaults to (text, None, label)\n        maxlength: maximum sequence length, defaults to tokenizer.model_max_length\n        stride: chunk size for splitting data for QA tasks\n        task: optional model task or category, determines the model type, defaults to \"text-classification\"\n        prefix: optional source prefix\n        metrics: optional function that computes and returns a dict of evaluation metrics\n        tokenizers: optional number of concurrent tokenizers, defaults to None\n        checkpoint: optional resume from checkpoint flag or path to checkpoint directory, defaults to None\n        quantize: quantization configuration to pass to base model\n        lora: lora configuration to pass to PEFT model\n        merge: determines how chunks are combined for language modeling tasks - \"concat\" (default), \"pack\" or None\n        args: training arguments\n\n    Returns:\n        (model, tokenizer)\n    \"\"\"\n\n    # Quantization / LoRA support\n    if (quantize or lora) and not PEFT:\n        raise ImportError('PEFT is not available - install \"pipeline\" extra to enable')\n\n    # Parse TrainingArguments\n    args = self.parse(args)\n\n    # Set seed for model reproducibility\n    set_seed(args.seed)\n\n    # Load model configuration, tokenizer and max sequence length\n    config, tokenizer, maxlength = self.load(base, maxlength)\n\n    # Default tokenizer pad token if it's not set\n    tokenizer.pad_token = tokenizer.pad_token if tokenizer.pad_token is not None else tokenizer.eos_token\n\n    # Prepare parameters\n    process, collator, labels = self.prepare(task, train, tokenizer, columns, maxlength, stride, prefix, merge, args)\n\n    # Tokenize training and validation data\n    train, validation = process(train, validation, os.cpu_count() if tokenizers and isinstance(tokenizers, bool) else tokenizers)\n\n    # Create model to train\n    model = self.model(task, base, config, labels, tokenizer, quantize)\n\n    # Default config pad token if it's not set\n    model.config.pad_token_id = model.config.pad_token_id if model.config.pad_token_id is not None else model.config.eos_token_id\n\n    # Load as PEFT model, if necessary\n    model = self.peft(task, lora, model)\n\n    # Add model to collator\n    if collator:\n        collator.model = model\n\n    # Build trainer\n    trainer = Trainer(\n        model=model,\n        processing_class=tokenizer,\n        data_collator=collator,\n        args=args,\n        train_dataset=train,\n        eval_dataset=validation if validation else None,\n        compute_metrics=metrics,\n    )\n\n    # Run training\n    trainer.train(resume_from_checkpoint=checkpoint)\n\n    # Run evaluation\n    if validation:\n        trainer.evaluate()\n\n    # Save model outputs\n    if args.should_save:\n        trainer.save_model()\n        trainer.save_state()\n\n    # Put model in eval mode to disable weight updates and return (model, tokenizer)\n    return (model.eval(), tokenizer)\n</code></pre>"},{"location":"workflow/","title":"Workflow","text":"<p>Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows operate well with pipelines but can work with any callable object. Workflows are streaming and work on data in batches, allowing large volumes of data to be processed efficiently.</p> <p>Given that pipelines are callable objects, workflows enable efficient processing of pipeline data. Large language models typically work with smaller batches of data, workflows are well suited to feed a series of transformers pipelines. </p> <p>An example of the most basic workflow:</p> <pre><code>workflow = Workflow([Task(lambda x: [y * 2 for y in x])])\nlist(workflow([1, 2, 3]))\n</code></pre> <p>This example multiplies each input value by 2 and returns transformed elements via a generator.</p> <p>Since workflows run as generators, output must be consumed for execution to occur. The following snippets show how output can be consumed.</p> <pre><code># Small dataset where output fits in memory\nlist(workflow(elements))\n\n# Large dataset\nfor output in workflow(elements):\n    function(output)\n\n# Large dataset where output is discarded\nfor _ in workflow(elements):\n    pass\n</code></pre> <p>Workflows are run with Python or configuration. Examples of both methods are shown below.</p>"},{"location":"workflow/#example","title":"Example","text":"<p>A full-featured example is shown below in Python. This workflow transcribes a set of audio files, translates the text into French and indexes the data.</p> <pre><code>from txtai import Embeddings\nfrom txtai.pipeline import Transcription, Translation\nfrom txtai.workflow import FileTask, Task, Workflow\n\n# Embeddings instance\nembeddings = Embeddings({\n    \"path\": \"sentence-transformers/paraphrase-MiniLM-L3-v2\",\n    \"content\": True\n})\n\n# Transcription instance\ntranscribe = Transcription()\n\n# Translation instance\ntranslate = Translation()\n\ntasks = [\n    FileTask(transcribe, r\"\\.wav$\"),\n    Task(lambda x: translate(x, \"fr\"))\n]\n\n# List of files to process\ndata = [\n  \"US_tops_5_million.wav\",\n  \"Canadas_last_fully.wav\",\n  \"Beijing_mobilises.wav\",\n  \"The_National_Park.wav\",\n  \"Maine_man_wins_1_mil.wav\",\n  \"Make_huge_profits.wav\"\n]\n\n# Workflow that translate text to French\nworkflow = Workflow(tasks)\n\n# Index data\nembeddings.index((uid, text, None) for uid, text in enumerate(workflow(data)))\n\n# Search\nembeddings.search(\"wildlife\", 1)\n</code></pre>"},{"location":"workflow/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Workflows can also be defined with YAML configuration.</p> <pre><code>writable: true\nembeddings:\n  path: sentence-transformers/paraphrase-MiniLM-L3-v2\n  content: true\n\n# Transcribe audio to text\ntranscription:\n\n# Translate text between languages\ntranslation:\n\nworkflow:\n  index:\n    tasks:\n      - action: transcription\n        select: \"\\\\.wav$\"\n        task: file\n      - action: translation\n        args: [\"fr\"]\n      - action: index\n</code></pre> <pre><code># Create and run the workflow\nfrom txtai import Application\n\n# Create and run the workflow\napp = Application(\"workflow.yml\")\nlist(app.workflow(\"index\", [\n  \"US_tops_5_million.wav\",\n  \"Canadas_last_fully.wav\",\n  \"Beijing_mobilises.wav\",\n  \"The_National_Park.wav\",\n  \"Maine_man_wins_1_mil.wav\",\n  \"Make_huge_profits.wav\"\n]))\n\n# Search\napp.search(\"wildlife\")\n</code></pre> <p>The code above executes a workflow defined in the file `workflow.yml.</p>"},{"location":"workflow/#llm-workflow-example","title":"LLM workflow example","text":"<p>Workflows can connect multiple LLM prompting tasks together.</p> <pre><code>llm:\n  path: openai/gpt-oss-20b\n\nworkflow:\n  llm:\n    tasks:\n      - task: template\n        template: |\n          Extract keywords for the following text.\n\n          {text}\n        action: llm\n      - task: template\n        template: |\n          Translate the following text into French.\n\n          {text}\n        action: llm\n</code></pre> <pre><code>from txtai import Application\n\napp = Application(\"workflow.yml\")\nlist(app.workflow(\"llm\", [\n  \"\"\"\n  txtai is an open-source platform for semantic search\n  and workflows powered by language models.\n  \"\"\"\n]))\n</code></pre> <p>Any txtai pipeline/workflow task can be connected in workflows with LLMs.</p> <pre><code>llm:\n  path: openai/gpt-oss-20b\n\ntranslation:\n\nworkflow:\n  llm:\n    tasks:\n      - task: template\n        template: |\n          Extract keywords for the following text.\n\n          {text}\n        action: llm\n      - action: translation\n        args:\n          - fr\n</code></pre> <p>See the following links for more information.</p> <ul> <li>Workflow Demo</li> <li>Workflow YAML Examples</li> <li>Workflow YAML Guide</li> </ul>"},{"location":"workflow/#methods","title":"Methods","text":"<p>Workflows are callable objects. Workflows take an input of iterable data elements and output iterable data elements. </p>"},{"location":"workflow/#txtai.workflow.Workflow.__init__","title":"<code>__init__(tasks, batch=100, workers=None, name=None, stream=None)</code>","text":"<p>Creates a new workflow. Workflows are lists of tasks to execute.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <p>list of workflow tasks</p> required <code>batch</code> <p>how many items to process at a time, defaults to 100</p> <code>100</code> <code>workers</code> <p>number of concurrent workers</p> <code>None</code> <code>name</code> <p>workflow name</p> <code>None</code> <code>stream</code> <p>workflow stream processor</p> <code>None</code> Source code in <code>txtai/workflow/base.py</code> <pre><code>def __init__(self, tasks, batch=100, workers=None, name=None, stream=None):\n    \"\"\"\n    Creates a new workflow. Workflows are lists of tasks to execute.\n\n    Args:\n        tasks: list of workflow tasks\n        batch: how many items to process at a time, defaults to 100\n        workers: number of concurrent workers\n        name: workflow name\n        stream: workflow stream processor\n    \"\"\"\n\n    self.tasks = tasks\n    self.batch = batch\n    self.workers = workers\n    self.name = name\n    self.stream = stream\n\n    # Set default number of executor workers to max number of actions in a task\n    self.workers = max(len(task.action) for task in self.tasks) if not self.workers else self.workers\n</code></pre>"},{"location":"workflow/#txtai.workflow.Workflow.__call__","title":"<code>__call__(elements)</code>","text":"<p>Executes a workflow for input elements. This method returns a generator that yields transformed data elements.</p> <p>Parameters:</p> Name Type Description Default <code>elements</code> <p>iterable data elements</p> required <p>Returns:</p> Type Description <p>generator that yields transformed data elements</p> Source code in <code>txtai/workflow/base.py</code> <pre><code>def __call__(self, elements):\n    \"\"\"\n    Executes a workflow for input elements. This method returns a generator that yields transformed\n    data elements.\n\n    Args:\n        elements: iterable data elements\n\n    Returns:\n        generator that yields transformed data elements\n    \"\"\"\n\n    # Create execute instance for this run\n    with Execute(self.workers) as executor:\n        # Run task initializers\n        self.initialize()\n\n        # Process elements with stream processor, if available\n        elements = self.stream(elements) if self.stream else elements\n\n        # Process elements in batches\n        for batch in self.chunk(elements):\n            yield from self.process(batch, executor)\n\n        # Run task finalizers\n        self.finalize()\n</code></pre>"},{"location":"workflow/#txtai.workflow.Workflow.schedule","title":"<code>schedule(cron, elements, iterations=None)</code>","text":"<p>Schedules a workflow using a cron expression and elements.</p> <p>Parameters:</p> Name Type Description Default <code>cron</code> <p>cron expression</p> required <code>elements</code> <p>iterable data elements passed to workflow each call</p> required <code>iterations</code> <p>number of times to run workflow, defaults to run indefinitely</p> <code>None</code> Source code in <code>txtai/workflow/base.py</code> <pre><code>def schedule(self, cron, elements, iterations=None):\n    \"\"\"\n    Schedules a workflow using a cron expression and elements.\n\n    Args:\n        cron: cron expression\n        elements: iterable data elements passed to workflow each call\n        iterations: number of times to run workflow, defaults to run indefinitely\n    \"\"\"\n\n    # Check that croniter is installed\n    if not CRONITER:\n        raise ImportError('Workflow scheduling is not available - install \"workflow\" extra to enable')\n\n    logger.info(\"'%s' scheduler started with schedule %s\", self.name, cron)\n\n    maxiterations = iterations\n    while iterations is None or iterations &gt; 0:\n        # Schedule using localtime\n        schedule = croniter(cron, datetime.now().astimezone()).get_next(datetime)\n        logger.info(\"'%s' next run scheduled for %s\", self.name, schedule.isoformat())\n        time.sleep(schedule.timestamp() - time.time())\n\n        # Run workflow\n        # pylint: disable=W0703\n        try:\n            for _ in self(elements):\n                pass\n        except Exception:\n            logger.error(traceback.format_exc())\n\n        # Decrement iterations remaining, if necessary\n        if iterations is not None:\n            iterations -= 1\n\n    logger.info(\"'%s' max iterations (%d) reached\", self.name, maxiterations)\n</code></pre>"},{"location":"workflow/#more-examples","title":"More examples","text":"<p>Check out this Workflow Quickstart Example. See this link for a full list of workflow examples.</p>"},{"location":"workflow/schedule/","title":"Schedule","text":"<p>Workflows can run on a repeating basis with schedules. This is suitable in cases where a workflow is run against a dynamically expanding input, like an API service or directory of files. </p> <p>The schedule method takes a cron expression, list of static elements (which dynamically expand i.e. API service, directory listing) and an optional maximum number of iterations.</p> <p>Below are a couple example cron expressions.</p> <pre><code># \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n# | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n# | | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31)\n# | | | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12)\n# | | | | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6)\n# | | | | | \u250c\u2500\u2500\u2500\u2500\u2500 second (0 - 59)\n# | | | | | |\n  * * * * * *      # Run every second\n0/5 * * * *        # Run every 5 minutes\n  0 0 1 * *        # Run monthly on 1st\n  0 0 1 1 *        # Run on Jan 1 at 12am\n  0 0 * * mon,wed  # Run Monday and Wednesday\n</code></pre>"},{"location":"workflow/schedule/#python","title":"Python","text":"<p>Simple workflow scheduled with Python.</p> <pre><code>workflow = Workflow(tasks)\nworkflow.schedule(\"0/5 * * * *\", elements)\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Workflow Scheduling Schedule workflows with cron expressions"},{"location":"workflow/schedule/#configuration","title":"Configuration","text":"<p>Simple workflow scheduled with configuration.</p> <pre><code>workflow:\n  index:\n    schedule:\n      cron: 0/5 * * * *\n      elements: [...]\n    tasks: [...]\n</code></pre> <pre><code># Create and run the workflow\nfrom txtai import Application\n\n# Create and run the workflow\napp = Application(\"workflow.yml\")\n\n# Wait for scheduled workflows\napp.wait()\n</code></pre> <p>See the links below for more information on cron expressions.</p> <ul> <li>cron overview</li> <li>croniter - library used by txtai</li> </ul>"},{"location":"workflow/task/","title":"Tasks","text":"<p>Workflows execute tasks. Tasks are callable objects with a number of parameters to control the processing of data at a given step. While similar to pipelines, tasks encapsulate processing and don't perform signficant transformations on their own. Tasks perform logic to prepare content for the underlying action(s).</p> <p>A simple task is shown below.</p> <pre><code>Task(lambda x: [y * 2 for y in x])\n</code></pre> <p>The task above executes the function above for all input elements.</p> <p>Tasks work well with pipelines, since pipelines are callable objects. The example below will summarize each input element.</p> <pre><code>summary = Summary()\nTask(summary)\n</code></pre> <p>Tasks can operate independently but work best with workflows, as workflows add large-scale stream processing.</p> <pre><code>summary = Summary()\ntask = Task(summary)\ntask([\"Very long text here\"])\n\nworkflow = Workflow([task])\nlist(workflow([\"Very long text here\"]))\n</code></pre> <p>Tasks can also be created with configuration as part of a workflow.</p> <pre><code>workflow:\n  tasks:\n    - action: summary \n</code></pre>"},{"location":"workflow/task/#txtai.workflow.Task.__init__","title":"<code>__init__(action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>","text":"<p>Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <p>action(s) to execute on each data element</p> <code>None</code> <code>select</code> <p>filter(s) used to select data to process</p> <code>None</code> <code>unpack</code> <p>if data elements should be unpacked or unwrapped from (id, data, tag) tuples</p> <code>True</code> <code>column</code> <p>column index to select if element is a tuple, defaults to all</p> <code>None</code> <code>merge</code> <p>merge mode for joining multi-action outputs, defaults to hstack</p> <code>'hstack'</code> <code>initialize</code> <p>action to execute before processing</p> <code>None</code> <code>finalize</code> <p>action to execute after processing</p> <code>None</code> <code>concurrency</code> <p>sets concurrency method when execute instance available          valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency</p> <code>None</code> <code>onetomany</code> <p>if one-to-many data transformations should be enabled, defaults to True</p> <code>True</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n    \"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/#multi-action-task-concurrency","title":"Multi-action task concurrency","text":"<p>The default processing mode is to run actions sequentially. Multiprocessing support is already built in at a number of levels. Any of the GPU models will maximize GPU utilization for example and even in CPU mode, concurrency is utilized. But there are still use cases for task action concurrency. For example, if the system has multiple GPUs, the task runs external sequential code, or the task has a large number of I/O tasks.</p> <p>In addition to sequential processing, multi-action tasks can run either multithreaded or with multiple processes. The advantages of each approach are discussed below.</p> <ul> <li> <p>multithreading - no overhead of creating separate processes or pickling data. But Python can only execute a single thread due the GIL, so this approach won't help with CPU bound actions. This method works well with I/O bound actions and GPU actions.</p> </li> <li> <p>multiprocessing - separate subprocesses are created and data is exchanged via pickling. This method can fully utilize all CPU cores since each process runs independently. This method works well with CPU bound actions.</p> </li> </ul> <p>More information on multiprocessing can be found in the Python documentation.</p>"},{"location":"workflow/task/#multi-action-task-merges","title":"Multi-action task merges","text":"<p>Multi-action tasks will generate parallel outputs for the input data. The task output can be merged together in a couple different ways.</p>"},{"location":"workflow/task/#txtai.workflow.Task.hstack","title":"<code>hstack(outputs)</code>","text":"<p>Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation.</p> <p>Column-wise merge example (2 actions)</p> <p>Inputs: [a, b, c]</p> <p>Outputs =&gt; [[a1, b1, c1], [a2, b2, c2]]</p> <p>Column Merge =&gt; [(a1, a2), (b1, b2), (c1, c2)]</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <p>task outputs</p> required <p>Returns:</p> Type Description <p>list of aggregated/zipped outputs as tuples (column-wise)</p> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def hstack(self, outputs):\n    \"\"\"\n    Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation.\n\n    Column-wise merge example (2 actions)\n\n      Inputs: [a, b, c]\n\n      Outputs =&gt; [[a1, b1, c1], [a2, b2, c2]]\n\n      Column Merge =&gt; [(a1, a2), (b1, b2), (c1, c2)]\n\n    Args:\n        outputs: task outputs\n\n    Returns:\n        list of aggregated/zipped outputs as tuples (column-wise)\n    \"\"\"\n\n    # If all outputs are numpy arrays, use native method\n    if all(isinstance(output, np.ndarray) for output in outputs):\n        return np.stack(outputs, axis=1)\n\n    # If all outputs are torch tensors, use native method\n    # pylint: disable=E1101\n    if all(torch.is_tensor(output) for output in outputs):\n        return torch.stack(outputs, axis=1)\n\n    return list(zip(*outputs))\n</code></pre>"},{"location":"workflow/task/#txtai.workflow.Task.vstack","title":"<code>vstack(outputs)</code>","text":"<p>Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation.</p> <p>Row-wise merge example (2 actions)</p> <p>Inputs: [a, b, c]</p> <p>Outputs =&gt; [[a1, b1, c1], [a2, b2, c2]]</p> <p>Row Merge =&gt; [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2]</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <p>task outputs</p> required <p>Returns:</p> Type Description <p>list of aggregated/zipped outputs as one to many transforms (row-wise)</p> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def vstack(self, outputs):\n    \"\"\"\n    Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation.\n\n    Row-wise merge example (2 actions)\n\n      Inputs: [a, b, c]\n\n      Outputs =&gt; [[a1, b1, c1], [a2, b2, c2]]\n\n      Row Merge =&gt; [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2]\n\n    Args:\n        outputs: task outputs\n\n    Returns:\n        list of aggregated/zipped outputs as one to many transforms (row-wise)\n    \"\"\"\n\n    # If all outputs are numpy arrays, use native method\n    if all(isinstance(output, np.ndarray) for output in outputs):\n        return np.concatenate(np.stack(outputs, axis=1))\n\n    # If all outputs are torch tensors, use native method\n    # pylint: disable=E1101\n    if all(torch.is_tensor(output) for output in outputs):\n        return torch.cat(tuple(torch.stack(outputs, axis=1)))\n\n    # Flatten into lists of outputs per input row. Wrap as one to many transformation.\n    merge = []\n    for x in zip(*outputs):\n        combine = []\n        for y in x:\n            if isinstance(y, list):\n                combine.extend(y)\n            else:\n                combine.append(y)\n\n        merge.append(OneToMany(combine))\n\n    return merge\n</code></pre>"},{"location":"workflow/task/#txtai.workflow.Task.concat","title":"<code>concat(outputs)</code>","text":"<p>Merges outputs column-wise and concats values together into a string. Returns a list of strings.</p> <p>Concat merge example (2 actions)</p> <p>Inputs: [a, b, c]</p> <p>Outputs =&gt; [[a1, b1, c1], [a2, b2, c2]]</p> <p>Concat Merge =&gt; [(a1, a2), (b1, b2), (c1, c2)] =&gt; [\"a1. a2\", \"b1. b2\", \"c1. c2\"]</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <p>task outputs</p> required <p>Returns:</p> Type Description <p>list of concat outputs</p> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def concat(self, outputs):\n    \"\"\"\n    Merges outputs column-wise and concats values together into a string. Returns a list of strings.\n\n    Concat merge example (2 actions)\n\n      Inputs: [a, b, c]\n\n      Outputs =&gt; [[a1, b1, c1], [a2, b2, c2]]\n\n      Concat Merge =&gt; [(a1, a2), (b1, b2), (c1, c2)] =&gt; [\"a1. a2\", \"b1. b2\", \"c1. c2\"]\n\n    Args:\n        outputs: task outputs\n\n    Returns:\n        list of concat outputs\n    \"\"\"\n\n    return [\". \".join([str(y) for y in x if y]) for x in self.hstack(outputs)]\n</code></pre>"},{"location":"workflow/task/#extract-task-output-columns","title":"Extract task output columns","text":"<p>With column-wise merging, each output row will be a tuple of output values for each task action. This can be fed as input to a downstream task and that task can have separate tasks work with each element.</p> <p>A simple example:</p> <pre><code>workflow = Workflow([Task(lambda x: [y * 3 for y in x], unpack=False, column=0)])\nlist(workflow([(2, 8)]))\n</code></pre> <p>For the example input tuple of (2, 2), the workflow will only select the first element (2) and run the task against that element. </p> <pre><code>workflow = Workflow([Task([lambda x: [y * 3 for y in x], \n                           lambda x: [y - 1 for y in x]],\n                           unpack=False, column={0:0, 1:1})])\nlist(workflow([(2, 8)]))\n</code></pre> <p>The example above applies a separate action to each input column. This simple construct can help build extremely powerful workflow graphs!</p>"},{"location":"workflow/task/console/","title":"Console Task","text":"<p>The Console Task prints task inputs and outputs to standard output. This task is mainly used for debugging and can be added at any point in a workflow.</p>"},{"location":"workflow/task/console/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import FileTask, Workflow\n\nworkflow = Workflow([ConsoleTask()])\nworkflow([\"Input 1\", \"Input2\"])\n</code></pre>"},{"location":"workflow/task/console/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\n  tasks:\n    - task: console\n</code></pre>"},{"location":"workflow/task/console/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/console/#txtai.workflow.ConsoleTask.__init__","title":"<code>__init__(action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>","text":"<p>Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <p>action(s) to execute on each data element</p> <code>None</code> <code>select</code> <p>filter(s) used to select data to process</p> <code>None</code> <code>unpack</code> <p>if data elements should be unpacked or unwrapped from (id, data, tag) tuples</p> <code>True</code> <code>column</code> <p>column index to select if element is a tuple, defaults to all</p> <code>None</code> <code>merge</code> <p>merge mode for joining multi-action outputs, defaults to hstack</p> <code>'hstack'</code> <code>initialize</code> <p>action to execute before processing</p> <code>None</code> <code>finalize</code> <p>action to execute after processing</p> <code>None</code> <code>concurrency</code> <p>sets concurrency method when execute instance available          valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency</p> <code>None</code> <code>onetomany</code> <p>if one-to-many data transformations should be enabled, defaults to True</p> <code>True</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n    \"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/export/","title":"Export Task","text":"<p>The Export Task exports task outputs to CSV or Excel.</p>"},{"location":"workflow/task/export/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import FileTask, Workflow\n\nworkflow = Workflow([ExportTask()])\nworkflow([\"Input 1\", \"Input2\"])\n</code></pre>"},{"location":"workflow/task/export/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\n  tasks:\n    - task: export\n</code></pre>"},{"location":"workflow/task/export/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/export/#txtai.workflow.ExportTask.__init__","title":"<code>__init__(action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>","text":"<p>Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <p>action(s) to execute on each data element</p> <code>None</code> <code>select</code> <p>filter(s) used to select data to process</p> <code>None</code> <code>unpack</code> <p>if data elements should be unpacked or unwrapped from (id, data, tag) tuples</p> <code>True</code> <code>column</code> <p>column index to select if element is a tuple, defaults to all</p> <code>None</code> <code>merge</code> <p>merge mode for joining multi-action outputs, defaults to hstack</p> <code>'hstack'</code> <code>initialize</code> <p>action to execute before processing</p> <code>None</code> <code>finalize</code> <p>action to execute after processing</p> <code>None</code> <code>concurrency</code> <p>sets concurrency method when execute instance available          valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency</p> <code>None</code> <code>onetomany</code> <p>if one-to-many data transformations should be enabled, defaults to True</p> <code>True</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n    \"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/export/#txtai.workflow.ExportTask.register","title":"<code>register(output=None, timestamp=None)</code>","text":"<p>Add export parameters to task. Checks if required dependencies are installed.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <p>output file path</p> <code>None</code> <code>timestamp</code> <p>true if output file should be timestamped</p> <code>None</code> Source code in <code>txtai/workflow/task/export.py</code> <pre><code>def register(self, output=None, timestamp=None):\n    \"\"\"\n    Add export parameters to task. Checks if required dependencies are installed.\n\n    Args:\n        output: output file path\n        timestamp: true if output file should be timestamped\n    \"\"\"\n\n    if not PANDAS:\n        raise ImportError('ExportTask is not available - install \"workflow\" extra to enable')\n\n    # pylint: disable=W0201\n    self.output = output\n    self.timestamp = timestamp\n</code></pre>"},{"location":"workflow/task/file/","title":"File Task","text":"<p>The File Task validates a file exists. It handles both file paths and local file urls. Note that this task only works with local files.</p>"},{"location":"workflow/task/file/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import FileTask, Workflow\n\nworkflow = Workflow([FileTask()])\nworkflow([\"/path/to/file\", \"file:///path/to/file\"])\n</code></pre>"},{"location":"workflow/task/file/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\n  tasks:\n    - task: file\n</code></pre>"},{"location":"workflow/task/file/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/file/#txtai.workflow.FileTask.__init__","title":"<code>__init__(action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>","text":"<p>Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <p>action(s) to execute on each data element</p> <code>None</code> <code>select</code> <p>filter(s) used to select data to process</p> <code>None</code> <code>unpack</code> <p>if data elements should be unpacked or unwrapped from (id, data, tag) tuples</p> <code>True</code> <code>column</code> <p>column index to select if element is a tuple, defaults to all</p> <code>None</code> <code>merge</code> <p>merge mode for joining multi-action outputs, defaults to hstack</p> <code>'hstack'</code> <code>initialize</code> <p>action to execute before processing</p> <code>None</code> <code>finalize</code> <p>action to execute after processing</p> <code>None</code> <code>concurrency</code> <p>sets concurrency method when execute instance available          valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency</p> <code>None</code> <code>onetomany</code> <p>if one-to-many data transformations should be enabled, defaults to True</p> <code>True</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n    \"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/image/","title":"Image Task","text":"<p>The Image Task reads file paths, check the file is an image and opens it as an Image object. Note that this task only works with local files.</p>"},{"location":"workflow/task/image/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import ImageTask, Workflow\n\nworkflow = Workflow([ImageTask()])\nworkflow([\"image.jpg\", \"image.gif\"])\n</code></pre>"},{"location":"workflow/task/image/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\n  tasks:\n    - task: image\n</code></pre>"},{"location":"workflow/task/image/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/image/#txtai.workflow.ImageTask.__init__","title":"<code>__init__(action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>","text":"<p>Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <p>action(s) to execute on each data element</p> <code>None</code> <code>select</code> <p>filter(s) used to select data to process</p> <code>None</code> <code>unpack</code> <p>if data elements should be unpacked or unwrapped from (id, data, tag) tuples</p> <code>True</code> <code>column</code> <p>column index to select if element is a tuple, defaults to all</p> <code>None</code> <code>merge</code> <p>merge mode for joining multi-action outputs, defaults to hstack</p> <code>'hstack'</code> <code>initialize</code> <p>action to execute before processing</p> <code>None</code> <code>finalize</code> <p>action to execute after processing</p> <code>None</code> <code>concurrency</code> <p>sets concurrency method when execute instance available          valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency</p> <code>None</code> <code>onetomany</code> <p>if one-to-many data transformations should be enabled, defaults to True</p> <code>True</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n    \"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/retrieve/","title":"Retrieve Task","text":"<p>The Retrieve Task connects to a url and downloads the content locally. This task is helpful when working with actions that require data to be available locally.</p>"},{"location":"workflow/task/retrieve/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import RetrieveTask, Workflow\n\nworkflow = Workflow([RetrieveTask(directory=\"/tmp\")])\nworkflow([\"https://file.to.download\", \"/local/file/to/copy\"])\n</code></pre>"},{"location":"workflow/task/retrieve/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\n  tasks:\n    - task: retrieve\n      directory: /tmp\n</code></pre>"},{"location":"workflow/task/retrieve/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/retrieve/#txtai.workflow.RetrieveTask.__init__","title":"<code>__init__(action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>","text":"<p>Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <p>action(s) to execute on each data element</p> <code>None</code> <code>select</code> <p>filter(s) used to select data to process</p> <code>None</code> <code>unpack</code> <p>if data elements should be unpacked or unwrapped from (id, data, tag) tuples</p> <code>True</code> <code>column</code> <p>column index to select if element is a tuple, defaults to all</p> <code>None</code> <code>merge</code> <p>merge mode for joining multi-action outputs, defaults to hstack</p> <code>'hstack'</code> <code>initialize</code> <p>action to execute before processing</p> <code>None</code> <code>finalize</code> <p>action to execute after processing</p> <code>None</code> <code>concurrency</code> <p>sets concurrency method when execute instance available          valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency</p> <code>None</code> <code>onetomany</code> <p>if one-to-many data transformations should be enabled, defaults to True</p> <code>True</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n    \"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/retrieve/#txtai.workflow.RetrieveTask.register","title":"<code>register(directory=None, flatten=True)</code>","text":"<p>Adds retrieve parameters to task.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <p>local directory used to store retrieved files</p> <code>None</code> <code>flatten</code> <p>flatten input directory structure, defaults to True</p> <code>True</code> Source code in <code>txtai/workflow/task/retrieve.py</code> <pre><code>def register(self, directory=None, flatten=True):\n    \"\"\"\n    Adds retrieve parameters to task.\n\n    Args:\n        directory: local directory used to store retrieved files\n        flatten: flatten input directory structure, defaults to True\n    \"\"\"\n\n    # pylint: disable=W0201\n    # Create default temporary directory if not specified\n    if not directory:\n        # Save tempdir to prevent content from being deleted until this task is out of scope\n        # pylint: disable=R1732\n        self.tempdir = tempfile.TemporaryDirectory()\n        directory = self.tempdir.name\n\n    # Create output directory if necessary\n    os.makedirs(directory, exist_ok=True)\n\n    self.directory = directory\n    self.flatten = flatten\n</code></pre>"},{"location":"workflow/task/service/","title":"Service Task","text":"<p>The Service Task extracts content from a http service.</p>"},{"location":"workflow/task/service/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import ServiceTask, Workflow\n\nworkflow = Workflow([ServiceTask(url=\"https://service.url/action)])\nworkflow([\"parameter\"])\n</code></pre>"},{"location":"workflow/task/service/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\n  tasks:\n    - task: service\n      url: https://service.url/action\n</code></pre>"},{"location":"workflow/task/service/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/service/#txtai.workflow.ServiceTask.__init__","title":"<code>__init__(action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>","text":"<p>Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <p>action(s) to execute on each data element</p> <code>None</code> <code>select</code> <p>filter(s) used to select data to process</p> <code>None</code> <code>unpack</code> <p>if data elements should be unpacked or unwrapped from (id, data, tag) tuples</p> <code>True</code> <code>column</code> <p>column index to select if element is a tuple, defaults to all</p> <code>None</code> <code>merge</code> <p>merge mode for joining multi-action outputs, defaults to hstack</p> <code>'hstack'</code> <code>initialize</code> <p>action to execute before processing</p> <code>None</code> <code>finalize</code> <p>action to execute after processing</p> <code>None</code> <code>concurrency</code> <p>sets concurrency method when execute instance available          valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency</p> <code>None</code> <code>onetomany</code> <p>if one-to-many data transformations should be enabled, defaults to True</p> <code>True</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n    \"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/service/#txtai.workflow.ServiceTask.register","title":"<code>register(url=None, method=None, params=None, batch=True, extract=None)</code>","text":"<p>Adds service parameters to task. Checks if required dependencies are installed.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <p>url to connect to</p> <code>None</code> <code>method</code> <p>http method, GET or POST</p> <code>None</code> <code>params</code> <p>default query parameters</p> <code>None</code> <code>batch</code> <p>if True, all elements are passed in a single batch request, otherwise a service call is executed per element</p> <code>True</code> <code>extract</code> <p>list of sections to extract from response</p> <code>None</code> Source code in <code>txtai/workflow/task/service.py</code> <pre><code>def register(self, url=None, method=None, params=None, batch=True, extract=None):\n    \"\"\"\n    Adds service parameters to task. Checks if required dependencies are installed.\n\n    Args:\n        url: url to connect to\n        method: http method, GET or POST\n        params: default query parameters\n        batch: if True, all elements are passed in a single batch request, otherwise a service call is executed per element\n        extract: list of sections to extract from response\n    \"\"\"\n\n    if not XML_TO_DICT:\n        raise ImportError('ServiceTask is not available - install \"workflow\" extra to enable')\n\n    # pylint: disable=W0201\n    # Save URL, method and parameter defaults\n    self.url = url\n    self.method = method\n    self.params = params\n\n    # If True, all elements are passed in a single batch request, otherwise a service call is executed per element\n    self.batch = batch\n\n    # Save sections to extract. Supports both a single string and a hierarchical list of sections.\n    self.extract = extract\n    if self.extract:\n        self.extract = [self.extract] if isinstance(self.extract, str) else self.extract\n</code></pre>"},{"location":"workflow/task/storage/","title":"Storage Task","text":"<p>The Storage Task expands a local directory or cloud storage bucket into a list of URLs to process.</p>"},{"location":"workflow/task/storage/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import StorageTask, Workflow\n\nworkflow = Workflow([StorageTask()])\nworkflow([\"s3://path/to/bucket\", \"local://local/directory\"])\n</code></pre>"},{"location":"workflow/task/storage/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\n  tasks:\n    - task: storage\n</code></pre>"},{"location":"workflow/task/storage/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/storage/#txtai.workflow.StorageTask.__init__","title":"<code>__init__(action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>","text":"<p>Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <p>action(s) to execute on each data element</p> <code>None</code> <code>select</code> <p>filter(s) used to select data to process</p> <code>None</code> <code>unpack</code> <p>if data elements should be unpacked or unwrapped from (id, data, tag) tuples</p> <code>True</code> <code>column</code> <p>column index to select if element is a tuple, defaults to all</p> <code>None</code> <code>merge</code> <p>merge mode for joining multi-action outputs, defaults to hstack</p> <code>'hstack'</code> <code>initialize</code> <p>action to execute before processing</p> <code>None</code> <code>finalize</code> <p>action to execute after processing</p> <code>None</code> <code>concurrency</code> <p>sets concurrency method when execute instance available          valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency</p> <code>None</code> <code>onetomany</code> <p>if one-to-many data transformations should be enabled, defaults to True</p> <code>True</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n    \"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/template/","title":"Template Task","text":"<p>The Template Task generates text from a template and task inputs. Templates can be used to prepare data for a number of tasks including generating large language model (LLM) prompts.</p>"},{"location":"workflow/task/template/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import TemplateTask, Workflow\n\nworkflow = Workflow([TemplateTask(template=\"This is a {text} task\")])\nworkflow([{\"text\": \"template\"}])\n</code></pre>"},{"location":"workflow/task/template/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\n  tasks:\n    - task: template\n      template: This is a {text} task\n</code></pre>"},{"location":"workflow/task/template/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/template/#txtai.workflow.TemplateTask.__init__","title":"<code>__init__(action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>","text":"<p>Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <p>action(s) to execute on each data element</p> <code>None</code> <code>select</code> <p>filter(s) used to select data to process</p> <code>None</code> <code>unpack</code> <p>if data elements should be unpacked or unwrapped from (id, data, tag) tuples</p> <code>True</code> <code>column</code> <p>column index to select if element is a tuple, defaults to all</p> <code>None</code> <code>merge</code> <p>merge mode for joining multi-action outputs, defaults to hstack</p> <code>'hstack'</code> <code>initialize</code> <p>action to execute before processing</p> <code>None</code> <code>finalize</code> <p>action to execute after processing</p> <code>None</code> <code>concurrency</code> <p>sets concurrency method when execute instance available          valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency</p> <code>None</code> <code>onetomany</code> <p>if one-to-many data transformations should be enabled, defaults to True</p> <code>True</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n    \"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/url/","title":"Url Task","text":"<p>The Url Task validates that inputs start with a url prefix.</p>"},{"location":"workflow/task/url/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import UrlTask, Workflow\n\nworkflow = Workflow([UrlTask()])\nworkflow([\"https://file.to.download\", \"file:////local/file/to/copy\"])\n</code></pre>"},{"location":"workflow/task/url/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\n  tasks:\n    - task: url\n</code></pre>"},{"location":"workflow/task/url/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/url/#txtai.workflow.UrlTask.__init__","title":"<code>__init__(action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>","text":"<p>Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <p>action(s) to execute on each data element</p> <code>None</code> <code>select</code> <p>filter(s) used to select data to process</p> <code>None</code> <code>unpack</code> <p>if data elements should be unpacked or unwrapped from (id, data, tag) tuples</p> <code>True</code> <code>column</code> <p>column index to select if element is a tuple, defaults to all</p> <code>None</code> <code>merge</code> <p>merge mode for joining multi-action outputs, defaults to hstack</p> <code>'hstack'</code> <code>initialize</code> <p>action to execute before processing</p> <code>None</code> <code>finalize</code> <p>action to execute after processing</p> <code>None</code> <code>concurrency</code> <p>sets concurrency method when execute instance available          valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency</p> <code>None</code> <code>onetomany</code> <p>if one-to-many data transformations should be enabled, defaults to True</p> <code>True</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n    \"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/workflow/","title":"Workflow Task","text":"<p>The Workflow Task runs a workflow. Allows creating workflows of workflows.</p>"},{"location":"workflow/task/workflow/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import WorkflowTask, Workflow\n\nworkflow = Workflow([WorkflowTask(otherworkflow)])\nworkflow([\"input data\"])\n</code></pre>"},{"location":"workflow/task/workflow/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/workflow/#txtai.workflow.WorkflowTask.__init__","title":"<code>__init__(action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>","text":"<p>Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <p>action(s) to execute on each data element</p> <code>None</code> <code>select</code> <p>filter(s) used to select data to process</p> <code>None</code> <code>unpack</code> <p>if data elements should be unpacked or unwrapped from (id, data, tag) tuples</p> <code>True</code> <code>column</code> <p>column index to select if element is a tuple, defaults to all</p> <code>None</code> <code>merge</code> <p>merge mode for joining multi-action outputs, defaults to hstack</p> <code>'hstack'</code> <code>initialize</code> <p>action to execute before processing</p> <code>None</code> <code>finalize</code> <p>action to execute after processing</p> <code>None</code> <code>concurrency</code> <p>sets concurrency method when execute instance available          valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency</p> <code>None</code> <code>onetomany</code> <p>if one-to-many data transformations should be enabled, defaults to True</p> <code>True</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n    \"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"}]}