{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8ab3b0",
   "metadata": {},
   "source": [
    "# RAG is more than Vector Search\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is often associated with vector search. And while that is a primary use case, any search will do.\n",
    "\n",
    "- ✅ Vector Search\n",
    "- ✅ Web Search\n",
    "- ✅ SQL Query\n",
    "\n",
    "This notebook will go over a few RAG examples covering different retrieval methods. These examples require txtai 9.3+."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a77d1",
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "\n",
    "Install `txtai` and all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bebb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline-data]\n",
    "\n",
    "# Download example SQL database\n",
    "!wget https://huggingface.co/NeuML/txtai-wikipedia-slim/resolve/main/documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece2b09",
   "metadata": {},
   "source": [
    "# RAG with Late Interaction\n",
    "\n",
    "The first example will cover RAG with ColBERT / Late Interaction retrieval. TxtAI 9.0 added support for [MUVERA](https://arxiv.org/abs/2405.19504) and [ColBERT](https://arxiv.org/abs/2112.01488) multi-vector ranking. \n",
    "\n",
    "We'll build a pipeline that reads the ColBERT v2 paper, extracts the text into sections and builds an index with a ColBERT model. Then we'll wrap that as a [Reranker pipeline](https://neuml.github.io/txtai/pipeline/text/reranker/) using the same ColBERT model. Finally a RAG pipeline will utilize this for retrieval.\n",
    "\n",
    "_Note: This uses the custom [ColBERT Muvera Nano](https://huggingface.co/NeuML/colbert-muvera-nano) model which is only 970K parameters! That's right thousands. It's surprisingly effective._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae2e6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper introduces ColBERTv2, a neural information retrieval model that enhances the quality and efficiency of late interaction by combining an aggressive residual compression mechanism with a denoised supervision strategy, achieving state-of-the-art performance across diverse benchmarks while reducing the model's space footprint by 6–10× compared to previous methods.\n"
     ]
    }
   ],
   "source": [
    "from txtai import Embeddings, RAG, Textractor\n",
    "from txtai.pipeline import Reranker, Similarity\n",
    "\n",
    "# Get text from ColBERT v2 paper\n",
    "textractor = Textractor(sections=True, backend=\"docling\")\n",
    "data = textractor(\"https://arxiv.org/pdf/2112.01488\")\n",
    "\n",
    "# MUVERA fixed dimensional encodings\n",
    "embeddings = Embeddings(content=True, path=\"neuml/colbert-muvera-nano\", vectors={\"trust_remote_code\": True})\n",
    "embeddings.index(data)\n",
    "\n",
    "# Re-rank using same late interaction model\n",
    "reranker = Reranker(embeddings, Similarity(\"neuml/colbert-muvera-nano\", lateencode=True, vectors={\"trust_remote_code\": True}))\n",
    "\n",
    "template = \"\"\"\n",
    "  Answer the following question using the provided context.\n",
    "\n",
    "  Question:\n",
    "  {question}\n",
    "\n",
    "  Context:\n",
    "  {context}\n",
    "\"\"\"\n",
    "\n",
    "# RAG with late interaction models\n",
    "rag = RAG(reranker, \"Qwen/Qwen3-4B-Instruct-2507\", template=template, output=\"flatten\")\n",
    "print(rag(\"Write a sentence abstract about this paper\", maxlength=2048))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e46de",
   "metadata": {},
   "source": [
    "# RAG with a Web Search\n",
    "\n",
    "Next we'll run a RAG pipeline using a web search as the retrieval method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61b684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It involves technologies like machine learning, deep learning, and natural language processing, and enables machines to simulate human-like learning, comprehension, problem solving, decision-making, creativity, and autonomy.\n"
     ]
    }
   ],
   "source": [
    "from smolagents import WebSearchTool\n",
    "\n",
    "tool = WebSearchTool()\n",
    "\n",
    "def websearch(queries, limit):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        result = [\n",
    "            {\"id\": i, \"text\": f'{x[\"title\"]} {x[\"description\"]}', \"score\": 1.0} for i, x in enumerate(tool.search(query))\n",
    "        ]\n",
    "        results.append(result[:limit])\n",
    "\n",
    "    return results\n",
    "\n",
    "# RAG with a websearch\n",
    "rag = RAG(websearch, \"Qwen/Qwen3-4B-Instruct-2507\", template=template, output=\"flatten\")\n",
    "print(rag(\"What is AI?\", maxlength=2048))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8a096",
   "metadata": {},
   "source": [
    "# RAG with a SQL Query\n",
    "\n",
    "The last example we'll cover is running RAG with a SQL query. We'll use the SQL database that's a component of the [txtai-wikipedia-slim](https://huggingface.co/NeuML/txtai-wikipedia-slim) embeddings database.\n",
    "\n",
    "Since this is just a database with Wikipedia abstracts, we'll need a way to build a SQL query from a search query. For that we'll use an LLM to extract a keyword to use in a `LIKE` clause.\n",
    "\n",
    "Given that the LLM used was released in August 2025, let's ask it a question that can only be accurated answered with external data. `Who won the 2025 World Series?` which ended in November."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7ff2a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the 2025 World Series, the Los Angeles Dodgers defeated the Toronto Blue Jays in seven games to win the championship. The series took place from October 24 to November 1 (ending early on November 2, Toronto time). Dodgers pitcher Yoshinobu Yamamoto was named the World Series MVP. The series was televised by Fox in the United States and by Sportsnet in Canada.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "from txtai import LLM, RAG\n",
    "\n",
    "def keyword(query):\n",
    "    return llm(f\"\"\"\n",
    "        Extract a keyword for this search query: {query}.\n",
    "        Return only text with no other formatting or explanation.\n",
    "    \"\"\")\n",
    "\n",
    "def sqlsearch(queries, limit):\n",
    "    results = []\n",
    "    sql = \"SELECT id, text FROM sections WHERE id LIKE ? LIMIT ?\"\n",
    "\n",
    "    for query in queries:\n",
    "        # Extract a keyword for this search\n",
    "        query = keyword(query)\n",
    "\n",
    "        # Run the SQL Query\n",
    "        results.append([\n",
    "            {\"id\": uid, \"text\": text, \"score\": 1.0}\n",
    "            for uid, text in cursor.execute(sql, [f\"%{query}%\", limit])\n",
    "        ])\n",
    "\n",
    "    return results\n",
    "\n",
    "# Load the database\n",
    "cursor = sqlite3.connect(\"documents\")\n",
    "\n",
    "# Load the LLM\n",
    "llm = LLM(\"Qwen/Qwen3-4B-Instruct-2507\")\n",
    "\n",
    "template = \"\"\"\n",
    "  Answer the following question using the provided context.\n",
    "\n",
    "  Question:\n",
    "  {question}\n",
    "\n",
    "  Context:\n",
    "  {context}\n",
    "\"\"\"\n",
    "\n",
    "# RAG with a SQL query\n",
    "rag = RAG(sqlsearch, llm, template=template, output=\"flatten\")\n",
    "print(rag(\"Tell me what happened in the 2025 World Series\", maxlength=2048))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f78076",
   "metadata": {},
   "source": [
    "And as we see, this answer is using the SQL database!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1e282",
   "metadata": {},
   "source": [
    "# Wrapping up\n",
    "\n",
    "This notebook showed that RAG is about much more than vector search. With txtai 9.3+, any callable method is now supported for retrieval. Enjoy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
