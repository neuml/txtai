{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGeVB8M41jqW"
      },
      "source": [
        "# Build RAG pipelines with txtai\n",
        "\n",
        "Large Language Models (LLMs) have completely dominated the tech space in recent years. The results have been amazing and the public imagination is almost endless.\n",
        "\n",
        "While LLMs have been impressive, they are not problem free. The biggest challenge is with hallucinations. Hallucinations is the term for when a LLM generates output that is factually incorrect. The alarming part of this is that on a cursory glance, it actually sounds like good content. The default behavior of LLMs is to produce plausible answers even when no plausible answer exists. LLMs are not great at saying I don't know.\n",
        "\n",
        "Retrieval augmented generation (RAG) helps reduce the risk of hallucinations by limiting the context in which a LLM can generate answers. This is typically done with a vector search query that hydrates a prompt with a relevant context. RAG is one of the most practical and production-ready use cases for *Generative AI*. It's so popular now, that some are creating their entire companies around it.\n",
        "\n",
        "[txtai](https://github.com/neuml/txtai) has long had question-answering pipelines, which employ the same process of retrieving a relevant context. LLMs are now the preferred approach for analyzing that context and RAG pipelines are one of the main features of txtai. One of the other main features of txtai is that it's a vector database! You can build your prompts and limit your context all with one library. Hence the phrase *all-in-one AI framework*.\n",
        "\n",
        "This notebook shows how to build RAG pipelines with txtai."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQrHIw351lwE"
      },
      "source": [
        "# Install dependencies\n",
        "\n",
        "Install `txtai` and all dependencies. Since this notebook is using optional pipelines, we need to install the pipeline extras package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0AqRP7v1hdr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/neuml/txtai#egg=txtai[pipeline]\n",
        "\n",
        "# Get test data\n",
        "!wget -N https://github.com/neuml/txtai/releases/download/v6.2.0/tests.tar.gz\n",
        "!tar -xvzf tests.tar.gz\n",
        "\n",
        "# Install NLTK\n",
        "import nltk\n",
        "nltk.download(['punkt', 'punkt_tab'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmPN8RDF1pXd"
      },
      "source": [
        "# Start with the basics\n",
        "\n",
        "Let's jump right in and start with a simple LLM pipeline. The [LLM pipeline](https://neuml.github.io/txtai/pipeline/text/llm/) supports local LLM models via [Hugging Face Transformers](https://github.com/huggingface/transformers) and [llama.cpp](https://github.com/abetlen/llama-cpp-python).\n",
        "\n",
        "The LLM pipeline also supports [API services (i.e. OpenAI, Claude, Bedrock etc) via LiteLLM](https://github.com/BerriAI/litellm). The LLM pipeline automatically detects the underlying LLM framework from the `path` parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XZ7vPBIs1rGZ"
      },
      "outputs": [],
      "source": [
        "from txtai import LLM\n",
        "\n",
        "# Create LLM\n",
        "llm = LLM(\"Qwen/Qwen3-4B-Instruct-2507\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rmTWMxAH3Vx"
      },
      "source": [
        "Next, we'll load a document to query. The [Textractor pipeline](https://neuml.github.io/txtai/pipeline/data/textractor/) has support for extracting text from common document formats (docx, pdf, xlsx, web)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nifStGtOHuyc",
        "outputId": "5a4010e0-75f9-4095-a24c-cd4c859847d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# txtai â€“ the all-in-one embeddings database\n",
            "txtai is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.\n",
            "\n",
            "Summary of txtai features:\n",
            "Â· *Vector search* with SQL, object storage, topic modeling\n",
            "Â· Create *embeddings* for text, documents, audio, images and video\n",
            "Â· *Pipelines* powered by language models that run LLM prompts\n",
            "Â· *Workflows* to join pipelines together and aggregate business logic\n",
            "Â· Build with *Python* or *YAML* . API bindings available for JavaScript, Java, Rust and Go.\n",
            "Â· *Run local or scale out with container orchestration* \n",
            "\n",
            "\n",
            "## Examples\n",
            "List of example notebooks.\n",
            "|Notebook|Description|\n",
            "|---|---|\n",
            "|Introducing txtai |Overview of the functionality provided by txtai|\n",
            "|Similarity search with images|Embed images and text into the same space for search|\n",
            "|Build a QA database|Question matching with semantic search|\n",
            "|Semantic Graphs|Explore topics, data connectivity and run network analysis|\n",
            "\n",
            "## Install\n",
            "The easiest way to install is via pip and PyPI\n",
            "pip install txtai\n",
            "Python 3.10+ is supported. Using a Python virtual environment is **recommended** .\n",
            "See the detailed install instructions for more information covering optional dependencies, environment specific prerequisites, installing from source, conda support and how to run with containers.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "## Model guide\n",
            "The following shows a list of suggested models.\n",
            "|Component|Model(s)|\n",
            "|---|---|\n",
            "|Embeddings|all-MiniLM-L6-v2|\n",
            "||E5-base-v2|\n",
            "|Image Captions|BLIP|\n",
            "|Labels - Zero Shot|BART-Large-MNLI|\n",
            "|Labels - Fixed|Fine-tune with training pipeline|\n",
            "|Large Language Model (LLM)|Flan T5 XL|\n",
            "||Mistral 7B OpenOrca|\n",
            "|Summarization|DistilBART|\n",
            "|Text-to-Speech|ESPnet JETS|\n",
            "|Transcription|Whisper|\n",
            "|Translation|OPUS Model Series|\n"
          ]
        }
      ],
      "source": [
        "from txtai import Textractor\n",
        "\n",
        "# Create Textractor\n",
        "textractor = Textractor()\n",
        "text = textractor(\"txtai/document.docx\")\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jkamwgdIgEp"
      },
      "source": [
        "Now we'll define a simple LLM pipeline. It takes a question and context (which in this case is the whole file), creates a prompt and runs it with the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        },
        "id": "9HU6C0OIIAKn",
        "outputId": "f9d556c4-cd7a-4774-ef62-1f3fff90aa47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'txtai is an all-in-one embeddings database that supports semantic search, LLM orchestration, and language model workflows with features like vector search, embeddings for text, audio, images, and video, pipelines powered by language models, and scalable workflows available via Python or YAML with API bindings for multiple languages.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def execute(question, text):\n",
        "  return llm([\n",
        "    {\"role\": \"system\", \"content\": \"You are a friendly assistant. You answer questions from users.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"\"\"\n",
        "        Answer the following question using only the context below. Only include information specifically discussed.\n",
        "\n",
        "        question: {question}\n",
        "        context: {text} \n",
        "    \"\"\"}\n",
        "  ], maxlength=4096)\n",
        "\n",
        "execute(\"Tell me about txtai in one sentence\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "xxF7ajCPJP5_",
        "outputId": "0e6f6dbb-c784-4841-fe3f-82754ef478eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'txtai recommends using Whisper for transcription.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "execute(\"What model does txtai recommend for transcription?\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "AKmmTqsnJa5X",
        "outputId": "834bf3ee-b7ed-4e38-e2ef-d5950f99ed9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The best thing to read if you don\\'t know anything about txtai would be the \"Introducing txtai\" notebook, as it provides an overview of the functionality offered by txtai.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "execute(\"I don't know anything about txtai, what would be the best thing to read?\", text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaVeEHrIMpFr"
      },
      "source": [
        "If this is the first time you've seen *Generative AI*, then these statements are ðŸ¤¯. Even if you've been in the space a while, it's still amazing how much a language model can understand and the high level of quality in it's answers.\n",
        "\n",
        "While this use case is fun, lets try to scale it to a larger set of documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viVVft59NbKv"
      },
      "source": [
        "# Build a RAG pipeline with vector search\n",
        "\n",
        "Let's say we have a large number of documents, hundreds/thousands etc. We can't just put all those documents into a single prompt, we'll run out of GPU memory fast!\n",
        "\n",
        "This is where retrieval augmented generation enters the picture. We can use a query step that finds the best candidates to add to the prompt.\n",
        "\n",
        "Typically, this candidate query uses vector search but it can be anything that runs a search and returns results. In fact, many complex production systems have customized retrieval pipelines that feed a context into LLM prompts.\n",
        "\n",
        "The first step in building our RAG pipeline is creating the knowledge store. In this case, it's a vector database of file content. The files will be split into paragraphs with each paragraph stored as a separate row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipmsmtN1NahT",
        "outputId": "64733e1f-fb7b-4a2d-bf02-8930478a8ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indexing txtai/article.pdf\n",
            "Indexing txtai/document.docx\n",
            "Indexing txtai/document.pdf\n",
            "Indexing txtai/spreadsheet.xlsx\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from txtai import Embeddings\n",
        "\n",
        "def stream(path):\n",
        "  for f in sorted(os.listdir(path)):\n",
        "    fpath = os.path.join(path, f)\n",
        "\n",
        "    # Only accept documents\n",
        "    if f.endswith((\"docx\", \"xlsx\", \"pdf\")):\n",
        "      print(f\"Indexing {fpath}\")\n",
        "      for paragraph in textractor(fpath):\n",
        "        yield paragraph\n",
        "\n",
        "# Document text extraction, split into paragraphs\n",
        "textractor = Textractor(paragraphs=True)\n",
        "\n",
        "# Vector Database\n",
        "embeddings = Embeddings(content=True)\n",
        "embeddings.index(stream(\"txtai\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASlmAaR3nBPN"
      },
      "source": [
        "The next step is defining the RAG pipeline. This pipeline takes the input question, runs a vector search and builds a context using the search results. The context is then inserted into a prompt template and run with the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "_-9SW6r4P5ha",
        "outputId": "6a7bcd69-bcd0-4f6e-81c1-e32c323b3ffb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'txtai recommends using BLIP for image captioning.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def context(question):\n",
        "  context =  \"\\n\".join(x[\"text\"] for x in embeddings.search(question))\n",
        "  return context\n",
        "\n",
        "def rag(question):\n",
        "  return execute(question, context(question))\n",
        "\n",
        "rag(\"What model does txtai recommend for image captioning?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbQhSunPQtB0",
        "outputId": "de0caf04-4cdb-48e8-aadf-37283be9909a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The BLIP model was added for image captioning on 2022-03-17.\n"
          ]
        }
      ],
      "source": [
        "result = rag(\"When was the BLIP model added for image captioning?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6HW-3GtnTFl"
      },
      "source": [
        "As we can see, the result is similar to what we had before without vector search. The difference is that we only used a relevant portion of the documents to generate the answer.\n",
        "\n",
        "As we discussed before, this is important when dealing with large volumes of data. Not all of the data can be added to a LLM prompt. Additionally, having only the most relevant context helps the LLM generate higher quality answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ-yPC-xiUqa"
      },
      "source": [
        "# Citations for LLMs\n",
        "\n",
        "A healthy level of skepticism should be applied to answers generated by AI. We're far from the day where we can blindly trust answers from an AI model.\n",
        "\n",
        "txtai has a couple approaches for generating citations. The basic approach is to take the answer and search the vector database for the closest match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obttSg_dSFT5",
        "outputId": "c7ae7675-6959-4bcb-ad06-065ea8609c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E5-base-v2\n",
            "Image Captions BLIP\n",
            "Labels - Zero Shot BART-Large-MNLI\n",
            "# Model Guide\n",
            "|Component |Model(s)|Date Added|\n",
            "|---|---|---|\n",
            "|Embeddings |all-MiniLM-L6-v2|2022-04-15|\n",
            "|Image Captions |BLIP|2022-03-17|\n",
            "|Labels - Zero Shot |BART-Large-MNLI|2022-01-01|\n",
            "|Large Language Model (LLM) |Mistral 7B OpenOrca|2023-10-01|\n",
            "|Summarization |DistilBART|2021-02-22|\n",
            "|Text-to-Speech |ESPnet JETS|2022-08-01|\n",
            "|Transcription |Whisper|2022-08-01|\n",
            "|Translation |OPUS Model Series|2021-04-06|\n",
            "&\"Times New Roman,Regular\"&12&A\n",
            "## Model guide\n",
            "The following shows a list of suggested models.\n",
            "|Component|Model(s)|\n",
            "|---|---|\n",
            "|Embeddings|all-MiniLM-L6-v2|\n",
            "||E5-base-v2|\n",
            "|Image Captions|BLIP|\n",
            "|Labels - Zero Shot|BART-Large-MNLI|\n",
            "|Labels - Fixed|Fine-tune with training pipeline|\n",
            "|Large Language Model (LLM)|Flan T5 XL|\n",
            "||Mistral 7B OpenOrca|\n",
            "|Summarization|DistilBART|\n",
            "|Text-to-Speech|ESPnet JETS|\n",
            "|Transcription|Whisper|\n",
            "|Translation|OPUS Model Series|\n"
          ]
        }
      ],
      "source": [
        "for x in embeddings.search(result):\n",
        "  print(x[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDcB7GCWY6TO"
      },
      "source": [
        "While the basic approach above works in this case, txtai has a more robust pipeline to handle citations and references.\n",
        "\n",
        "The RAG pipeline is defined below. A RAG pipeline works in the same way as a LLM + Vector Search pipeline, except it has special logic for generating citations. This pipeline takes the answers and compares it to the context passed to the LLM to determine the most likely reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Lm6gg85_Y7ot"
      },
      "outputs": [],
      "source": [
        "from txtai import RAG\n",
        "\n",
        "# Create the RAG pipeline\n",
        "rag = RAG(embeddings, \"Qwen/Qwen3-4B-Instruct-2507\", template=\"\"\"\n",
        "  Answer the following question using the provided context.\n",
        "\n",
        "  Question:\n",
        "  {question}\n",
        "\n",
        "  Context:\n",
        "  {context}\n",
        "\"\"\", output=\"reference\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pOfE5paZatH",
        "outputId": "2bed2de5-22ff-4f7b-dba5-41b8e4cc6c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANSWER: Python 3.10 and later versions (Python 3.10+) are supported.\n",
            "CITATION: [{'id': '24', 'text': 'Python 3.10+ is supported. Using a Python virtual environment is recommended.'}]\n"
          ]
        }
      ],
      "source": [
        "result = rag(\"What version of Python is supported?\", maxlength=4096)\n",
        "print(\"ANSWER:\", result[\"answer\"])\n",
        "print(\"CITATION:\", embeddings.search(\"select id, text from txtai where id = :id\", limit=1, parameters={\"id\": result[\"reference\"]}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHdE2Q59jNnF"
      },
      "source": [
        "And as we can see, not only is the answer to the statement shown, the RAG pipeline also provides a citation. This step is crucial in any line of work where answers must be verified (which is most lines of work)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPwgCgBc2Er2"
      },
      "source": [
        "# Wrapping up\n",
        "\n",
        "This notebook introduced retrieval augmented generation (RAG), explained why we need it and showed the options available for running RAG pipelines with txtai.\n",
        "\n",
        "The advantages of building RAG pipelines with txtai are:\n",
        "\n",
        "- **All-in-one AI framework** - one library can handle LLM inference and vector search retrieval\n",
        "- **Generating citations** - generating answers is useful but referencing where those answers came from is crucial in gaining the trust of users\n",
        "- **Simple yet powerful** - building pipelines can be done in a small amount of Python. Options are available to build pipelines in YAML and/or run through the API"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
