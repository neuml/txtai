{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "01 - Introducing txtai",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POWZoSJR6XzK"
      },
      "source": [
        "# Part 1: Introducing txtai\n",
        "\n",
        "[txtai](https://github.com/neuml/txtai) builds an AI-powered index over sections of text. txtai supports building text indices to perform similarity searches and create extractive question-answering based systems. \n",
        "\n",
        "NeuML uses txtai and/or the concepts behind it to power all of our Natural Language Processing (NLP) applications. Example applications:\n",
        "\n",
        "- [paperai](https://github.com/neuml/paperai) - AI-powered literature discovery and review engine for medical/scientific papers\n",
        "- [tldrstory](https://github.com/neuml/tldrstory) - AI-powered understanding of headlines and story text\n",
        "- [neuspo](https://neuspo.com) - a fact-driven, real-time sports event and news site\n",
        "- [codequestion](https://github.com/neuml/codequestion) - Ask coding questions directly from the terminal\n",
        "\n",
        "txtai is built on the following stack:\n",
        "\n",
        "- [sentence-transformers](https://github.com/UKPLab/sentence-transformers)\n",
        "- [transformers](https://github.com/huggingface/transformers)\n",
        "- [faiss](https://github.com/facebookresearch/faiss)\n",
        "- Python 3.6+\n",
        "\n",
        "This notebook gives an overview of txtai and how to run similarity searches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa_PPKVX6XzN"
      },
      "source": [
        "# Install dependencies\n",
        "\n",
        "Install txtai and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-output": true,
        "id": "24q-1n5i6XzQ"
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/neuml/txtai"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHxb_jm16Xzd"
      },
      "source": [
        "# Create an Embeddings instance\n",
        "\n",
        "The Embeddings instance is the main entrypoint for txtai. An Embeddings instance defines the method used to tokenize and convert a text section into an embeddings vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QxX9EtIc6Xzg"
      },
      "source": [
        "%%capture\n",
        "\n",
        "from txtai.embeddings import Embeddings\n",
        "\n",
        "# Create embeddings model, backed by sentence-transformers & transformers\n",
        "embeddings = Embeddings({\"method\": \"transformers\", \"path\": \"sentence-transformers/bert-base-nli-mean-tokens\"})"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAO61bcy6Xzo"
      },
      "source": [
        "# Running similarity queries\n",
        "\n",
        "An embedding instance relies on the underlying transformer model to build text embeddings. The following example shows how to use an transformers Embedding instance to run similarity searches for a list of different concepts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "2j_CFGDR6Xzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b50e3b2c-337a-4bbe-cf7b-72eb2e75bada"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "sections = [\"US tops 5 million confirmed virus cases\",\n",
        "            \"Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\",\n",
        "            \"Beijing mobilises invasion craft along coast as Taiwan tensions escalate\",\n",
        "            \"The National Park Service warns against sacrificing slower friends in a bear attack\",\n",
        "            \"Maine man wins $1M from $25 lottery ticket\",\n",
        "            \"Make huge profits without work, earn up to $100,000 a day\"]\n",
        "\n",
        "print(\"%-20s %s\" % (\"Query\", \"Best Match\"))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for query in (\"feel good story\", \"climate change\", \"health\", \"war\", \"wildlife\", \"asia\", \"north america\", \"dishonest junk\"):\n",
        "    # Get index of best section that best matches query\n",
        "    uid = np.argmax(embeddings.similarity(query, sections))\n",
        "\n",
        "    print(\"%-20s %s\" % (query, sections[uid]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query                Best Match\n",
            "--------------------------------------------------\n",
            "feel good story      Maine man wins $1M from $25 lottery ticket\n",
            "climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\n",
            "health               US tops 5 million confirmed virus cases\n",
            "war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate\n",
            "wildlife             The National Park Service warns against sacrificing slower friends in a bear attack\n",
            "asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate\n",
            "north america        US tops 5 million confirmed virus cases\n",
            "dishonest junk       Make huge profits without work, earn up to $100,000 a day\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIMbLW0t6Xzw"
      },
      "source": [
        "The example above shows for almost all of the queries, the actual text isn't stored in the list of text sections. This is the true power of transformer models over token based search. What you get out of the box is ðŸ”¥ðŸ”¥ðŸ”¥!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLIjSzbq6Xzx"
      },
      "source": [
        "# Building an Embeddings index\n",
        "\n",
        "For small lists of texts, the method above works. But for larger repositories of documents, it doesn't make sense to tokenize and convert to embeddings on each query. txtai supports building pre-computed indices which signficantly improve performance. \n",
        "\n",
        "Building on the previous example, the following example runs an index method to build and store the text embeddings. In this case, only the query is converted to an embeddings vector each search. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cXfZtdHD6Xzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf66b86f-9c46-441d-e24c-2d00a455de59"
      },
      "source": [
        "# Create an index for the list of sections\n",
        "embeddings.index([(uid, text, None) for uid, text in enumerate(sections)])\n",
        "\n",
        "print(\"%-20s %s\" % (\"Query\", \"Best Match\"))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Run an embeddings search for each query\n",
        "for query in (\"feel good story\", \"climate change\", \"health\", \"war\", \"wildlife\", \"asia\", \"north america\", \"dishonest junk\"):\n",
        "    # Extract uid of first result\n",
        "    # search result format: (uid, score)\n",
        "    uid = embeddings.search(query, 1)[0][0]\n",
        "\n",
        "    # Print section\n",
        "    print(\"%-20s %s\" % (query, sections[uid]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query                Best Match\n",
            "--------------------------------------------------\n",
            "feel good story      Maine man wins $1M from $25 lottery ticket\n",
            "climate change       Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\n",
            "health               US tops 5 million confirmed virus cases\n",
            "war                  Beijing mobilises invasion craft along coast as Taiwan tensions escalate\n",
            "wildlife             The National Park Service warns against sacrificing slower friends in a bear attack\n",
            "asia                 Beijing mobilises invasion craft along coast as Taiwan tensions escalate\n",
            "north america        US tops 5 million confirmed virus cases\n",
            "dishonest junk       Make huge profits without work, earn up to $100,000 a day\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TCVl6QA6Xz5"
      },
      "source": [
        "# Embeddings load/save\n",
        "\n",
        "Embeddings indices can be saved to disk and reloaded. At this time, indices are not incrementally created, the index needs a full rebuild to incorporate new data. But that enhancement is in the backlog."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5gyO90Hc6Xz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52db38d6-4a0f-4d81-bb77-60b5cf62eccd"
      },
      "source": [
        "embeddings.save(\"index\")\n",
        "\n",
        "embeddings = Embeddings()\n",
        "embeddings.load(\"index\")\n",
        "\n",
        "uid = embeddings.search(\"climate change\", 1)[0][0]\n",
        "print(sections[uid])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d2uCotAE6X0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf029ec-9bf0-48c2-c2bd-07326b073010"
      },
      "source": [
        "!ls index"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config\tembeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDIF3tYt6X0O"
      },
      "source": [
        "# Embedding methods\n",
        "\n",
        "Embeddings supports two methods for creating text vectors, the sentence-transformers library and word embeddings vectors. Both methods have their merits as shown below:\n",
        "\n",
        "- [sentence-transformers](https://github.com/UKPLab/sentence-transformers)\n",
        "  - Creates a single embeddings vector via mean pooling of vectors generated by the transformers library. \n",
        "  - Supports models stored on Hugging Face's model hub or stored locally. \n",
        "  - See sentence-transformers for details on how to create custom models, which can be kept local or uploaded to Hugging Face's model hub.\n",
        "  - Base models require significant compute capability (GPU preferred). Possible to build smaller/lighter weight models that tradeoff accuracy for speed.\n",
        "- word embeddings\n",
        "  - Creates a single embeddings vector via BM25 scoring of each word component. See this [Medium article](https://towardsdatascience.com/building-a-sentence-embedding-index-with-fasttext-and-bm25-f07e7148d240) for the logic behind this method.\n",
        "  - Backed by the [pymagnitude](https://github.com/plasticityai/magnitude) library. Pre-trained word vectors can be installed from the referenced link.\n",
        "  - See [vectors.py](https://github.com/neuml/txtai/blob/master/src/python/txtai/vectors.py) for code that can build word vectors for custom datasets.\n",
        "  - Significantly better performance with default models. For larger datasets, it offers a good tradeoff of speed and accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nw7nbHg6X0P"
      },
      "source": [
        "# Next\n",
        "In part 2 of this series, we'll look at how to use txtai to run extractive searches"
      ]
    }
  ]
}